name: Unified Scaling & Normalization Component
description: Train-aware scaling for unified preprocessing with algorithm-specific recommendations. Fits scaling on train indices only, transforms all combined data. Supports 13 methods including Standard, MinMax, Robust, MaxAbs, L1/L2 (column-wise for clustering), Quantile, and Power transformers. Provides algorithm compatibility warnings.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: method
    type: String
    description: 'Scaling method: standard, minmax, robust, maxabs, l1_columnwise, l2_columnwise, quantile_uniform, quantile_normal, power_yeo-johnson, power_box-cox, none'
    default: 'standard'
  - name: method_params
    type: String
    description: 'Method parameters as JSON. Examples: {"feature_range":[0,1]}, {"quantile_range":[25,75]}'
    default: '{}'
  - name: target_algorithm
    type: String
    description: 'Target clustering algorithm for compatibility check: kmeans, dbscan, hierarchical, gmm, spectral, none'
    default: 'none'

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Scaled dataset (CSV)'
  - name: metadata
    type: Data
    description: 'Scaling metadata with algorithm compatibility report (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import (
            StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler,
            QuantileTransformer, PowerTransformer
        )
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('scaling_unified')
        
        # Algorithm-specific scaler recommendations
        SCALER_RECOMMENDATIONS = {
            'kmeans': {
                'recommended': ['standard', 'minmax'],
                'avoid': ['robust'],
                'reason': 'Distance-based algorithm, sensitive to scale. Needs consistent scaling.'
            },
            'dbscan': {
                'recommended': ['robust', 'minmax', 'standard'],
                'avoid': [],
                'reason': 'Density-based, benefits from robust scaling if outliers present.'
            },
            'hierarchical': {
                'recommended': ['standard', 'minmax', 'robust'],
                'avoid': [],
                'reason': 'Distance-based, any scaling works but affects dendrogram.'
            },
            'gmm': {
                'recommended': ['standard'],
                'avoid': ['robust', 'minmax'],
                'reason': 'Assumes Gaussian distributions, standard scaling preserves this.'
            },
            'spectral': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Graph-based, benefits from standard scaling for affinity matrix.'
            }
        }
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            ext = Path(input_path).suffix.lower()
            if ext in ['.parquet', '.pq']:
                return pd.read_parquet(input_path)
            return pd.read_csv(input_path)
        
        def load_indices(indices_path):
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def check_algorithm_compatibility(method, target_algorithm):
            """Check if scaler is compatible with target algorithm"""
            if target_algorithm == 'none' or target_algorithm not in SCALER_RECOMMENDATIONS:
                return {'compatible': True, 'message': 'No algorithm specified'}
            
            rec = SCALER_RECOMMENDATIONS[target_algorithm]
            
            compatibility = {
                'algorithm': target_algorithm,
                'scaler': method,
                'recommended_scalers': rec['recommended'],
                'reason': rec['reason']
            }
            
            if method in rec['recommended']:
                compatibility['compatible'] = True
                compatibility['message'] = f"✓ {method} is RECOMMENDED for {target_algorithm}"
                compatibility['level'] = 'optimal'
            elif method in rec['avoid']:
                compatibility['compatible'] = False
                compatibility['message'] = f"⚠ {method} NOT RECOMMENDED for {target_algorithm}"
                compatibility['level'] = 'warning'
            else:
                compatibility['compatible'] = True
                compatibility['message'] = f"○ {method} is ACCEPTABLE for {target_algorithm}"
                compatibility['level'] = 'acceptable'
            
            return compatibility
        
        def scale_unified(df, train_indices, method, params):
            """Scale data using train-aware unified approach"""
            logger.info("="*80)
            logger.info("UNIFIED SCALING & NORMALIZATION")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            logger.info(f"Train indices: {len(train_indices)} rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping")
                return df, {
                    'method': 'none',
                    'n_columns_scaled': 0
                }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns")
                return df, {
                    'method': method,
                    'n_columns_scaled': 0,
                    'warning': 'no_numeric_columns'
                }
            
            logger.info(f"Scaling {len(numeric_cols)} numeric columns")
            
            df_scaled = df.copy()
            
            # Extract TRAIN subset for fitting
            train_df = df.iloc[train_indices]
            
            metadata = {
                'method': method,
                'params': params,
                'n_columns_scaled': len(numeric_cols),
                'scaled_columns': numeric_cols
            }
            
            # METHOD 1: Standard Scaling
            if method in ['standard', 'zscore']:
                scaler = StandardScaler()
                
                # Fit on TRAIN only
                scaler.fit(train_df[numeric_cols])
                
                # Transform ALL data
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                
                logger.info("Standard scaling applied")
                metadata['means'] = scaler.mean_.tolist()[:5]  # Sample
                metadata['stds'] = scaler.scale_.tolist()[:5]
            
            # METHOD 2: Min-Max Scaling
            elif method == 'minmax':
                feature_range = params.get('feature_range', [0, 1])
                scaler = MinMaxScaler(feature_range=tuple(feature_range))
                
                # Fit on TRAIN only
                scaler.fit(train_df[numeric_cols])
                
                # Transform ALL data
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                
                logger.info(f"Min-Max scaling to {feature_range}")
                metadata['feature_range'] = feature_range
            
            # METHOD 3: Robust Scaling
            elif method == 'robust':
                quantile_range = params.get('quantile_range', [25.0, 75.0])
                scaler = RobustScaler(quantile_range=tuple(quantile_range))
                
                # Fit on TRAIN only
                scaler.fit(train_df[numeric_cols])
                
                # Transform ALL data
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                
                logger.info(f"Robust scaling (quantile {quantile_range})")
                metadata['quantile_range'] = quantile_range
            
            # METHOD 4: MaxAbs Scaling
            elif method == 'maxabs':
                scaler = MaxAbsScaler()
                
                # Fit on TRAIN only
                scaler.fit(train_df[numeric_cols])
                
                # Transform ALL data
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                
                logger.info("MaxAbs scaling applied")
            
            # METHOD 5: L1 Column-wise Normalization (CLUSTERING-FRIENDLY)
            elif method == 'l1_columnwise':
                logger.info("L1 COLUMN-WISE normalization (clustering-friendly)")
                
                # Calculate L1 norms from TRAIN only
                train_norms = {}
                for col in numeric_cols:
                    train_norms[col] = np.abs(train_df[col]).sum()
                
                # Apply to ALL data
                for col in numeric_cols:
                    if train_norms[col] > 0:
                        df_scaled[col] = df[col] / train_norms[col]
                
                metadata['normalization_axis'] = 'columns'
                metadata['train_norms_sample'] = {k: v for k, v in list(train_norms.items())[:5]}
                logger.info("Applied L1 per column")
            
            # METHOD 6: L2 Column-wise Normalization (CLUSTERING-FRIENDLY)
            elif method == 'l2_columnwise':
                logger.info("L2 COLUMN-WISE normalization (clustering-friendly)")
                
                # Calculate L2 norms from TRAIN only
                train_norms = {}
                for col in numeric_cols:
                    train_norms[col] = np.sqrt((train_df[col] ** 2).sum())
                
                # Apply to ALL data
                for col in numeric_cols:
                    if train_norms[col] > 0:
                        df_scaled[col] = df[col] / train_norms[col]
                
                metadata['normalization_axis'] = 'columns'
                metadata['train_norms_sample'] = {k: v for k, v in list(train_norms.items())[:5]}
                logger.info("Applied L2 per column")
            
            # METHOD 7: Quantile Transformer (Uniform)
            elif method == 'quantile_uniform':
                n_quantiles = min(params.get('n_quantiles', 1000), len(train_df))
                scaler = QuantileTransformer(
                    output_distribution='uniform',
                    n_quantiles=n_quantiles,
                    random_state=42
                )
                
                # Fit on TRAIN only
                scaler.fit(train_df[numeric_cols])
                
                # Transform ALL data
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                
                logger.info(f"Quantile uniform (n={n_quantiles})")
                metadata['n_quantiles'] = n_quantiles
            
            # METHOD 8: Quantile Transformer (Normal)
            elif method == 'quantile_normal':
                n_quantiles = min(params.get('n_quantiles', 1000), len(train_df))
                scaler = QuantileTransformer(
                    output_distribution='normal',
                    n_quantiles=n_quantiles,
                    random_state=42
                )
                
                # Fit on TRAIN only
                scaler.fit(train_df[numeric_cols])
                
                # Transform ALL data
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                
                logger.info(f"Quantile normal (n={n_quantiles})")
                metadata['n_quantiles'] = n_quantiles
            
            # METHOD 9: Power Transformer (Yeo-Johnson)
            elif method == 'power_yeo-johnson':
                standardize = params.get('standardize', True)
                scaler = PowerTransformer(method='yeo-johnson', standardize=standardize)
                
                # Fit on TRAIN only
                scaler.fit(train_df[numeric_cols])
                
                # Transform ALL data
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                
                logger.info("Power transform (Yeo-Johnson)")
                metadata['power_method'] = 'yeo-johnson'
            
            # METHOD 10: Power Transformer (Box-Cox)
            elif method == 'power_box-cox':
                standardize = params.get('standardize', True)
                
                if (train_df[numeric_cols] > 0).all().all():
                    scaler = PowerTransformer(method='box-cox', standardize=standardize)
                    scaler.fit(train_df[numeric_cols])
                    df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                    logger.info("Power transform (Box-Cox)")
                    metadata['power_method'] = 'box-cox'
                else:
                    logger.warning("Box-Cox requires positive, using Yeo-Johnson")
                    scaler = PowerTransformer(method='yeo-johnson', standardize=standardize)
                    scaler.fit(train_df[numeric_cols])
                    df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                    logger.info("Fallback: Yeo-Johnson")
                    metadata['power_method'] = 'yeo-johnson (fallback)'
            
            else:
                raise ValueError(
                    f"Unknown method: {method}. "
                    f"Available: standard, minmax, robust, maxabs, "
                    f"l1_columnwise, l2_columnwise, quantile_uniform, quantile_normal, "
                    f"power_yeo-johnson, power_box-cox, none"
                )
            
            logger.info(f"\nScaling completed: {df_scaled.shape}")
            logger.info("")
            
            return df_scaled, metadata
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--method", default='standard')
            parser.add_argument("--method_params", default='{}')
            parser.add_argument("--target_algorithm", default='none')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_metadata", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED SCALING & NORMALIZATION")
            logger.info("="*80)
            logger.info(f"Method: {args.method}")
            logger.info(f"Target algorithm: {args.target_algorithm}")
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_metadata)
                
                # Parse parameters
                params = json.loads(args.method_params)
                
                # Load data
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Check algorithm compatibility
                compatibility = check_algorithm_compatibility(
                    method=args.method,
                    target_algorithm=args.target_algorithm
                )
                
                if not compatibility['compatible']:
                    logger.warning("="*80)
                    logger.warning("ALGORITHM COMPATIBILITY WARNING")
                    logger.warning("="*80)
                    logger.warning(compatibility['message'])
                    logger.warning(f"Recommended: {compatibility['recommended_scalers']}")
                    logger.warning(f"Reason: {compatibility['reason']}")
                    logger.warning("="*80)
                    logger.warning("")
                else:
                    logger.info(compatibility['message'])
                    logger.info("")
                
                # Scale
                df_scaled, scaling_metadata = scale_unified(
                    df=df,
                    train_indices=train_indices,
                    method=args.method,
                    params=params
                )
                
                # Save
                df_scaled.to_csv(args.output_preprocessed_data, index=False)
                logger.info(f"Scaled data saved")
                
                # Add compatibility to metadata
                full_metadata = {
                    'scaling': scaling_metadata,
                    'algorithm_compatibility': compatibility
                }
                
                with open(args.output_metadata, 'w') as f:
                    json.dump(full_metadata, f, indent=2)
                logger.info(f"Metadata saved")
                
                logger.info("")
                logger.info("="*80)
                logger.info("SCALING COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.method}")
                logger.info(f"Columns scaled: {scaling_metadata['n_columns_scaled']}")
                logger.info(f"Algorithm compatibility: {compatibility['level']}")
                logger.info(f"Final shape: {df_scaled.shape}")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --target_algorithm
      - {inputValue: target_algorithm}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_metadata
      - {outputPath: metadata}
