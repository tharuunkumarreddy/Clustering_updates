name: Generic Scaling & Normalization Component - All ML Algorithms
description: Train-aware scaling for unified preprocessing supporting ALL ML algorithms (supervised and unsupervised). Fits scaling on train indices only, transforms all combined data. Supports 13 scaling methods. Provides algorithm-specific compatibility recommendations for 30+ algorithms. Outputs fitted scaler for production inference.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (Parquet or CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: method
    type: String
    description: 'Scaling method: standard, minmax, robust, maxabs, l1_columnwise, l2_columnwise, quantile_uniform, quantile_normal, power_yeo-johnson, power_box-cox, none'
    default: 'standard'
  - name: method_params
    type: String
    description: 'Method parameters as JSON. Examples: {"feature_range":[0,1]}, {"quantile_range":[25,75]}'
    default: '{}'
  - name: target_algorithm
    type: String
    description: 'Target algorithm for scaler compatibility check. Clustering: KMeans, DBSCAN, HDBSCAN, AgglomerativeClustering, BIRCH, GaussianMixture, BayesianGaussianMixture, FuzzyCMeans, MeanShift, AffinityPropagation, SpectralClustering, MiniBatchKMeans, BisectingKMeans, KMedoids, OPTICS. Supervised: RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, XGBClassifier, XGBRegressor, LGBMClassifier, LGBMRegressor, SVC, SVR, LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, KNeighborsClassifier, KNeighborsRegressor, DecisionTreeClassifier, DecisionTreeRegressor, MLPClassifier, MLPRegressor, AdaBoostClassifier, AdaBoostRegressor, GaussianNB. Use "none" to skip check.'
    default: 'none'

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Scaled dataset (PARQUET format - 10x faster than CSV)'
  - name: metadata
    type: Data
    description: 'Scaling metadata with algorithm compatibility report (JSON)'
  - name: fitted_scaler
    type: Data
    description: 'Fitted scaler object for production inference (PKL). Use this to transform new data with same scaling parameters.'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        import pickle
        from sklearn.preprocessing import (
            StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler,
            QuantileTransformer, PowerTransformer
        )
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('scaling_unified')
        
        SCALER_RECOMMENDATIONS = {
            # Centroid-Based Algorithms
            'KMeans': {
                'recommended': ['standard', 'minmax'],
                'avoid': ['robust'],
                'reason': 'Distance-based algorithm sensitive to scale. Needs consistent scaling across features.'
            },
            'MiniBatchKMeans': {
                'recommended': ['standard', 'minmax'],
                'avoid': ['robust'],
                'reason': 'Same as KMeans - distance-based, benefits from uniform scaling.'
            },
            'BisectingKMeans': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Hierarchical K-means variant, same scaling needs as KMeans.'
            },
            'KMedoids': {
                'recommended': ['robust', 'minmax', 'standard'],
                'avoid': [],
                'reason': 'Medoid-based, more robust to outliers than K-means. Works well with various scalings.'
            },
            
            # Density-Based Algorithms
            'DBSCAN': {
                'recommended': ['robust', 'minmax', 'standard'],
                'avoid': [],
                'reason': 'Density-based, benefits from robust scaling when outliers present.'
            },
            'OPTICS': {
                'recommended': ['robust', 'standard'],
                'avoid': [],
                'reason': 'Hierarchical density-based, robust scaling helps with varying densities.'
            },
            'HDBSCAN': {
                'recommended': ['robust', 'standard'],
                'avoid': [],
                'reason': 'Hierarchical density clustering, robust scaling optimal for varying densities.'
            },
            
            # Hierarchical Algorithms
            'AgglomerativeClustering': {
                'recommended': ['standard', 'minmax', 'robust'],
                'avoid': [],
                'reason': 'Distance-based hierarchical method. Any scaling works but affects dendrogram structure.'
            },
            'BIRCH': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Hierarchical using cluster features. Standard scaling preserves tree structure.'
            },
            
            # Distribution-Based Algorithms
            'GaussianMixture': {
                'recommended': ['standard'],
                'avoid': ['robust', 'minmax'],
                'reason': 'Assumes Gaussian distributions. Standard scaling preserves distributional properties.'
            },
            'BayesianGaussianMixture': {
                'recommended': ['standard'],
                'avoid': ['robust', 'minmax'],
                'reason': 'Bayesian GMM also assumes Gaussian distributions. Standard scaling optimal.'
            },
            
            # Fuzzy Algorithms
            'FuzzyCMeans': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Distance-based fuzzy clustering similar to K-means. Uniform scaling important.'
            },
            
            # Other Algorithms
            'MeanShift': {
                'recommended': ['standard', 'robust'],
                'avoid': [],
                'reason': 'Mode-seeking algorithm. Robust scaling helps with bandwidth estimation.'
            },
            'AffinityPropagation': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Similarity-based message passing. Benefits from normalized features for affinity matrix.'
            },
            'SpectralClustering': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Graph-based using eigenvectors. Standard scaling optimal for affinity matrix computation.'
            },
            # ── TREE-BASED (scale-invariant) ─────────────────────────────────
            'RandomForestClassifier': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'Tree-based — splits on thresholds, not distances. Scaling has no effect on accuracy.'
            },
            'RandomForestRegressor': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'Tree-based — scale-invariant. Scaling unnecessary but harmless.'
            },
            'GradientBoostingClassifier': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'Boosted trees — scale-invariant. Scaling has no effect.'
            },
            'GradientBoostingRegressor': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'Boosted trees — scale-invariant.'
            },
            'XGBClassifier': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'XGBoost is tree-based and scale-invariant.'
            },
            'XGBRegressor': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'XGBoost is tree-based and scale-invariant.'
            },
            'LGBMClassifier': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'LightGBM is tree-based and scale-invariant.'
            },
            'LGBMRegressor': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'LightGBM is tree-based and scale-invariant.'
            },
            'AdaBoostClassifier': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'AdaBoost with decision stumps is scale-invariant.'
            },
            'AdaBoostRegressor': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'AdaBoost with decision stumps is scale-invariant.'
            },
            'DecisionTreeClassifier': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'Decision trees are scale-invariant.'
            },
            'DecisionTreeRegressor': {
                'recommended': ['none'],
                'avoid': [],
                'reason': 'Decision trees are scale-invariant.'
            },
            # ── SVM ──────────────────────────────────────────────────────────
            'SVC': {
                'recommended': ['standard', 'minmax'],
                'avoid': ['robust'],
                'reason': 'SVM uses kernel distances — very sensitive to feature scale. Standard scaling is critical.'
            },
            'SVR': {
                'recommended': ['standard', 'minmax'],
                'avoid': ['robust'],
                'reason': 'SVR is scale-sensitive. Standard or MinMax scaling strongly recommended.'
            },
            # ── LINEAR MODELS ─────────────────────────────────────────────────
            'LogisticRegression': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Gradient-based optimisation converges faster with scaled features.'
            },
            'LinearRegression': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Scaling improves numerical stability and coefficient interpretability.'
            },
            'Ridge': {
                'recommended': ['standard'],
                'avoid': [],
                'reason': 'L2 regularisation penalises large coefficients — scaling ensures fair penalisation.'
            },
            'Lasso': {
                'recommended': ['standard'],
                'avoid': [],
                'reason': 'L1 regularisation — scaling ensures fair feature selection.'
            },
            'ElasticNet': {
                'recommended': ['standard'],
                'avoid': [],
                'reason': 'Combined L1/L2 — scaling ensures fair regularisation across features.'
            },
            # ── NEIGHBORS ────────────────────────────────────────────────────
            'KNeighborsClassifier': {
                'recommended': ['standard', 'minmax', 'robust'],
                'avoid': [],
                'reason': 'KNN uses distances directly — scaling is critical for correct neighbor computation.'
            },
            'KNeighborsRegressor': {
                'recommended': ['standard', 'minmax', 'robust'],
                'avoid': [],
                'reason': 'KNN uses distances directly — scaling is critical.'
            },
            # ── NEURAL NETWORKS ───────────────────────────────────────────────
            'MLPClassifier': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Neural networks require scaled inputs for stable gradient descent.'
            },
            'MLPRegressor': {
                'recommended': ['standard', 'minmax'],
                'avoid': [],
                'reason': 'Neural networks require scaled inputs for stable gradient descent.'
            },
            # ── NAIVE BAYES ───────────────────────────────────────────────────
            'GaussianNB': {
                'recommended': ['none', 'standard'],
                'avoid': ['minmax'],
                'reason': 'GaussianNB models feature distributions — scaling changes the distribution shape.'
            }
        }
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def detect_file_type(file_path):
            # Detect file type by MAGIC BYTES first (not extension).
            # CRITICAL FIX: Kubernetes passes files without extensions
            try:
                with open(file_path, 'rb') as f:
                    header = f.read(8)
                if header[:4] == b'PAR1':
                    logger.info("[OK] Detected: Parquet (PAR1 magic bytes)")
                    return 'parquet'
                if header[1:6] == b'NUMPY':
                    logger.info("[OK] Detected: NumPy array")
                    return 'numpy'
                if header[:2] in [b'PK', b'\x80\x04']:
                    logger.info("[OK] Detected: Pickle/ZIP")
                    return 'pickle'
                try:
                    text_start = open(file_path, 'r', errors='replace').read(512)
                    if text_start.strip().startswith('{') or text_start.strip().startswith('['):
                        return 'json'
                    return 'csv'
                except Exception:
                    return 'csv'
            except Exception as e:
                logger.warning("Magic byte detection failed: " + str(e) + ", defaulting to CSV")
                return 'csv'

        def load_data(input_path):
            # Load data with MAGIC BYTES detection first.
            # CRITICAL FIX: Kubernetes strips file extensions.
            logger.info("Loading dataset from: " + input_path)
            ext = Path(input_path).suffix.lower()
            logger.info("File extension: '" + ext + "' (may be empty on Kubernetes)")
            detected_type = detect_file_type(input_path)
            try:
                if ext in ['.parquet', '.pq'] or detected_type == 'parquet':
                    logger.info("Loading as Parquet...")
                    df = pd.read_parquet(input_path, engine='pyarrow')
                    logger.info("[OK] Loaded Parquet: " + str(df.shape[0]) + " rows x " + str(df.shape[1]) + " columns")
                    return df
                logger.info("Loading as CSV...")
                for enc in ['utf-8', 'latin-1', 'cp1252']:
                    try:
                        df = pd.read_csv(input_path, encoding=enc)
                        logger.info("[OK] Loaded CSV (" + enc + "): " + str(df.shape[0]) + " rows x " + str(df.shape[1]) + " columns")
                        return df
                    except UnicodeDecodeError:
                        continue
                raise ValueError("Could not decode file with any supported encoding")
            except Exception as e:
                logger.error("Error loading data: " + str(e))
                raise
        
        def load_indices(indices_path):
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def check_algorithm_compatibility(method, target_algorithm):
            if target_algorithm == 'none' or target_algorithm not in SCALER_RECOMMENDATIONS:
                return {
                    'compatible': True,
                    'message': 'No algorithm specified',
                    'level': 'none',
                    'algorithm': target_algorithm,
                    'scaler': method
                }
            
            rec = SCALER_RECOMMENDATIONS[target_algorithm]
            
            compatibility = {
                'algorithm': target_algorithm,
                'scaler': method,
                'recommended_scalers': rec['recommended'],
                'reason': rec['reason']
            }
            
            if method in rec['recommended']:
                compatibility['compatible'] = True
                compatibility['message'] = "[OK] " + method + " is RECOMMENDED for " + target_algorithm
                compatibility['level'] = 'optimal'
            elif method in rec['avoid']:
                compatibility['compatible'] = False
                compatibility['message'] = "[WARN] " + method + " NOT RECOMMENDED for " + target_algorithm
                compatibility['level'] = 'warning'
            else:
                compatibility['compatible'] = True
                compatibility['message'] = "[OK] " + method + " is ACCEPTABLE for " + target_algorithm
                compatibility['level'] = 'acceptable'
            
            return compatibility
        
        def scale_unified(df, train_indices, method, params):
            logger.info("="*80)
            logger.info("UNIFIED SCALING & NORMALIZATION")
            logger.info("="*80)
            logger.info("Method: " + str(method))
            logger.info("Train indices: " + str(len(train_indices)) + " rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping")
                return df, {
                    'method': 'none',
                    'n_columns_scaled': 0
                }, None
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns")
                return df, {
                    'method': method,
                    'n_columns_scaled': 0,
                    'warning': 'no_numeric_columns'
                }, None
            
            logger.info("Scaling " + str(len(numeric_cols)) + " numeric columns")
            
            df_scaled = df.copy()
            train_df = df.iloc[train_indices]
            
            metadata = {
                'method': method,
                'params': params,
                'n_columns_scaled': len(numeric_cols),
                'scaled_columns': numeric_cols
            }
            
            scaler_obj = None
            
            if method in ['standard', 'zscore']:
                scaler = StandardScaler()
                scaler.fit(train_df[numeric_cols])
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                logger.info("Standard scaling applied")
                metadata['means'] = scaler.mean_.tolist()[:5]
                metadata['stds'] = scaler.scale_.tolist()[:5]
                scaler_obj = {
                    'scaler': scaler,
                    'columns': numeric_cols,
                    'method': 'standard'
                }
            
            elif method == 'minmax':
                feature_range = params.get('feature_range', [0, 1])
                scaler = MinMaxScaler(feature_range=tuple(feature_range))
                scaler.fit(train_df[numeric_cols])
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                logger.info("Min-Max scaling to " + str(feature_range))
                metadata['feature_range'] = feature_range
                scaler_obj = {
                    'scaler': scaler,
                    'columns': numeric_cols,
                    'method': 'minmax'
                }
            
            elif method == 'robust':
                quantile_range = params.get('quantile_range', [25.0, 75.0])
                scaler = RobustScaler(quantile_range=tuple(quantile_range))
                scaler.fit(train_df[numeric_cols])
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                logger.info("Robust scaling (quantile " + str(quantile_range) + ")")
                metadata['quantile_range'] = quantile_range
                scaler_obj = {
                    'scaler': scaler,
                    'columns': numeric_cols,
                    'method': 'robust'
                }
            
            elif method == 'maxabs':
                scaler = MaxAbsScaler()
                scaler.fit(train_df[numeric_cols])
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                logger.info("MaxAbs scaling applied")
                scaler_obj = {
                    'scaler': scaler,
                    'columns': numeric_cols,
                    'method': 'maxabs'
                }
            
            elif method == 'l1_columnwise':
                logger.info("L1 COLUMN-WISE normalization (clustering-friendly)")
                train_norms = {}
                for col in numeric_cols:
                    train_norms[col] = np.abs(train_df[col]).sum()
                for col in numeric_cols:
                    if train_norms[col] > 0:
                        df_scaled[col] = df[col] / train_norms[col]
                metadata['normalization_axis'] = 'columns'
                metadata['train_norms_sample'] = {k: v for k, v in list(train_norms.items())[:5]}
                logger.info("Applied L1 per column")
                scaler_obj = {
                    'norms': train_norms,
                    'columns': numeric_cols,
                    'method': 'l1_columnwise'
                }
            
            elif method == 'l2_columnwise':
                logger.info("L2 COLUMN-WISE normalization (clustering-friendly)")
                train_norms = {}
                for col in numeric_cols:
                    train_norms[col] = np.sqrt((train_df[col] ** 2).sum())
                for col in numeric_cols:
                    if train_norms[col] > 0:
                        df_scaled[col] = df[col] / train_norms[col]
                metadata['normalization_axis'] = 'columns'
                metadata['train_norms_sample'] = {k: v for k, v in list(train_norms.items())[:5]}
                logger.info("Applied L2 per column")
                scaler_obj = {
                    'norms': train_norms,
                    'columns': numeric_cols,
                    'method': 'l2_columnwise'
                }
            
            elif method == 'quantile_uniform':
                n_quantiles = min(params.get('n_quantiles', 1000), len(train_df))
                scaler = QuantileTransformer(
                    output_distribution='uniform',
                    n_quantiles=n_quantiles,
                    random_state=42
                )
                scaler.fit(train_df[numeric_cols])
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                logger.info("Quantile uniform (n=" + str(n_quantiles) + ")")
                metadata['n_quantiles'] = n_quantiles
                scaler_obj = {
                    'scaler': scaler,
                    'columns': numeric_cols,
                    'method': 'quantile_uniform'
                }
            
            elif method == 'quantile_normal':
                n_quantiles = min(params.get('n_quantiles', 1000), len(train_df))
                scaler = QuantileTransformer(
                    output_distribution='normal',
                    n_quantiles=n_quantiles,
                    random_state=42
                )
                scaler.fit(train_df[numeric_cols])
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                logger.info("Quantile normal (n=" + str(n_quantiles) + ")")
                metadata['n_quantiles'] = n_quantiles
                scaler_obj = {
                    'scaler': scaler,
                    'columns': numeric_cols,
                    'method': 'quantile_normal'
                }
            
            elif method == 'power_yeo-johnson':
                standardize = params.get('standardize', True)
                scaler = PowerTransformer(method='yeo-johnson', standardize=standardize)
                scaler.fit(train_df[numeric_cols])
                df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                logger.info("Power transform (Yeo-Johnson)")
                metadata['power_method'] = 'yeo-johnson'
                scaler_obj = {
                    'scaler': scaler,
                    'columns': numeric_cols,
                    'method': 'power_yeo-johnson'
                }
            
            elif method == 'power_box-cox':
                standardize = params.get('standardize', True)
                if (train_df[numeric_cols] > 0).all().all():
                    scaler = PowerTransformer(method='box-cox', standardize=standardize)
                    scaler.fit(train_df[numeric_cols])
                    df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                    logger.info("Power transform (Box-Cox)")
                    metadata['power_method'] = 'box-cox'
                    scaler_obj = {
                        'scaler': scaler,
                        'columns': numeric_cols,
                        'method': 'power_box-cox'
                    }
                else:
                    logger.warning("Box-Cox requires positive, using Yeo-Johnson")
                    scaler = PowerTransformer(method='yeo-johnson', standardize=standardize)
                    scaler.fit(train_df[numeric_cols])
                    df_scaled[numeric_cols] = scaler.transform(df[numeric_cols])
                    logger.info("Fallback: Yeo-Johnson")
                    metadata['power_method'] = 'yeo-johnson (fallback)'
                    scaler_obj = {
                        'scaler': scaler,
                        'columns': numeric_cols,
                        'method': 'power_yeo-johnson'
                    }
            
            else:
                raise ValueError(
                    "Unknown method: " + method + ". "
                    "Available: standard, minmax, robust, maxabs, "
                    "l1_columnwise, l2_columnwise, quantile_uniform, quantile_normal, "
                    "power_yeo-johnson, power_box-cox, none"
                )
            
            logger.info("")
            logger.info("Scaling completed: " + str(df_scaled.shape))
            if scaler_obj:
                logger.info("[OK] Fitted scaler saved for production inference")
            logger.info("")
            
            return df_scaled, metadata, scaler_obj
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--method", default='standard')
            parser.add_argument("--method_params", default='{}')
            parser.add_argument("--target_algorithm", default='none')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_metadata", required=True)
            parser.add_argument("--output_fitted_scaler", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED SCALING & NORMALIZATION (WITH FITTED SCALER)")
            logger.info("="*80)
            logger.info("Method: " + args.method)
            logger.info("Target algorithm: " + args.target_algorithm)
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_metadata)
                ensure_directory_exists(args.output_fitted_scaler)
                
                params = json.loads(args.method_params)
                
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                compatibility = check_algorithm_compatibility(
                    method=args.method,
                    target_algorithm=args.target_algorithm
                )
                
                if not compatibility['compatible']:
                    logger.warning("="*80)
                    logger.warning("ALGORITHM COMPATIBILITY WARNING")
                    logger.warning("="*80)
                    logger.warning(compatibility['message'])
                    logger.warning("Recommended: " + str(compatibility.get('recommended_scalers', [])))
                    logger.warning("Reason: " + compatibility.get('reason', 'N/A'))
                    logger.warning("="*80)
                    logger.warning("")
                else:
                    logger.info(compatibility['message'])
                    logger.info("")
                
                df_scaled, scaling_metadata, scaler_obj = scale_unified(
                    df=df,
                    train_indices=train_indices,
                    method=args.method,
                    params=params
                )
                
                # Save scaled data as PARQUET
                df_scaled.to_parquet(args.output_preprocessed_data, index=False, engine='pyarrow', compression='snappy')
                logger.info("Scaled data saved as Parquet")
                
                # Save metadata
                full_metadata = {
                    'scaling': scaling_metadata,
                    'algorithm_compatibility': compatibility
                }
                
                with open(args.output_metadata, 'w') as f:
                    json.dump(full_metadata, f, indent=2)
                logger.info("Metadata saved")
                
                # Save fitted scaler
                if scaler_obj:
                    with open(args.output_fitted_scaler, 'wb') as f:
                        pickle.dump(scaler_obj, f)
                    logger.info("Fitted scaler saved: " + args.output_fitted_scaler)
                    logger.info("  Use this for production inference!")
                else:
                    # Create empty placeholder if no scaler
                    with open(args.output_fitted_scaler, 'wb') as f:
                        pickle.dump({'method': 'none', 'scaler': None}, f)
                    logger.info("No scaler (method=none)")
                
                logger.info("")
                logger.info("="*80)
                logger.info("SCALING COMPLETED")
                logger.info("="*80)
                logger.info("Method: " + args.method)
                logger.info("Columns scaled: " + str(scaling_metadata['n_columns_scaled']))
                logger.info("Algorithm compatibility: " + str(compatibility.get('level', 'none')))
                logger.info("Final shape: " + str(df_scaled.shape))
                logger.info("="*80)
                
            except Exception as e:
                logger.error("ERROR: " + str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --target_algorithm
      - {inputValue: target_algorithm}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_metadata
      - {outputPath: metadata}
      - --output_fitted_scaler
      - {outputPath: fitted_scaler}
