name: Unified Scaling & Normalization - PARQUET OPTIMIZED (With Fitted Scaler)
description: Train-aware scaling for unified preprocessing with algorithm-specific recommendations. Supports 13 methods. Loads Parquet/CSV, SAVES AS PARQUET. OUTPUTS FITTED SCALER for production!

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (Parquet or CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON)'
  - name: method
    type: String
    description: 'Scaling method: standard, minmax, robust, maxabs, l1_columnwise, l2_columnwise, quantile_uniform, quantile_normal, power_yeo-johnson, power_box-cox, none'
    default: 'standard'
  - name: method_params
    type: String
    description: 'Method parameters (JSON)'
    default: '{}'
  - name: target_algorithm
    type: String
    description: 'Target algorithm for compatibility check'
    default: 'none'

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Scaled dataset (PARQUET - 10x faster)'
  - name: metadata
    type: Data
    description: 'Scaling metadata with compatibility (JSON)'
  - name: fitted_scaler
    type: Data
    description: 'Fitted scaler for production (PKL)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command: [python3, -u, -c]
    args:
      - |
        import os, sys, json, argparse, logging, pandas as pd, numpy as np, pickle
        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer, PowerTransformer
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('scaling_unified')
        
        # [ALGORITHM COMPATIBILITY DATABASE - PRESERVED IN FULL]
        SCALER_RECOMMENDATIONS = {
            'KMeans': {'recommended': ['standard', 'minmax'], 'avoid': ['robust'], 'reason': 'Distance-based, needs consistent scaling'},
            'MiniBatchKMeans': {'recommended': ['standard', 'minmax'], 'avoid': ['robust'], 'reason': 'Same as KMeans'},
            'BisectingKMeans': {'recommended': ['standard', 'minmax'], 'avoid': [], 'reason': 'Hierarchical K-means variant'},
            'KMedoids': {'recommended': ['robust', 'minmax', 'standard'], 'avoid': [], 'reason': 'Medoid-based, robust to outliers'},
            'DBSCAN': {'recommended': ['robust', 'minmax', 'standard'], 'avoid': [], 'reason': 'Density-based, benefits from robust scaling'},
            'OPTICS': {'recommended': ['robust', 'standard'], 'avoid': [], 'reason': 'Hierarchical density-based'},
            'HDBSCAN': {'recommended': ['robust', 'standard'], 'avoid': [], 'reason': 'Hierarchical density clustering'},
            'AgglomerativeClustering': {'recommended': ['standard', 'minmax', 'robust'], 'avoid': [], 'reason': 'Distance-based hierarchical'},
            'BIRCH': {'recommended': ['standard', 'minmax'], 'avoid': [], 'reason': 'Hierarchical using cluster features'},
            'GaussianMixture': {'recommended': ['standard'], 'avoid': ['robust', 'minmax'], 'reason': 'Assumes Gaussian distributions'},
            'BayesianGaussianMixture': {'recommended': ['standard'], 'avoid': ['robust', 'minmax'], 'reason': 'Bayesian GMM, Gaussian assumption'},
            'FuzzyCMeans': {'recommended': ['standard', 'minmax'], 'avoid': [], 'reason': 'Distance-based fuzzy clustering'},
            'MeanShift': {'recommended': ['standard', 'robust'], 'avoid': [], 'reason': 'Mode-seeking, robust helps bandwidth'},
            'AffinityPropagation': {'recommended': ['standard', 'minmax'], 'avoid': [], 'reason': 'Similarity-based message passing'},
            'SpectralClustering': {'recommended': ['standard', 'minmax'], 'avoid': [], 'reason': 'Graph-based using eigenvectors'}
        }
        
        def ensure_directory_exists(p):
            d = os.path.dirname(p)
            if d: os.makedirs(d, exist_ok=True)
        
        def load_data(p):
            logger.info(f"Loading: {p}")
            ext = Path(p).suffix.lower()
            df = pd.read_parquet(p, engine='pyarrow') if ext in ['.parquet', '.pq'] else pd.read_csv(p)
            logger.info(f"✓ Loaded {ext}: {df.shape}")
            return df
        
        def load_indices(p):
            with open(p) as f: return json.load(f)
        
        def check_compat(m, alg):
            if alg == 'none' or alg not in SCALER_RECOMMENDATIONS:
                return {'compatible': True, 'message': 'No algorithm', 'level': 'none', 'algorithm': alg, 'scaler': m}
            rec = SCALER_RECOMMENDATIONS[alg]
            comp = {'algorithm': alg, 'scaler': m, 'recommended_scalers': rec['recommended'], 'reason': rec['reason']}
            if m in rec['recommended']:
                comp.update({'compatible': True, 'message': f"[OK] {m} RECOMMENDED for {alg}", 'level': 'optimal'})
            elif m in rec['avoid']:
                comp.update({'compatible': False, 'message': f"[WARN] {m} NOT RECOMMENDED for {alg}", 'level': 'warning'})
            else:
                comp.update({'compatible': True, 'message': f"[OK] {m} ACCEPTABLE for {alg}", 'level': 'acceptable'})
            return comp
        
        def scale_unified(df, ti, m, p):
            logger.info("="*80 + "\nUNIFIED SCALING\n" + "="*80)
            logger.info(f"Method: {m}, Train: {len(ti)} rows\n")
            if m == 'none':
                logger.info("Skipping (method=none)")
                return df, {'method': 'none', 'n_columns_scaled': 0}, None
            nc = df.select_dtypes(include=[np.number]).columns.tolist()
            if not nc:
                logger.warning("No numeric columns")
                return df, {'method': m, 'n_columns_scaled': 0, 'warning': 'no_numeric'}, None
            logger.info(f"Scaling {len(nc)} columns")
            ds, td = df.copy(), df.iloc[ti]
            meta = {'method': m, 'params': p, 'n_columns_scaled': len(nc), 'scaled_columns': nc}
            so = None
            if m in ['standard', 'zscore']:
                sc = StandardScaler()
                sc.fit(td[nc])
                ds[nc] = sc.transform(df[nc])
                logger.info("✓ Standard")
                meta.update({'means': sc.mean_.tolist()[:5], 'stds': sc.scale_.tolist()[:5]})
                so = {'scaler': sc, 'columns': nc, 'method': 'standard'}
            elif m == 'minmax':
                fr = p.get('feature_range', [0,1])
                sc = MinMaxScaler(feature_range=tuple(fr))
                sc.fit(td[nc])
                ds[nc] = sc.transform(df[nc])
                logger.info(f"✓ MinMax {fr}")
                meta['feature_range'] = fr
                so = {'scaler': sc, 'columns': nc, 'method': 'minmax'}
            elif m == 'robust':
                qr = p.get('quantile_range', [25.0,75.0])
                sc = RobustScaler(quantile_range=tuple(qr))
                sc.fit(td[nc])
                ds[nc] = sc.transform(df[nc])
                logger.info(f"✓ Robust {qr}")
                meta['quantile_range'] = qr
                so = {'scaler': sc, 'columns': nc, 'method': 'robust'}
            elif m == 'maxabs':
                sc = MaxAbsScaler()
                sc.fit(td[nc])
                ds[nc] = sc.transform(df[nc])
                logger.info("✓ MaxAbs")
                so = {'scaler': sc, 'columns': nc, 'method': 'maxabs'}
            elif m == 'l1_columnwise':
                logger.info("L1 column-wise")
                tn = {c: np.abs(td[c]).sum() for c in nc}
                for c in nc:
                    if tn[c] > 0: ds[c] = df[c] / tn[c]
                meta.update({'normalization_axis': 'columns', 'train_norms_sample': dict(list(tn.items())[:5])})
                logger.info("✓ L1")
                so = {'norms': tn, 'columns': nc, 'method': 'l1_columnwise'}
            elif m == 'l2_columnwise':
                logger.info("L2 column-wise")
                tn = {c: np.sqrt((td[c]**2).sum()) for c in nc}
                for c in nc:
                    if tn[c] > 0: ds[c] = df[c] / tn[c]
                meta.update({'normalization_axis': 'columns', 'train_norms_sample': dict(list(tn.items())[:5])})
                logger.info("✓ L2")
                so = {'norms': tn, 'columns': nc, 'method': 'l2_columnwise'}
            elif m == 'quantile_uniform':
                nq = min(p.get('n_quantiles', 1000), len(td))
                sc = QuantileTransformer(output_distribution='uniform', n_quantiles=nq, random_state=42)
                sc.fit(td[nc])
                ds[nc] = sc.transform(df[nc])
                logger.info(f"✓ Quantile uniform n={nq}")
                meta['n_quantiles'] = nq
                so = {'scaler': sc, 'columns': nc, 'method': 'quantile_uniform'}
            elif m == 'quantile_normal':
                nq = min(p.get('n_quantiles', 1000), len(td))
                sc = QuantileTransformer(output_distribution='normal', n_quantiles=nq, random_state=42)
                sc.fit(td[nc])
                ds[nc] = sc.transform(df[nc])
                logger.info(f"✓ Quantile normal n={nq}")
                meta['n_quantiles'] = nq
                so = {'scaler': sc, 'columns': nc, 'method': 'quantile_normal'}
            elif m == 'power_yeo-johnson':
                std = p.get('standardize', True)
                sc = PowerTransformer(method='yeo-johnson', standardize=std)
                sc.fit(td[nc])
                ds[nc] = sc.transform(df[nc])
                logger.info("✓ Power Yeo-Johnson")
                meta['power_method'] = 'yeo-johnson'
                so = {'scaler': sc, 'columns': nc, 'method': 'power_yeo-johnson'}
            elif m == 'power_box-cox':
                std = p.get('standardize', True)
                if (td[nc] > 0).all().all():
                    sc = PowerTransformer(method='box-cox', standardize=std)
                    sc.fit(td[nc])
                    ds[nc] = sc.transform(df[nc])
                    logger.info("✓ Power Box-Cox")
                    meta['power_method'] = 'box-cox'
                    so = {'scaler': sc, 'columns': nc, 'method': 'power_box-cox'}
                else:
                    logger.warning("Box-Cox needs positive, fallback Yeo-Johnson")
                    sc = PowerTransformer(method='yeo-johnson', standardize=std)
                    sc.fit(td[nc])
                    ds[nc] = sc.transform(df[nc])
                    logger.info("✓ Fallback: Yeo-Johnson")
                    meta['power_method'] = 'yeo-johnson (fallback)'
                    so = {'scaler': sc, 'columns': nc, 'method': 'power_yeo-johnson'}
            else:
                raise ValueError(f"Unknown: {m}")
            logger.info(f"\n✓ Scaling done: {ds.shape}")
            if so: logger.info("✓ Fitted scaler saved\n")
            return ds, meta, so
        
        def main():
            p = argparse.ArgumentParser()
            p.add_argument("--combined_data", required=True)
            p.add_argument("--train_indices", required=True)
            p.add_argument("--method", default='standard')
            p.add_argument("--method_params", default='{}')
            p.add_argument("--target_algorithm", default='none')
            p.add_argument("--output_preprocessed_data", required=True)
            p.add_argument("--output_metadata", required=True)
            p.add_argument("--output_fitted_scaler", required=True)
            args = p.parse_args()
            
            logger.info("="*80 + f"\nSCALING (PARQUET + FITTED SCALER)\n" + "="*80)
            logger.info(f"Method: {args.method}, Algorithm: {args.target_algorithm}\n")
            
            try:
                for o in [args.output_preprocessed_data, args.output_metadata, args.output_fitted_scaler]:
                    ensure_directory_exists(o)
                params = json.loads(args.method_params)
                df = load_data(args.combined_data)
                ti = load_indices(args.train_indices)
                if df.empty:
                    logger.error("Empty dataset")
                    sys.exit(1)
                
                # Compatibility check
                comp = check_compat(args.method, args.target_algorithm)
                if not comp['compatible']:
                    logger.warning("="*80 + "\nCOMPATIBILITY WARNING\n" + "="*80)
                    logger.warning(f"{comp['message']}\nRecommended: {comp.get('recommended_scalers')}\nReason: {comp.get('reason')}\n" + "="*80 + "\n")
                else:
                    logger.info(f"{comp['message']}\n")
                
                # Scale
                ds, meta, so = scale_unified(df, ti, args.method, params)
                
                # SAVE AS PARQUET
                op = args.output_preprocessed_data.replace('.csv', '.parquet') if not args.output_preprocessed_data.endswith('.parquet') else args.output_preprocessed_data
                logger.info("="*80 + "\nSAVING AS PARQUET\n" + "="*80)
                ds.to_parquet(op, index=False, engine='pyarrow', compression='snappy')
                ps = os.path.getsize(op) / 1024**2
                logger.info(f"✓ Saved PARQUET: {ps:.2f} MB\n  Path: {op}")
                meta['output_format'] = {'format': 'parquet', 'engine': 'pyarrow', 'compression': 'snappy', 'file_size_mb': float(ps)}
                
                # Save metadata
                with open(args.output_metadata, 'w') as f:
                    json.dump({'scaling': meta, 'algorithm_compatibility': comp}, f, indent=2)
                logger.info(f"✓ Metadata: {args.output_metadata}")
                
                # Save fitted scaler
                with open(args.output_fitted_scaler, 'wb') as f:
                    pickle.dump(so or {'method': 'none', 'scaler': None}, f)
                logger.info(f"✓ Scaler: {args.output_fitted_scaler}")
                if so: logger.info("  Use for production inference!")
                
                logger.info("\n" + "="*80 + "\nCOMPLETED\n" + "="*80)
                logger.info(f"Method: {args.method}\nScaled: {meta['n_columns_scaled']} cols\nCompat: {comp.get('level')}\nShape: {ds.shape}\nFormat: PARQUET\nSize: {ps:.2f} MB\n" + "="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {e}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --target_algorithm
      - {inputValue: target_algorithm}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_metadata
      - {outputPath: metadata}
      - --output_fitted_scaler
      - {outputPath: fitted_scaler}
