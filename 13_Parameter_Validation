name: Parameter Validation Component
description: Validates and optimizes clustering algorithm parameters before training. Provides data-driven parameter recommendations, validates parameter constraints, and ensures optimal configuration for clustering quality.

inputs:
  - name: train_data
    type: Data
    description: 'Preprocessed training dataset (CSV)'
  - name: distance_statistics
    type: Data
    description: 'Distance statistics from validation (JSON)'
  - name: algorithm
    type: String
    description: 'Clustering algorithm: kmeans, dbscan, hierarchical, gmm, spectral, optics, meanshift, birch'
    default: 'kmeans'
  - name: proposed_params
    type: String
    description: 'Proposed algorithm parameters as JSON to validate and optimize'
    default: '{}'

outputs:
  - name: validated_params
    type: Data
    description: 'Validated and optimized parameters (JSON)'
  - name: parameter_report
    type: Data
    description: 'Parameter validation report with recommendations (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from pathlib import Path
        import math
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('parameter_validation')
        
        PARAMETER_CONSTRAINTS = {
            'kmeans': {
                'n_clusters': {
                    'type': int,
                    'min': 2,
                    'max_rule': 'sqrt(n/2)',
                    'recommended_rule': 'elbow_method',
                    'description': 'Number of clusters'
                },
                'init': {
                    'type': str,
                    'options': ['k-means++', 'random'],
                    'default': 'k-means++',
                    'description': 'Initialization method'
                },
                'n_init': {
                    'type': int,
                    'min': 1,
                    'max': 100,
                    'default': 10,
                    'description': 'Number of initializations'
                },
                'max_iter': {
                    'type': int,
                    'min': 100,
                    'max': 10000,
                    'default': 300,
                    'description': 'Maximum iterations'
                }
            },
            'dbscan': {
                'eps': {
                    'type': float,
                    'min': 0.0,
                    'recommended_rule': 'knn_distance',
                    'description': 'Neighborhood radius'
                },
                'min_samples': {
                    'type': int,
                    'min': 2,
                    'recommended_rule': '2*n_features',
                    'default': 5,
                    'description': 'Minimum points in neighborhood'
                },
                'metric': {
                    'type': str,
                    'options': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],
                    'default': 'euclidean',
                    'description': 'Distance metric'
                }
            },
            'hierarchical': {
                'n_clusters': {
                    'type': int,
                    'min': 2,
                    'max_rule': 'sqrt(n)',
                    'description': 'Number of clusters'
                },
                'linkage': {
                    'type': str,
                    'options': ['ward', 'complete', 'average', 'single'],
                    'default': 'ward',
                    'description': 'Linkage criterion'
                },
                'metric': {
                    'type': str,
                    'options': ['euclidean', 'manhattan', 'cosine'],
                    'default': 'euclidean',
                    'description': 'Distance metric'
                }
            },
            'gmm': {
                'n_components': {
                    'type': int,
                    'min': 1,
                    'max_rule': 'sqrt(n/10)',
                    'description': 'Number of Gaussian components'
                },
                'covariance_type': {
                    'type': str,
                    'options': ['full', 'tied', 'diag', 'spherical'],
                    'default': 'full',
                    'description': 'Covariance structure'
                },
                'max_iter': {
                    'type': int,
                    'min': 100,
                    'max': 10000,
                    'default': 100,
                    'description': 'Maximum EM iterations'
                }
            },
            'spectral': {
                'n_clusters': {
                    'type': int,
                    'min': 2,
                    'max_rule': 'sqrt(n/2)',
                    'description': 'Number of clusters'
                },
                'affinity': {
                    'type': str,
                    'options': ['nearest_neighbors', 'rbf'],
                    'default': 'rbf',
                    'description': 'Affinity matrix construction'
                },
                'n_neighbors': {
                    'type': int,
                    'min': 3,
                    'recommended_rule': 'log(n)',
                    'default': 10,
                    'description': 'Neighbors for affinity'
                }
            },
            'optics': {
                'min_samples': {
                    'type': int,
                    'min': 2,
                    'recommended_rule': '2*n_features',
                    'default': 5,
                    'description': 'Minimum cluster size'
                },
                'max_eps': {
                    'type': float,
                    'min': 0.0,
                    'recommended_rule': 'auto_from_distances',
                    'description': 'Maximum epsilon neighborhood'
                },
                'metric': {
                    'type': str,
                    'options': ['euclidean', 'manhattan', 'chebyshev'],
                    'default': 'euclidean',
                    'description': 'Distance metric'
                }
            },
            'meanshift': {
                'bandwidth': {
                    'type': float,
                    'min': 0.0,
                    'recommended_rule': 'estimate_bandwidth',
                    'description': 'Kernel bandwidth'
                }
            },
            'birch': {
                'n_clusters': {
                    'type': int,
                    'min': 2,
                    'max_rule': 'sqrt(n)',
                    'description': 'Number of clusters'
                },
                'threshold': {
                    'type': float,
                    'min': 0.0,
                    'recommended_rule': 'mean_distance/2',
                    'default': 0.5,
                    'description': 'Subcluster radius'
                },
                'branching_factor': {
                    'type': int,
                    'min': 2,
                    'max': 100,
                    'default': 50,
                    'description': 'Maximum CF subclusters'
                }
            }
        }
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            ext = Path(input_path).suffix.lower()
            if ext in ['.parquet', '.pq']:
                return pd.read_parquet(input_path)
            return pd.read_csv(input_path)
        
        def load_json(input_path):
            with open(input_path, 'r') as f:
                return json.load(f)
        
        def calculate_optimal_parameters(algorithm, train_data, distance_stats):
            logger.info("Calculating optimal parameters for " + algorithm)
            
            n_samples = len(train_data)
            n_features = train_data.select_dtypes(include=[np.number]).shape[1]
            
            optimal_params = {}
            
            if algorithm == 'kmeans':
                max_k = min(int(math.sqrt(n_samples / 2)), 50)
                optimal_params['n_clusters'] = {
                    'calculated': min(8, max_k),
                    'min': 2,
                    'max': max_k,
                    'reasoning': 'Rule of thumb: k ~ sqrt(n/2), capped at 50'
                }
                optimal_params['init'] = {
                    'calculated': 'k-means++',
                    'reasoning': 'k-means++ provides better initialization than random'
                }
                optimal_params['n_init'] = {
                    'calculated': 10,
                    'reasoning': 'Standard practice for stability'
                }
            
            elif algorithm == 'dbscan':
                if 'euclidean' in distance_stats:
                    mean_dist = distance_stats['euclidean']['distribution']['mean']
                    std_dist = distance_stats['euclidean']['distribution']['std']
                    median_dist = distance_stats['euclidean']['distribution']['median']
                    
                    eps_estimate = mean_dist * 0.5
                    
                    optimal_params['eps'] = {
                        'calculated': round(eps_estimate, 4),
                        'min': round(median_dist * 0.1, 4),
                        'max': round(mean_dist * 2, 4),
                        'reasoning': 'Estimated as 50% of mean pairwise distance'
                    }
                
                min_pts = max(5, min(2 * n_features, 20))
                optimal_params['min_samples'] = {
                    'calculated': min_pts,
                    'reasoning': 'Rule of thumb: 2*n_features, capped between 5-20'
                }
            
            elif algorithm == 'hierarchical':
                max_clusters = min(int(math.sqrt(n_samples)), 30)
                optimal_params['n_clusters'] = {
                    'calculated': min(8, max_clusters),
                    'min': 2,
                    'max': max_clusters,
                    'reasoning': 'Based on sample size sqrt(n)'
                }
                optimal_params['linkage'] = {
                    'calculated': 'ward',
                    'reasoning': 'Ward minimizes within-cluster variance'
                }
            
            elif algorithm == 'gmm':
                max_components = min(int(math.sqrt(n_samples / 10)), 20)
                optimal_params['n_components'] = {
                    'calculated': min(5, max_components),
                    'min': 1,
                    'max': max_components,
                    'reasoning': 'Balance between model complexity and sample size'
                }
                optimal_params['covariance_type'] = {
                    'calculated': 'full' if n_features < 50 else 'diag',
                    'reasoning': 'Full covariance for low dimensions, diag for high'
                }
            
            elif algorithm == 'spectral':
                max_clusters = min(int(math.sqrt(n_samples / 2)), 20)
                optimal_params['n_clusters'] = {
                    'calculated': min(8, max_clusters),
                    'min': 2,
                    'max': max_clusters,
                    'reasoning': 'Based on spectral gap analysis rule of thumb'
                }
                n_neighbors = min(int(math.log(n_samples) * 2), 50)
                optimal_params['n_neighbors'] = {
                    'calculated': max(10, n_neighbors),
                    'min': 3,
                    'max': 50,
                    'reasoning': 'Log-scale with sample size for graph connectivity'
                }
            
            elif algorithm == 'optics':
                min_pts = max(5, min(2 * n_features, 20))
                optimal_params['min_samples'] = {
                    'calculated': min_pts,
                    'reasoning': 'Similar to DBSCAN min_samples rule'
                }
                
                if 'euclidean' in distance_stats:
                    p90 = distance_stats['euclidean']['distribution']['percentiles']['90']
                    optimal_params['max_eps'] = {
                        'calculated': round(p90, 4),
                        'reasoning': '90th percentile of pairwise distances'
                    }
            
            elif algorithm == 'birch':
                max_clusters = min(int(math.sqrt(n_samples)), 30)
                optimal_params['n_clusters'] = {
                    'calculated': min(8, max_clusters),
                    'min': 2,
                    'max': max_clusters,
                    'reasoning': 'Based on sample size'
                }
                
                if 'euclidean' in distance_stats:
                    mean_dist = distance_stats['euclidean']['distribution']['mean']
                    optimal_params['threshold'] = {
                        'calculated': round(mean_dist / 2, 4),
                        'reasoning': 'Half of mean pairwise distance for subclusters'
                    }
            
            return optimal_params
        
        def validate_parameters(algorithm, proposed_params, optimal_params, constraints):
            logger.info("")
            logger.info("Validating proposed parameters")
            
            validated_params = {}
            warnings = []
            recommendations = []
            
            for param_name, constraint in constraints.items():
                logger.info("  " + param_name + ":")
                
                if param_name in proposed_params:
                    value = proposed_params[param_name]
                    logger.info("    Proposed: " + str(value))
                    
                    expected_type = constraint['type']
                    if not isinstance(value, expected_type):
                        try:
                            value = expected_type(value)
                            warnings.append("Converted " + param_name + " to " + str(expected_type.__name__))
                        except:
                            warnings.append("Invalid type for " + param_name + ", using optimal")
                            value = optimal_params.get(param_name, {}).get('calculated')
                    
                    if 'options' in constraint and value not in constraint['options']:
                        warnings.append(param_name + " value " + str(value) + " not in " + str(constraint['options']))
                        value = constraint.get('default', constraint['options'][0])
                    
                    if 'min' in constraint and value < constraint['min']:
                        warnings.append(param_name + " below minimum, adjusting")
                        value = constraint['min']
                    
                    validated_params[param_name] = value
                    
                    if param_name in optimal_params:
                        optimal_value = optimal_params[param_name]['calculated']
                        if value != optimal_value:
                            recommendations.append(
                                param_name + ": Proposed=" + str(value) + 
                                ", Optimal=" + str(optimal_value) + 
                                " (" + optimal_params[param_name]['reasoning'] + ")"
                            )
                    
                    logger.info("    âœ“ Validated: " + str(value))
                    
                elif param_name in optimal_params:
                    value = optimal_params[param_name]['calculated']
                    validated_params[param_name] = value
                    warnings.append("Missing " + param_name + ", using optimal: " + str(value))
                    logger.info("    Using optimal: " + str(value))
                    
                elif 'default' in constraint:
                    value = constraint['default']
                    validated_params[param_name] = value
                    logger.info("    Using default: " + str(value))
            
            return validated_params, warnings, recommendations
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--train_data", required=True)
            parser.add_argument("--distance_statistics", required=True)
            parser.add_argument("--algorithm", default='kmeans')
            parser.add_argument("--proposed_params", default='{}')
            parser.add_argument("--output_validated_params", required=True)
            parser.add_argument("--output_parameter_report", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("PARAMETER VALIDATION & OPTIMIZATION")
            logger.info("="*80)
            logger.info("Algorithm: " + args.algorithm)
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_validated_params)
                ensure_directory_exists(args.output_parameter_report)
                
                proposed_params = json.loads(args.proposed_params)
                train_df = load_data(args.train_data)
                distance_stats = load_json(args.distance_statistics)
                
                if args.algorithm not in PARAMETER_CONSTRAINTS:
                    logger.error("ERROR: Unsupported algorithm: " + args.algorithm)
                    sys.exit(1)
                
                constraints = PARAMETER_CONSTRAINTS[args.algorithm]
                
                optimal_params = calculate_optimal_parameters(
                    args.algorithm,
                    train_df,
                    distance_stats
                )
                
                logger.info("")
                logger.info("Optimal parameters calculated:")
                for param, info in optimal_params.items():
                    logger.info("  " + param + ": " + str(info['calculated']) + 
                               " (" + info['reasoning'] + ")")
                
                validated_params, warnings, recommendations = validate_parameters(
                    args.algorithm,
                    proposed_params,
                    optimal_params,
                    constraints
                )
                
                parameter_report = {
                    'algorithm': args.algorithm,
                    'proposed_params': proposed_params,
                    'optimal_params': optimal_params,
                    'validated_params': validated_params,
                    'warnings': warnings,
                    'recommendations': recommendations,
                    'data_info': {
                        'n_samples': len(train_df),
                        'n_features': train_df.select_dtypes(include=[np.number]).shape[1]
                    }
                }
                
                with open(args.output_validated_params, 'w') as f:
                    json.dump(validated_params, f, indent=2)
                logger.info("")
                logger.info("Validated parameters saved")
                
                with open(args.output_parameter_report, 'w') as f:
                    json.dump(parameter_report, f, indent=2)
                logger.info("Parameter report saved")
                
                logger.info("")
                logger.info("="*80)
                logger.info("VALIDATION COMPLETED")
                logger.info("="*80)
                logger.info("Warnings: " + str(len(warnings)))
                logger.info("Recommendations: " + str(len(recommendations)))
                
                if warnings:
                    logger.info("")
                    logger.info("WARNINGS:")
                    for warning in warnings:
                        logger.warning("  - " + warning)
                
                if recommendations:
                    logger.info("")
                    logger.info("RECOMMENDATIONS:")
                    for rec in recommendations:
                        logger.info("  - " + rec)
                
                logger.info("="*80)
                
            except Exception as e:
                logger.error("ERROR: " + str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --train_data
      - {inputPath: train_data}
      - --distance_statistics
      - {inputPath: distance_statistics}
      - --algorithm
      - {inputValue: algorithm}
      - --proposed_params
      - {inputValue: proposed_params}
      - --output_validated_params
      - {outputPath: validated_params}
      - --output_parameter_report
      - {outputPath: parameter_report}
