name: Generic Parameter Validation Component - All ML Algorithms
description: Validates and optimizes parameters for ALL ML algorithms (supervised and unsupervised) before training. Provides data-driven parameter recommendations based on dataset characteristics (n_samples, n_features, distance stats). Supports 14 clustering algorithms + 15 supervised algorithms. Loads Parquet/CSV.

inputs:
  - name: train_data
    type: Data
    description: 'Preprocessed training dataset (Parquet or CSV)'
  - name: distance_statistics
    type: Data
    description: 'Distance statistics from validation (JSON)'
  - name: algorithm
    type: String
    description: 'Target algorithm. Clustering: KMeans, MiniBatchKMeans, BisectingKMeans, KMedoids, DBSCAN, OPTICS, HDBSCAN, AgglomerativeClustering, BIRCH, GaussianMixture, BayesianGaussianMixture, FuzzyCMeans, MeanShift, AffinityPropagation, SpectralClustering. Supervised: RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, XGBClassifier, XGBRegressor, LGBMClassifier, LGBMRegressor, SVC, SVR, LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, KNeighborsClassifier, KNeighborsRegressor, DecisionTreeClassifier, DecisionTreeRegressor, MLPClassifier, MLPRegressor, AdaBoostClassifier, AdaBoostRegressor'
    default: 'KMeans'
  - name: proposed_params
    type: String
    description: 'Proposed algorithm parameters as JSON string. Example: {"n_clusters": 3}'
    default: '{}'

outputs:
  - name: validated_params
    type: Data
    description: 'Validated and optimized parameters (JSON)'
  - name: parameter_report
    type: Data
    description: 'Parameter validation report with recommendations (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from pathlib import Path
        import math
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('parameter_validation')
        
        # PARAMETER CONSTRAINTS DATABASE - Clustering + Supervised ML
        PARAMETER_CONSTRAINTS = {
            # ── CLUSTERING ──────────────────────────────────────────────────
            'KMeans': {
                'n_clusters': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'recommended_rule': 'elbow_method', 'description': 'Number of clusters'},
                'init': {'type': str, 'options': ['k-means++', 'random'], 'default': 'k-means++', 'description': 'Initialization method'},
                'n_init': {'type': int, 'min': 1, 'max': 100, 'default': 10, 'description': 'Number of initializations'},
                'max_iter': {'type': int, 'min': 100, 'max': 10000, 'default': 300, 'description': 'Maximum iterations'}
            },
            'MiniBatchKMeans': {
                'n_clusters': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'description': 'Number of clusters'},
                'batch_size': {'type': int, 'min': 10, 'max': 10000, 'default': 256, 'description': 'Size of mini-batches'},
                'max_iter': {'type': int, 'min': 10, 'max': 1000, 'default': 100, 'description': 'Maximum iterations'}
            },
            'BisectingKMeans': {
                'n_clusters': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'description': 'Number of clusters'},
                'init': {'type': str, 'options': ['k-means++', 'random'], 'default': 'k-means++', 'description': 'Initialization method'},
                'bisecting_strategy': {'type': str, 'options': ['biggest_inertia', 'largest_cluster'], 'default': 'biggest_inertia', 'description': 'Strategy for selecting cluster to bisect'}
            },
            'KMedoids': {
                'n_clusters': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'description': 'Number of clusters'},
                'metric': {'type': str, 'options': ['euclidean', 'manhattan', 'cosine'], 'default': 'euclidean', 'description': 'Distance metric'},
                'method': {'type': str, 'options': ['pam', 'alternate'], 'default': 'pam', 'description': 'Medoid initialization method'}
            },
            'DBSCAN': {
                'eps': {'type': float, 'min': 0.0, 'recommended_rule': 'knn_distance', 'description': 'Neighborhood radius'},
                'min_samples': {'type': int, 'min': 2, 'recommended_rule': '2*n_features', 'default': 5, 'description': 'Minimum points in neighborhood'},
                'metric': {'type': str, 'options': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'], 'default': 'euclidean', 'description': 'Distance metric'}
            },
            'OPTICS': {
                'min_samples': {'type': int, 'min': 2, 'recommended_rule': '2*n_features', 'default': 5, 'description': 'Minimum points in neighborhood'},
                'max_eps': {'type': float, 'min': 0.0, 'description': 'Maximum epsilon neighborhood'},
                'metric': {'type': str, 'options': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'], 'default': 'euclidean', 'description': 'Distance metric'},
                'cluster_method': {'type': str, 'options': ['xi', 'dbscan'], 'default': 'xi', 'description': 'Cluster extraction method'}
            },
            'HDBSCAN': {
                'min_cluster_size': {'type': int, 'min': 2, 'recommended_rule': '1% of n_samples', 'default': 5, 'description': 'Minimum cluster size'},
                'min_samples': {'type': int, 'min': 1, 'description': 'Min samples for core points (None = min_cluster_size)'},
                'cluster_selection_epsilon': {'type': float, 'min': 0.0, 'default': 0.0, 'description': 'Distance threshold for cluster selection'},
                'metric': {'type': str, 'options': ['euclidean', 'manhattan', 'chebyshev'], 'default': 'euclidean', 'description': 'Distance metric'}
            },
            'AgglomerativeClustering': {
                'n_clusters': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'description': 'Number of clusters'},
                'linkage': {'type': str, 'options': ['ward', 'complete', 'average', 'single'], 'default': 'ward', 'description': 'Linkage criterion'},
                'affinity': {'type': str, 'options': ['euclidean', 'manhattan', 'cosine'], 'default': 'euclidean', 'description': 'Distance metric'}
            },
            'BIRCH': {
                'n_clusters': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'description': 'Number of clusters after hierarchical clustering'},
                'threshold': {'type': float, 'min': 0.0, 'default': 0.5, 'description': 'Radius of subcluster'},
                'branching_factor': {'type': int, 'min': 2, 'default': 50, 'description': 'Maximum CF subclusters in each node'}
            },
            'GaussianMixture': {
                'n_components': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'recommended_rule': 'bic_aic', 'description': 'Number of mixture components'},
                'covariance_type': {'type': str, 'options': ['full', 'tied', 'diag', 'spherical'], 'default': 'full', 'description': 'Covariance matrix type'},
                'max_iter': {'type': int, 'min': 100, 'max': 10000, 'default': 100, 'description': 'Maximum EM iterations'}
            },
            'BayesianGaussianMixture': {
                'n_components': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'description': 'Maximum number of mixture components'},
                'covariance_type': {'type': str, 'options': ['full', 'tied', 'diag', 'spherical'], 'default': 'full', 'description': 'Covariance matrix type'},
                'weight_concentration_prior_type': {'type': str, 'options': ['dirichlet_process', 'dirichlet_distribution'], 'default': 'dirichlet_process', 'description': 'Prior type for weights'},
                'max_iter': {'type': int, 'min': 100, 'max': 10000, 'default': 100, 'description': 'Maximum variational iterations'}
            },
            'FuzzyCMeans': {
                'c': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'description': 'Number of clusters'},
                'm': {'type': float, 'min': 1.0, 'max': 10.0, 'default': 2.0, 'description': 'Fuzziness parameter (m>1)'},
                'error': {'type': float, 'min': 1e-10, 'max': 1.0, 'default': 0.005, 'description': 'Stopping criterion'},
                'maxiter': {'type': int, 'min': 10, 'max': 10000, 'default': 1000, 'description': 'Maximum iterations'}
            },
            'MeanShift': {
                'bandwidth': {'type': float, 'min': 0.0, 'recommended_rule': 'estimate_bandwidth', 'description': 'Kernel bandwidth (None = auto-estimate)'},
                'bin_seeding': {'type': bool, 'default': False, 'description': 'Use discretized binning for seeding'},
                'min_bin_freq': {'type': int, 'min': 1, 'default': 1, 'description': 'Minimum frequency for bin seeding'}
            },
            'AffinityPropagation': {
                'damping': {'type': float, 'min': 0.5, 'max': 1.0, 'default': 0.5, 'description': 'Damping factor (0.5-1.0)'},
                'max_iter': {'type': int, 'min': 10, 'max': 1000, 'default': 200, 'description': 'Maximum iterations'},
                'convergence_iter': {'type': int, 'min': 1, 'max': 100, 'default': 15, 'description': 'Iterations without change for convergence'}
            },
            'SpectralClustering': {
                'n_clusters': {'type': int, 'min': 2, 'max_rule': 'sqrt(n/2)', 'description': 'Number of clusters'},
                'affinity': {'type': str, 'options': ['nearest_neighbors', 'rbf', 'precomputed'], 'default': 'rbf', 'description': 'Kernel type for affinity matrix'},
                'n_neighbors': {'type': int, 'min': 2, 'default': 10, 'description': 'Number of neighbors (if using nearest_neighbors)'},
                'assign_labels': {'type': str, 'options': ['kmeans', 'discretize', 'cluster_qr'], 'default': 'kmeans', 'description': 'Strategy for assigning labels from eigenvectors'}
            },
            # ── ENSEMBLE / TREE-BASED ────────────────────────────────────────
            'RandomForestClassifier': {
                'n_estimators': {'type': int, 'min': 10, 'max': 2000, 'default': 100, 'description': 'Number of trees'},
                'max_depth': {'type': int, 'min': 1, 'default': None, 'description': 'Max tree depth (None = unlimited)'},
                'min_samples_split': {'type': int, 'min': 2, 'default': 2, 'description': 'Min samples to split a node'},
                'min_samples_leaf': {'type': int, 'min': 1, 'default': 1, 'description': 'Min samples in a leaf'},
                'max_features': {'type': str, 'options': ['sqrt', 'log2', 'auto', None], 'default': 'sqrt', 'description': 'Features per split'},
                'criterion': {'type': str, 'options': ['gini', 'entropy', 'log_loss'], 'default': 'gini', 'description': 'Split quality measure'}
            },
            'RandomForestRegressor': {
                'n_estimators': {'type': int, 'min': 10, 'max': 2000, 'default': 100, 'description': 'Number of trees'},
                'max_depth': {'type': int, 'min': 1, 'default': None, 'description': 'Max tree depth'},
                'min_samples_split': {'type': int, 'min': 2, 'default': 2, 'description': 'Min samples to split a node'},
                'min_samples_leaf': {'type': int, 'min': 1, 'default': 1, 'description': 'Min samples in a leaf'},
                'max_features': {'type': str, 'options': ['sqrt', 'log2', 'auto', None], 'default': 'auto', 'description': 'Features per split'},
                'criterion': {'type': str, 'options': ['squared_error', 'absolute_error', 'friedman_mse'], 'default': 'squared_error', 'description': 'Split quality measure'}
            },
            'GradientBoostingClassifier': {
                'n_estimators': {'type': int, 'min': 10, 'max': 2000, 'default': 100, 'description': 'Number of boosting stages'},
                'learning_rate': {'type': float, 'min': 0.001, 'max': 1.0, 'default': 0.1, 'description': 'Shrinks contribution of each tree'},
                'max_depth': {'type': int, 'min': 1, 'max': 10, 'default': 3, 'description': 'Max depth of individual estimators'},
                'subsample': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of samples per tree'},
                'min_samples_split': {'type': int, 'min': 2, 'default': 2, 'description': 'Min samples to split a node'},
                'loss': {'type': str, 'options': ['log_loss', 'exponential'], 'default': 'log_loss', 'description': 'Loss function'}
            },
            'GradientBoostingRegressor': {
                'n_estimators': {'type': int, 'min': 10, 'max': 2000, 'default': 100, 'description': 'Number of boosting stages'},
                'learning_rate': {'type': float, 'min': 0.001, 'max': 1.0, 'default': 0.1, 'description': 'Shrinks contribution of each tree'},
                'max_depth': {'type': int, 'min': 1, 'max': 10, 'default': 3, 'description': 'Max depth of individual estimators'},
                'subsample': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of samples per tree'},
                'loss': {'type': str, 'options': ['squared_error', 'absolute_error', 'huber', 'quantile'], 'default': 'squared_error', 'description': 'Loss function'}
            },
            'XGBClassifier': {
                'n_estimators': {'type': int, 'min': 10, 'max': 2000, 'default': 100, 'description': 'Number of boosting rounds'},
                'learning_rate': {'type': float, 'min': 0.001, 'max': 1.0, 'default': 0.1, 'description': 'Step size shrinkage'},
                'max_depth': {'type': int, 'min': 1, 'max': 20, 'default': 6, 'description': 'Max tree depth'},
                'subsample': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of samples per tree'},
                'colsample_bytree': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of features per tree'},
                'reg_alpha': {'type': float, 'min': 0.0, 'default': 0.0, 'description': 'L1 regularization'},
                'reg_lambda': {'type': float, 'min': 0.0, 'default': 1.0, 'description': 'L2 regularization'}
            },
            'XGBRegressor': {
                'n_estimators': {'type': int, 'min': 10, 'max': 2000, 'default': 100, 'description': 'Number of boosting rounds'},
                'learning_rate': {'type': float, 'min': 0.001, 'max': 1.0, 'default': 0.1, 'description': 'Step size shrinkage'},
                'max_depth': {'type': int, 'min': 1, 'max': 20, 'default': 6, 'description': 'Max tree depth'},
                'subsample': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of samples per tree'},
                'colsample_bytree': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of features per tree'},
                'reg_alpha': {'type': float, 'min': 0.0, 'default': 0.0, 'description': 'L1 regularization'},
                'reg_lambda': {'type': float, 'min': 0.0, 'default': 1.0, 'description': 'L2 regularization'}
            },
            'LGBMClassifier': {
                'n_estimators': {'type': int, 'min': 10, 'max': 2000, 'default': 100, 'description': 'Number of boosting rounds'},
                'learning_rate': {'type': float, 'min': 0.001, 'max': 1.0, 'default': 0.1, 'description': 'Step size shrinkage'},
                'max_depth': {'type': int, 'min': -1, 'default': -1, 'description': 'Max tree depth (-1 = unlimited)'},
                'num_leaves': {'type': int, 'min': 2, 'max': 131072, 'default': 31, 'description': 'Max leaves per tree'},
                'subsample': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of samples per tree'},
                'colsample_bytree': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of features per tree'},
                'reg_alpha': {'type': float, 'min': 0.0, 'default': 0.0, 'description': 'L1 regularization'},
                'reg_lambda': {'type': float, 'min': 0.0, 'default': 0.0, 'description': 'L2 regularization'}
            },
            'LGBMRegressor': {
                'n_estimators': {'type': int, 'min': 10, 'max': 2000, 'default': 100, 'description': 'Number of boosting rounds'},
                'learning_rate': {'type': float, 'min': 0.001, 'max': 1.0, 'default': 0.1, 'description': 'Step size shrinkage'},
                'max_depth': {'type': int, 'min': -1, 'default': -1, 'description': 'Max tree depth (-1 = unlimited)'},
                'num_leaves': {'type': int, 'min': 2, 'max': 131072, 'default': 31, 'description': 'Max leaves per tree'},
                'subsample': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of samples per tree'},
                'colsample_bytree': {'type': float, 'min': 0.1, 'max': 1.0, 'default': 1.0, 'description': 'Fraction of features per tree'}
            },
            'AdaBoostClassifier': {
                'n_estimators': {'type': int, 'min': 10, 'max': 1000, 'default': 50, 'description': 'Number of weak learners'},
                'learning_rate': {'type': float, 'min': 0.001, 'max': 2.0, 'default': 1.0, 'description': 'Weight applied to each classifier'},
                'algorithm': {'type': str, 'options': ['SAMME', 'SAMME.R'], 'default': 'SAMME.R', 'description': 'Boosting algorithm'}
            },
            'AdaBoostRegressor': {
                'n_estimators': {'type': int, 'min': 10, 'max': 1000, 'default': 50, 'description': 'Number of weak learners'},
                'learning_rate': {'type': float, 'min': 0.001, 'max': 2.0, 'default': 1.0, 'description': 'Weight applied to each regressor'},
                'loss': {'type': str, 'options': ['linear', 'square', 'exponential'], 'default': 'linear', 'description': 'Loss function'}
            },
            'DecisionTreeClassifier': {
                'max_depth': {'type': int, 'min': 1, 'default': None, 'description': 'Max tree depth (None = unlimited)'},
                'min_samples_split': {'type': int, 'min': 2, 'default': 2, 'description': 'Min samples to split a node'},
                'min_samples_leaf': {'type': int, 'min': 1, 'default': 1, 'description': 'Min samples in a leaf'},
                'criterion': {'type': str, 'options': ['gini', 'entropy', 'log_loss'], 'default': 'gini', 'description': 'Split quality measure'},
                'max_features': {'type': str, 'options': ['sqrt', 'log2', 'auto', None], 'default': None, 'description': 'Features per split'}
            },
            'DecisionTreeRegressor': {
                'max_depth': {'type': int, 'min': 1, 'default': None, 'description': 'Max tree depth'},
                'min_samples_split': {'type': int, 'min': 2, 'default': 2, 'description': 'Min samples to split a node'},
                'min_samples_leaf': {'type': int, 'min': 1, 'default': 1, 'description': 'Min samples in a leaf'},
                'criterion': {'type': str, 'options': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'default': 'squared_error', 'description': 'Split quality measure'}
            },
            # ── SVM ──────────────────────────────────────────────────────────
            'SVC': {
                'C': {'type': float, 'min': 0.0001, 'max': 10000.0, 'default': 1.0, 'description': 'Regularization parameter'},
                'kernel': {'type': str, 'options': ['rbf', 'linear', 'poly', 'sigmoid'], 'default': 'rbf', 'description': 'Kernel type'},
                'gamma': {'type': str, 'options': ['scale', 'auto'], 'default': 'scale', 'description': 'Kernel coefficient'},
                'degree': {'type': int, 'min': 1, 'max': 10, 'default': 3, 'description': 'Degree for poly kernel'},
                'max_iter': {'type': int, 'min': -1, 'default': -1, 'description': 'Max iterations (-1 = no limit)'}
            },
            'SVR': {
                'C': {'type': float, 'min': 0.0001, 'max': 10000.0, 'default': 1.0, 'description': 'Regularization parameter'},
                'kernel': {'type': str, 'options': ['rbf', 'linear', 'poly', 'sigmoid'], 'default': 'rbf', 'description': 'Kernel type'},
                'gamma': {'type': str, 'options': ['scale', 'auto'], 'default': 'scale', 'description': 'Kernel coefficient'},
                'epsilon': {'type': float, 'min': 0.0, 'default': 0.1, 'description': 'Epsilon in epsilon-SVR model'},
                'degree': {'type': int, 'min': 1, 'max': 10, 'default': 3, 'description': 'Degree for poly kernel'}
            },
            # ── LINEAR MODELS ─────────────────────────────────────────────────
            'LogisticRegression': {
                'C': {'type': float, 'min': 0.0001, 'max': 10000.0, 'default': 1.0, 'description': 'Inverse regularization strength'},
                'penalty': {'type': str, 'options': ['l1', 'l2', 'elasticnet', None], 'default': 'l2', 'description': 'Regularization type'},
                'solver': {'type': str, 'options': ['lbfgs', 'liblinear', 'saga', 'sag', 'newton-cg'], 'default': 'lbfgs', 'description': 'Optimization algorithm'},
                'max_iter': {'type': int, 'min': 100, 'max': 10000, 'default': 1000, 'description': 'Max iterations'},
                'multi_class': {'type': str, 'options': ['auto', 'ovr', 'multinomial'], 'default': 'auto', 'description': 'Multi-class strategy'}
            },
            'LinearRegression': {
                'fit_intercept': {'type': bool, 'default': True, 'description': 'Whether to fit intercept'},
                'positive': {'type': bool, 'default': False, 'description': 'Force positive coefficients'}
            },
            'Ridge': {
                'alpha': {'type': float, 'min': 0.0, 'default': 1.0, 'description': 'Regularization strength'},
                'fit_intercept': {'type': bool, 'default': True, 'description': 'Whether to fit intercept'},
                'max_iter': {'type': int, 'min': 1, 'default': None, 'description': 'Max iterations for solvers'}
            },
            'Lasso': {
                'alpha': {'type': float, 'min': 0.0, 'default': 1.0, 'description': 'Regularization strength'},
                'fit_intercept': {'type': bool, 'default': True, 'description': 'Whether to fit intercept'},
                'max_iter': {'type': int, 'min': 100, 'default': 1000, 'description': 'Max iterations'}
            },
            'ElasticNet': {
                'alpha': {'type': float, 'min': 0.0, 'default': 1.0, 'description': 'Regularization strength'},
                'l1_ratio': {'type': float, 'min': 0.0, 'max': 1.0, 'default': 0.5, 'description': 'L1 vs L2 mix (0=Ridge, 1=Lasso)'},
                'fit_intercept': {'type': bool, 'default': True, 'description': 'Whether to fit intercept'},
                'max_iter': {'type': int, 'min': 100, 'default': 1000, 'description': 'Max iterations'}
            },
            # ── NEIGHBORS ────────────────────────────────────────────────────
            'KNeighborsClassifier': {
                'n_neighbors': {'type': int, 'min': 1, 'max_rule': 'sqrt(n)', 'default': 5, 'description': 'Number of neighbors'},
                'weights': {'type': str, 'options': ['uniform', 'distance'], 'default': 'uniform', 'description': 'Weight function'},
                'metric': {'type': str, 'options': ['euclidean', 'manhattan', 'minkowski', 'chebyshev'], 'default': 'minkowski', 'description': 'Distance metric'},
                'algorithm': {'type': str, 'options': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'default': 'auto', 'description': 'Algorithm for neighbor search'}
            },
            'KNeighborsRegressor': {
                'n_neighbors': {'type': int, 'min': 1, 'max_rule': 'sqrt(n)', 'default': 5, 'description': 'Number of neighbors'},
                'weights': {'type': str, 'options': ['uniform', 'distance'], 'default': 'uniform', 'description': 'Weight function'},
                'metric': {'type': str, 'options': ['euclidean', 'manhattan', 'minkowski', 'chebyshev'], 'default': 'minkowski', 'description': 'Distance metric'},
                'algorithm': {'type': str, 'options': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'default': 'auto', 'description': 'Algorithm for neighbor search'}
            },
            # ── NEURAL NETWORKS ───────────────────────────────────────────────
            'MLPClassifier': {
                'hidden_layer_sizes': {'type': tuple, 'default': (100,), 'description': 'Neurons per hidden layer e.g. (100,50)'},
                'activation': {'type': str, 'options': ['relu', 'tanh', 'logistic', 'identity'], 'default': 'relu', 'description': 'Activation function'},
                'solver': {'type': str, 'options': ['adam', 'sgd', 'lbfgs'], 'default': 'adam', 'description': 'Weight optimizer'},
                'alpha': {'type': float, 'min': 0.0, 'default': 0.0001, 'description': 'L2 regularization term'},
                'learning_rate': {'type': str, 'options': ['constant', 'invscaling', 'adaptive'], 'default': 'constant', 'description': 'Learning rate schedule'},
                'max_iter': {'type': int, 'min': 10, 'max': 10000, 'default': 200, 'description': 'Max iterations'},
                'batch_size': {'type': int, 'min': 1, 'default': 200, 'description': 'Mini-batch size (adam/sgd)'}
            },
            'MLPRegressor': {
                'hidden_layer_sizes': {'type': tuple, 'default': (100,), 'description': 'Neurons per hidden layer'},
                'activation': {'type': str, 'options': ['relu', 'tanh', 'logistic', 'identity'], 'default': 'relu', 'description': 'Activation function'},
                'solver': {'type': str, 'options': ['adam', 'sgd', 'lbfgs'], 'default': 'adam', 'description': 'Weight optimizer'},
                'alpha': {'type': float, 'min': 0.0, 'default': 0.0001, 'description': 'L2 regularization term'},
                'learning_rate': {'type': str, 'options': ['constant', 'invscaling', 'adaptive'], 'default': 'constant', 'description': 'Learning rate schedule'},
                'max_iter': {'type': int, 'min': 10, 'max': 10000, 'default': 200, 'description': 'Max iterations'}
            }
        }
        
        def ensure_directory_exists(filepath):
            Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        def detect_file_type(file_path):
            # Detect file type by MAGIC BYTES first (not extension).
            # CRITICAL FIX: Kubernetes passes files without extensions
            try:
                with open(file_path, 'rb') as f:
                    header = f.read(8)
                if header[:4] == b'PAR1':
                    logger.info("[OK] Detected: Parquet (PAR1 magic bytes)")
                    return 'parquet'
                if header[1:6] == b'NUMPY':
                    logger.info("[OK] Detected: NumPy array")
                    return 'numpy'
                if header[:2] in [b'PK', b'\x80\x04']:
                    logger.info("[OK] Detected: Pickle/ZIP")
                    return 'pickle'
                try:
                    text_start = open(file_path, 'r', errors='replace').read(512)
                    if text_start.strip().startswith('{') or text_start.strip().startswith('['):
                        return 'json'
                    return 'csv'
                except Exception:
                    return 'csv'
            except Exception as e:
                logger.warning("Magic byte detection failed: " + str(e) + ", defaulting to CSV")
                return 'csv'

        def load_data(filepath):
            # Load data with MAGIC BYTES detection first.
            # CRITICAL FIX: Kubernetes strips file extensions.
            logger.info("Loading data from: " + filepath)
            ext = Path(filepath).suffix.lower()
            logger.info("File extension: '" + ext + "' (may be empty on Kubernetes)")
            detected_type = detect_file_type(filepath)
            try:
                if ext in ['.parquet', '.pq'] or detected_type == 'parquet':
                    logger.info("Loading as Parquet...")
                    df = pd.read_parquet(filepath, engine='pyarrow')
                    logger.info("[OK] Loaded Parquet: " + str(df.shape[0]) + " rows x " + str(df.shape[1]) + " columns")
                    return df
                logger.info("Loading as CSV...")
                for enc in ['utf-8', 'latin-1', 'cp1252']:
                    try:
                        df = pd.read_csv(filepath, encoding=enc)
                        logger.info("[OK] Loaded CSV (" + enc + "): " + str(df.shape[0]) + " rows x " + str(df.shape[1]) + " columns")
                        return df
                    except UnicodeDecodeError:
                        continue
                raise ValueError("Could not decode file with any supported encoding")
            except Exception as e:
                logger.error("Error loading data: " + str(e))
                raise
        
        def load_json(filepath):
            logger.info("Loading JSON from: " + filepath)
            with open(filepath, 'r') as f:
                return json.load(f)
        
        def parse_proposed_params(param_string):
            logger.info("Parsing proposed_params")
            logger.info("  Input: " + repr(param_string))
            
            if not param_string or param_string.strip() in ['', '{}']:
                logger.info("  [OK] Empty parameters, using defaults")
                return {}
            
            try:
                params = json.loads(param_string)
                logger.info("  [OK] Parsed successfully: " + str(params))
                return params
            except json.JSONDecodeError as e:
                example_format = '{"n_clusters": 3}'
                error_msg = (
                    "Invalid JSON in proposed_params: " + str(e) + chr(10) +
                    "Expected format: " + example_format + chr(10) +
                    "Received: " + str(param_string)
                )
                logger.error(error_msg)
                raise ValueError(error_msg)
        
        def calculate_optimal_parameters(algorithm, train_df, distance_stats):
            logger.info("")
            logger.info("Calculating optimal parameters based on data characteristics")
            
            n_samples = len(train_df)
            n_features = len(train_df.columns)
            
            logger.info("  Data shape: " + str(n_samples) + " samples x " + str(n_features) + " features")
            
            optimal_params = {}
            
            if algorithm == 'KMeans':
                max_k = int(math.sqrt(n_samples / 2))
                elbow_k = min(max(3, int(math.sqrt(n_samples / 10))), max_k)
                optimal_params['n_clusters'] = {'calculated': elbow_k, 'reasoning': "Elbow method heuristic: sqrt(n/10), capped at sqrt(n/2)"}
                optimal_params['init'] = {'calculated': 'k-means++', 'reasoning': 'Best practice for initialization'}
                optimal_params['n_init'] = {'calculated': 10, 'reasoning': 'Standard number of initializations'}
                
            elif algorithm == 'MiniBatchKMeans':
                max_k = int(math.sqrt(n_samples / 2))
                optimal_k = min(max(3, int(math.sqrt(n_samples / 10))), max_k)
                optimal_params['n_clusters'] = {'calculated': optimal_k, 'reasoning': "Same as KMeans heuristic"}
                optimal_params['batch_size'] = {'calculated': min(256, n_samples // 10), 'reasoning': "Auto-sized based on dataset"}
            
            elif algorithm == 'BisectingKMeans':
                max_k = int(math.sqrt(n_samples / 2))
                optimal_k = min(max(3, int(math.sqrt(n_samples / 10))), max_k)
                optimal_params['n_clusters'] = {'calculated': optimal_k, 'reasoning': "Hierarchical K-means heuristic"}
            
            elif algorithm == 'KMedoids':
                max_k = int(math.sqrt(n_samples / 2))
                optimal_k = min(max(3, int(math.sqrt(n_samples / 10))), max_k)
                optimal_params['n_clusters'] = {'calculated': optimal_k, 'reasoning': "Similar to KMeans"}
                
            elif algorithm == 'DBSCAN':
                mean_knn_dist = distance_stats.get('mean_distance', 1.0)
                optimal_eps = mean_knn_dist * 1.5
                optimal_min_samples = max(2, min(2 * n_features, 10))
                optimal_params['eps'] = {'calculated': round(optimal_eps, 4), 'reasoning': "1.5x mean k-NN distance"}
                optimal_params['min_samples'] = {'calculated': optimal_min_samples, 'reasoning': "2 * n_features, capped at 10"}
            
            elif algorithm == 'OPTICS':
                optimal_min_samples = max(2, min(2 * n_features, 10))
                optimal_params['min_samples'] = {'calculated': optimal_min_samples, 'reasoning': "2 * n_features"}
            
            elif algorithm == 'HDBSCAN':
                suggested_size = max(5, int(n_samples * 0.01))
                optimal_params['min_cluster_size'] = {'calculated': suggested_size, 'reasoning': "1% of samples"}
                
            elif algorithm == 'AgglomerativeClustering':
                max_k = int(math.sqrt(n_samples / 2))
                optimal_k = min(max(3, int(math.sqrt(n_samples / 10))), max_k)
                optimal_params['n_clusters'] = {'calculated': optimal_k, 'reasoning': "Similar to k-means heuristic"}
                optimal_params['linkage'] = {'calculated': 'ward', 'reasoning': "Best for compact clusters with Euclidean distance"}
            
            elif algorithm == 'BIRCH':
                max_k = int(math.sqrt(n_samples / 2))
                optimal_k = min(max(2, int(math.sqrt(n_samples / 10))), max_k)
                optimal_params['n_clusters'] = {'calculated': optimal_k, 'reasoning': "Based on sample size"}
                optimal_params['threshold'] = {'calculated': 0.5, 'reasoning': "Default threshold for subcluster radius"}
                
            elif algorithm == 'GaussianMixture':
                max_k = int(math.sqrt(n_samples / 2))
                optimal_k = min(max(2, int(math.sqrt(n_samples / 10))), max_k)
                optimal_params['n_components'] = {'calculated': optimal_k, 'reasoning': "Based on sample size heuristic"}
                optimal_params['covariance_type'] = {'calculated': 'full', 'reasoning': "Most flexible, suitable for general data"}
            
            elif algorithm == 'BayesianGaussianMixture':
                max_k = int(math.sqrt(n_samples / 2))
                optimal_k = min(max(2, int(math.sqrt(n_samples / 10))), max_k)
                optimal_params['n_components'] = {'calculated': optimal_k, 'reasoning': "Similar to GaussianMixture"}
            
            elif algorithm == 'FuzzyCMeans':
                max_k = int(math.sqrt(n_samples / 2))
                optimal_k = min(max(2, int(math.sqrt(n_samples / 10))), 20)
                optimal_params['c'] = {'calculated': optimal_k, 'reasoning': "Fuzzy cluster count"}
                optimal_params['m'] = {'calculated': 2.0, 'reasoning': "Standard fuzziness parameter"}
                
            elif algorithm == 'SpectralClustering':
                max_k = int(math.sqrt(n_samples / 2))
                optimal_k = min(max(2, int(math.sqrt(n_samples / 10))), max_k)
                optimal_params['n_clusters'] = {'calculated': optimal_k, 'reasoning': "Based on sample size"}
                optimal_params['affinity'] = {'calculated': 'rbf', 'reasoning': "Good default for non-linear boundaries"}
            
            elif algorithm == 'MeanShift':
                mean_dist = distance_stats.get('mean_distance', 1.0)
                optimal_bandwidth = mean_dist * 2.0
                optimal_params['bandwidth'] = {'calculated': round(optimal_bandwidth, 4), 'reasoning': "2x mean distance for appropriate kernel width"}
            
            elif algorithm == 'AffinityPropagation':
                optimal_params['damping'] = {'calculated': 0.5, 'reasoning': "Default damping factor"}

            # ── ENSEMBLE / TREE-BASED ────────────────────────────────────────
            elif algorithm in ('RandomForestClassifier', 'RandomForestRegressor'):
                n_est = min(200, max(50, n_samples // 50))
                max_d = max(3, min(20, int(math.log2(n_features + 1)) + 3))
                optimal_params['n_estimators'] = {'calculated': n_est, 'reasoning': "Scaled to dataset size (n//50, capped 50-200)"}
                optimal_params['max_depth'] = {'calculated': max_d, 'reasoning': "log2(n_features)+3, capped at 20"}
                optimal_params['min_samples_leaf'] = {'calculated': max(1, n_samples // 500), 'reasoning': "n//500 for regularisation"}

            elif algorithm in ('GradientBoostingClassifier', 'GradientBoostingRegressor'):
                n_est = min(300, max(50, n_samples // 30))
                optimal_params['n_estimators'] = {'calculated': n_est, 'reasoning': "Scaled to dataset size (n//30, capped 50-300)"}
                optimal_params['learning_rate'] = {'calculated': round(0.1 * (100 / max(n_est, 100)), 4), 'reasoning': "Lower LR for more estimators"}
                optimal_params['max_depth'] = {'calculated': 3, 'reasoning': "Shallow trees recommended for boosting"}
                optimal_params['subsample'] = {'calculated': 0.8 if n_samples > 1000 else 1.0, 'reasoning': "Stochastic GB for large datasets"}

            elif algorithm in ('XGBClassifier', 'XGBRegressor'):
                n_est = min(500, max(50, n_samples // 20))
                optimal_params['n_estimators'] = {'calculated': n_est, 'reasoning': "Scaled to dataset size (n//20, capped 50-500)"}
                optimal_params['learning_rate'] = {'calculated': 0.05 if n_est > 200 else 0.1, 'reasoning': "Lower LR for more rounds"}
                optimal_params['max_depth'] = {'calculated': 6, 'reasoning': "XGBoost default, good starting point"}
                optimal_params['subsample'] = {'calculated': 0.8, 'reasoning': "Stochastic boosting reduces overfitting"}
                optimal_params['colsample_bytree'] = {'calculated': max(0.5, round(math.sqrt(n_features) / n_features, 2)), 'reasoning': "sqrt(n_features)/n_features"}

            elif algorithm in ('LGBMClassifier', 'LGBMRegressor'):
                n_est = min(500, max(50, n_samples // 20))
                optimal_params['n_estimators'] = {'calculated': n_est, 'reasoning': "Scaled to dataset size"}
                optimal_params['learning_rate'] = {'calculated': 0.05 if n_est > 200 else 0.1, 'reasoning': "Lower LR for more rounds"}
                optimal_params['num_leaves'] = {'calculated': min(127, max(15, n_features * 2)), 'reasoning': "2*n_features, capped 15-127"}
                optimal_params['subsample'] = {'calculated': 0.8, 'reasoning': "Bagging fraction for regularisation"}

            elif algorithm in ('AdaBoostClassifier', 'AdaBoostRegressor'):
                n_est = min(200, max(30, n_samples // 50))
                optimal_params['n_estimators'] = {'calculated': n_est, 'reasoning': "Scaled to dataset size"}
                optimal_params['learning_rate'] = {'calculated': 0.5 if n_est > 100 else 1.0, 'reasoning': "Lower LR for more estimators"}

            elif algorithm in ('DecisionTreeClassifier', 'DecisionTreeRegressor'):
                max_d = max(3, min(15, int(math.log2(n_features + 1)) + 2))
                optimal_params['max_depth'] = {'calculated': max_d, 'reasoning': "log2(n_features)+2 prevents overfitting"}
                optimal_params['min_samples_leaf'] = {'calculated': max(1, n_samples // 200), 'reasoning': "n//200 for regularisation"}

            # ── SVM ──────────────────────────────────────────────────────────
            elif algorithm in ('SVC', 'SVR'):
                c_val = 1.0 if n_samples < 5000 else 10.0
                optimal_params['C'] = {'calculated': c_val, 'reasoning': "Start at 1.0; increase for larger datasets"}
                optimal_params['kernel'] = {'calculated': 'rbf', 'reasoning': "RBF handles non-linear boundaries well"}
                optimal_params['gamma'] = {'calculated': 'scale', 'reasoning': "scale=1/(n_features*X.var()) robust default"}
                if algorithm == 'SVR':
                    optimal_params['epsilon'] = {'calculated': 0.1, 'reasoning': "Standard epsilon tube width"}

            # ── LINEAR MODELS ─────────────────────────────────────────────────
            elif algorithm == 'LogisticRegression':
                optimal_params['C'] = {'calculated': 1.0, 'reasoning': "Default; tune via cross-validation"}
                optimal_params['max_iter'] = {'calculated': max(1000, n_samples // 10), 'reasoning': "More iterations for larger datasets"}
                optimal_params['solver'] = {'calculated': 'saga' if n_samples > 10000 else 'lbfgs', 'reasoning': "saga scales better for large n"}

            elif algorithm == 'Ridge':
                optimal_params['alpha'] = {'calculated': 1.0, 'reasoning': "Default regularisation; tune via CV"}

            elif algorithm == 'Lasso':
                optimal_params['alpha'] = {'calculated': 0.01, 'reasoning': "Small alpha; tune via CV"}
                optimal_params['max_iter'] = {'calculated': max(1000, n_samples // 5), 'reasoning': "More iterations for large data"}

            elif algorithm == 'ElasticNet':
                optimal_params['alpha'] = {'calculated': 0.01, 'reasoning': "Small alpha; tune via CV"}
                optimal_params['l1_ratio'] = {'calculated': 0.5, 'reasoning': "Equal L1/L2 mix — adjust based on sparsity"}

            # ── NEIGHBORS ────────────────────────────────────────────────────
            elif algorithm in ('KNeighborsClassifier', 'KNeighborsRegressor'):
                k = max(3, min(20, int(math.sqrt(n_samples))))
                if k % 2 == 0:
                    k += 1
                optimal_params['n_neighbors'] = {'calculated': k, 'reasoning': "sqrt(n_samples), odd to avoid ties"}
                optimal_params['weights'] = {'calculated': 'distance', 'reasoning': "Distance weighting improves accuracy"}
                optimal_params['algorithm'] = {'calculated': 'ball_tree' if n_features > 20 else 'kd_tree', 'reasoning': "ball_tree better for high-dim data"}

            # ── NEURAL NETWORKS ───────────────────────────────────────────────
            elif algorithm in ('MLPClassifier', 'MLPRegressor'):
                layer1 = max(32, min(512, n_features * 2))
                layer2 = max(16, layer1 // 2)
                optimal_params['hidden_layer_sizes'] = {'calculated': (layer1, layer2), 'reasoning': "(2*n_features, n_features) two-layer architecture"}
                optimal_params['activation'] = {'calculated': 'relu', 'reasoning': "ReLU avoids vanishing gradient"}
                optimal_params['solver'] = {'calculated': 'adam' if n_samples > 1000 else 'lbfgs', 'reasoning': "adam for large data, lbfgs for small"}
                optimal_params['max_iter'] = {'calculated': max(200, n_samples // 20), 'reasoning': "More epochs for larger datasets"}
                optimal_params['alpha'] = {'calculated': 0.0001, 'reasoning': "Default L2 regularisation"}

            return optimal_params

        
        def validate_parameters(algorithm, proposed_params, optimal_params, constraints):
            logger.info("")
            logger.info("Validating proposed parameters")
            
            validated_params = {}
            warnings = []
            recommendations = []
            
            for param_name, constraint in constraints.items():
                logger.info("  " + param_name + ":")
                
                if param_name in proposed_params:
                    value = proposed_params[param_name]
                    logger.info("    Proposed: " + str(value))
                    
                    expected_type = constraint['type']
                    if not isinstance(value, expected_type):
                        try:
                            value = expected_type(value)
                            warnings.append("Converted " + param_name + " to " + str(expected_type.__name__))
                        except:
                            warnings.append("Invalid type for " + param_name + ", using optimal")
                            value = optimal_params.get(param_name, {}).get('calculated')
                    
                    if 'options' in constraint and value not in constraint['options']:
                        warnings.append(param_name + " value " + str(value) + " not in " + str(constraint['options']))
                        value = constraint.get('default', constraint['options'][0])
                    
                    if 'min' in constraint and value < constraint['min']:
                        warnings.append(param_name + " below minimum, adjusting")
                        value = constraint['min']
                    
                    validated_params[param_name] = value
                    
                    if param_name in optimal_params:
                        optimal_value = optimal_params[param_name]['calculated']
                        if value != optimal_value:
                            recommendations.append(
                                param_name + ": Proposed=" + str(value) +
                                ", Optimal=" + str(optimal_value) +
                                " (" + optimal_params[param_name]['reasoning'] + ")"
                            )
                    
                    logger.info("    [OK] Validated: " + str(value))
                    
                elif param_name in optimal_params:
                    value = optimal_params[param_name]['calculated']
                    validated_params[param_name] = value
                    warnings.append("Missing " + param_name + ", using optimal: " + str(value))
                    logger.info("    Using optimal: " + str(value))
                    
                elif 'default' in constraint:
                    value = constraint['default']
                    validated_params[param_name] = value
                    logger.info("    Using default: " + str(value))
            
            return validated_params, warnings, recommendations
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--train_data", required=True)
            parser.add_argument("--distance_statistics", required=True)
            parser.add_argument("--algorithm", default='KMeans')
            parser.add_argument("--proposed_params", default='{}')
            parser.add_argument("--output_validated_params", required=True)
            parser.add_argument("--output_parameter_report", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("PARAMETER VALIDATION & OPTIMIZATION (PARQUET)")
            logger.info("="*80)
            logger.info("Algorithm: " + args.algorithm)
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_validated_params)
                ensure_directory_exists(args.output_parameter_report)
                
                proposed_params = parse_proposed_params(args.proposed_params)
                logger.info("Successfully parsed parameters: " + str(proposed_params))
                logger.info("")
                
                train_df = load_data(args.train_data)
                distance_stats = load_json(args.distance_statistics)
                
                if args.algorithm not in PARAMETER_CONSTRAINTS:
                    logger.error("ERROR: Unsupported algorithm: " + args.algorithm)
                    sys.exit(1)
                
                constraints = PARAMETER_CONSTRAINTS[args.algorithm]
                
                optimal_params = calculate_optimal_parameters(
                    args.algorithm,
                    train_df,
                    distance_stats
                )
                
                logger.info("")
                logger.info("Optimal parameters calculated:")
                for param, info in optimal_params.items():
                    logger.info("  " + param + ": " + str(info['calculated']) + " (" + info['reasoning'] + ")")
                
                validated_params, warnings, recommendations = validate_parameters(
                    args.algorithm,
                    proposed_params,
                    optimal_params,
                    constraints
                )
                
                parameter_report = {
                    'algorithm': args.algorithm,
                    'proposed_params': proposed_params,
                    'validated_params': validated_params,
                    'optimal_params': {k: v['calculated'] for k, v in optimal_params.items()},
                    'warnings': warnings,
                    'recommendations': recommendations,
                    'data_characteristics': {
                        'n_samples': len(train_df),
                        'n_features': len(train_df.columns),
                        'distance_statistics': distance_stats
                    }
                }
                
                logger.info("")
                logger.info("="*80)
                logger.info("VALIDATION SUMMARY")
                logger.info("="*80)
                logger.info("[OK] Validated parameters: " + str(validated_params))
                
                if warnings:
                    logger.info("")
                    logger.info("[WARN] Warnings:")
                    for warning in warnings:
                        logger.info("  - " + warning)
                
                if recommendations:
                    logger.info("")
                    logger.info("[TIP] Recommendations:")
                    for rec in recommendations:
                        logger.info("  - " + rec)
                
                with open(args.output_validated_params, 'w') as f:
                    json.dump(validated_params, f, indent=2)
                logger.info("Validated params saved: " + args.output_validated_params)
                
                with open(args.output_parameter_report, 'w') as f:
                    json.dump(parameter_report, f, indent=2)
                logger.info("Report saved: " + args.output_parameter_report)
                
                logger.info("")
                logger.info("[OK] Parameter validation complete!")
                logger.info("="*80)
                
            except Exception as e:
                logger.error("ERROR: " + str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --train_data
      - {inputPath: train_data}
      - --distance_statistics
      - {inputPath: distance_statistics}
      - --algorithm
      - {inputValue: algorithm}
      - --proposed_params
      - {inputValue: proposed_params}
      - --output_validated_params
      - {outputPath: validated_params}
      - --output_parameter_report
      - {outputPath: parameter_report}
