name: Generic Data Loading Component (PI API + CDN + MinIO) - PARQUET OPTIMIZED
description: Universal data loader supporting PI API with pagination, CDN multi-format downloads, and MinIO object storage. Handles PyTorch, NumPy, Parquet, CSV, Excel, TSV formats with automatic detection. SAVES AS PARQUET for 10x faster pipelines and 90% smaller files.

inputs:
  - name: data_source
    type: String
    description: 'Data source - PI API URL, CDN URL, or MinIO path (bucket/path/to/file)'
  - name: source_type
    type: String
    description: 'Source type: "pi_api", "cdn", or "minio". If empty, auto-detects based on URL/path'
    default: ''
  - name: access_token
    type: String
    description: 'Bearer access token for PI API or CDN authentication (file path or token string)'
    default: ''
  - name: minio_endpoint
    type: String
    description: 'MinIO endpoint URL (e.g., http://minio-service.kubeflow.svc.cluster.local:9000)'
    default: 'http://minio-service.kubeflow.svc.cluster.local:9000'
  - name: minio_access_key
    type: String
    description: 'MinIO access key (username)'
    default: 'minio'
  - name: minio_secret_key
    type: String
    description: 'MinIO secret key (password)'
    default: 'HN1YBS5WHS40M1CJ75FNKH0HEHR42I'
  - name: target_column
    type: String
    description: 'Target column name for ground truth extraction (empty = no ground truth)'
    default: ''
  - name: target_mapping
    type: String
    description: 'JSON mapping for categorical targets (e.g., {"class_a":0,"class_b":1})'
    default: '{}'
  - name: page_size
    type: String
    description: 'Number of records per page for PI API pagination (max 2000, ignored for CDN/MinIO)'
    default: '2000'
  - name: db_type
    type: String
    description: 'Database type for PI API (TIDB, MYSQL, etc. - ignored for CDN/MinIO)'
    default: 'TIDB'
  - name: feature_columns
    type: String
    description: 'Comma-separated list of feature columns to select. Example: "col1,col2,col3"'
    default: ''
  - name: exclude_columns
    type: String
    description: 'Comma-separated list of columns to exclude. Example: "id,timestamp,created_at"'
    default: ''

outputs:
  - name: data
    type: Data
    description: 'Loaded feature data (PARQUET format - 10x faster than CSV, 90% smaller, preserves dtypes)'
  - name: metadata
    type: Data
    description: 'Dataset metadata and validation results (JSON)'
  - name: ground_truth
    type: Data
    description: 'Ground truth labels if target_column specified (NPY format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import shutil
        import tempfile
        import subprocess
        import pandas as pd
        import numpy as np
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import urllib.parse as urlparse
        from urllib.parse import urlencode
        from pathlib import Path
        from io import StringIO, BytesIO
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('generic_data_loader')
        
        # PI API hard limit
        PI_API_MAX_PAGE_SIZE = 2000
        
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info(f"Created directory: {directory}")
        
        
        def detect_source_type(data_source):
            #Auto-detect if source is PI API, CDN, or MinIO#
            data_source_lower = data_source.lower()
            
            # PI API indicators
            if 'pi-entity-instances-service' in data_source_lower:
                return 'pi_api'
            if '/schemas/' in data_source_lower and '/instances/' in data_source_lower:
                return 'pi_api'
            if 'showreferenceddata' in data_source_lower or 'showdbaasreservedkeywords' in data_source_lower:
                return 'pi_api'
            
            # MinIO indicators (bucket/path format without http)
            if not data_source.startswith(('http://', 'https://')) and '/' in data_source:
                return 'minio'
            
            # CDN indicators (file extensions) - ADD .parquet and .pq
            cdn_extensions = ['.csv', '.xlsx', '.xls', '.parquet', '.pq', '.npy', '.npz', '.pt', '.pth', '.tsv', '.txt']
            if any(data_source_lower.endswith(ext) for ext in cdn_extensions):
                return 'cdn'
            
            # Check for CDN domains
            cdn_patterns = ['cdn', 's3.amazonaws.com', 'blob.core.windows.net', 'storage.googleapis.com', 'cloudfront']
            if any(pattern in data_source_lower for pattern in cdn_patterns):
                return 'cdn'
            
            # Default to MinIO if it has bucket-like structure
            if '/' in data_source and not data_source.startswith('http'):
                logger.warning("Ambiguous format, defaulting to MinIO (bucket/path)")
                return 'minio'
            
            # Final fallback to PI API
            logger.warning("Could not definitively detect source type, defaulting to PI API")
            return 'pi_api'
        
        
        def install_minio_client():
            #Install MinIO client when needed#
            if shutil.which('mc'):
                logger.info("[OK] MinIO client already available")
                return True
            
            logger.info("Installing MinIO client...")
            
            # Try wget first, fallback to curl
            download_cmd = None
            if shutil.which('wget'):
                download_cmd = ['wget', '-q', 'https://dl.min.io/client/mc/release/linux-amd64/mc', '-O', '/tmp/mc']
            elif shutil.which('curl'):
                download_cmd = ['curl', '-sL', 'https://dl.min.io/client/mc/release/linux-amd64/mc', '-o', '/tmp/mc']
            else:
                logger.error("Neither wget nor curl available - cannot install MinIO client")
                return False
            
            try:
                subprocess.run(download_cmd, check=True, capture_output=True)
                os.chmod('/tmp/mc', 0o755)
                shutil.move('/tmp/mc', '/usr/local/bin/mc')
                logger.info("[OK] MinIO client installed successfully")
                return True
            except Exception as e:
                logger.error(f"Failed to install MinIO client: {e}")
                return False
        
        
        def configure_minio_client(endpoint, access_key, secret_key, alias='myminio'):
            #Configure MinIO client with credentials#
            logger.info("="*80)
            logger.info("CONFIGURING MINIO CLIENT")
            logger.info("="*80)
            logger.info(f"Endpoint: {endpoint}")
            logger.info(f"Access Key: {access_key}")
            logger.info(f"Alias: {alias}")
            
            try:
                cmd = [
                    'mc', 'alias', 'set', alias,
                    endpoint, access_key, secret_key
                ]
                
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                logger.info(f"[OK] MinIO client configured successfully")
                logger.info(f"  {result.stdout.strip()}")
                logger.info("")
                return alias
                
            except subprocess.CalledProcessError as e:
                logger.error(f"Failed to configure MinIO client: {e.stderr}")
                raise
        
        
        def fetch_from_minio(minio_path, minio_alias, endpoint, access_key, secret_key):
            #Fetch data from MinIO object storage#
            logger.info("="*80)
            logger.info("FETCHING DATA FROM MINIO")
            logger.info("="*80)
            logger.info(f"MinIO Path: {minio_path}")
            logger.info(f"Endpoint: {endpoint}")
            
            # Install MinIO client if needed
            if not install_minio_client():
                raise RuntimeError("MinIO client not available and could not be installed")
            
            try:
                # Configure MinIO client
                alias = configure_minio_client(endpoint, access_key, secret_key, minio_alias)
                
                # Parse MinIO path (bucket/path/to/file)
                parts = minio_path.split('/', 1)
                if len(parts) < 2:
                    raise ValueError(f"Invalid MinIO path format. Expected 'bucket/path/to/file', got '{minio_path}'")
                
                bucket = parts[0]
                file_path = parts[1]
                
                logger.info(f"Bucket: {bucket}")
                logger.info(f"File: {file_path}")
                
                # Check if source exists
                source_path = f"{alias}/{bucket}/{file_path}"
                check_cmd = ['mc', 'stat', source_path]
                
                try:
                    subprocess.run(check_cmd, capture_output=True, text=True, check=True)
                    logger.info("[OK] Source file exists in MinIO")
                except subprocess.CalledProcessError:
                    logger.error(f"Source file not found: {source_path}")
                    raise FileNotFoundError(f"MinIO file not found: {minio_path}")
                
                # Create temporary file for download
                fd, temp_file = tempfile.mkstemp()
                os.close(fd)
                
                try:
                    # Download from MinIO
                    logger.info(f"Downloading from MinIO...")
                    download_cmd = ['mc', 'cp', source_path, temp_file]
                    
                    result = subprocess.run(
                        download_cmd,
                        capture_output=True,
                        text=True,
                        check=True
                    )
                    
                    file_size_mb = os.path.getsize(temp_file) / (1024 * 1024)
                    logger.info(f"[OK] Downloaded {file_size_mb:.2f} MB")
                    logger.info("")
                    
                    # Use multi-format loader
                    df = load_data_multi_format(temp_file)
                    
                    return df
                    
                finally:
                    # Clean up temp file
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                        logger.info("Cleaned up temporary file")
                
            except subprocess.CalledProcessError as e:
                logger.error(f"MinIO command failed: {e.stderr}")
                raise
            except Exception as e:
                logger.error(f"Error loading data from MinIO: {e}")
                import traceback
                traceback.print_exc()
                raise
        
        
        def load_data_multi_format(file_path):
            #Robust multi-format data loader with fallback strategies
            #Supports: Parquet (PRIORITY!), PyTorch, NumPy, CSV, Excel, TSV
            logger.info("="*80)
            logger.info("MULTI-FORMAT DATA LOADING")
            logger.info("="*80)
            logger.info(f"File path: {file_path}")
            
            # List of loading strategies to try (PARQUET FIRST for optimal performance!)
            strategies = [
                # Strategy 1: Parquet (BEST for ML - try FIRST!)
                {
                    'name': 'Parquet',
                    'func': lambda: pd.read_parquet(file_path, engine="pyarrow")
                },
                # Strategy 2: PyTorch Tensor
                {
                    'name': 'PyTorch Tensor (.pt/.pth)',
                    'func': lambda: load_pytorch_format(file_path)
                },
                # Strategy 3: NumPy array (.npy)
                {
                    'name': 'NumPy array (.npy)',
                    'func': lambda: np.load(file_path, allow_pickle=True)
                },
                # Strategy 4: NumPy compressed (.npz)
                {
                    'name': 'NumPy compressed (.npz)',
                    'func': lambda: load_npz_format(file_path)
                },
                # Strategy 5: CSV
                {
                    'name': 'CSV',
                    'func': lambda: pd.read_csv(file_path)
                },
                # Strategy 6: Excel
                {
                    'name': 'Excel (.xlsx/.xls)',
                    'func': lambda: pd.read_excel(file_path)
                },
                # Strategy 7: Tab-separated values
                {
                    'name': 'TSV',
                    'func': lambda: pd.read_csv(file_path, sep='\t')
                },
                # Strategy 8: Space-separated values
                {
                    'name': 'Space-separated',
                    'func': lambda: pd.read_csv(file_path, sep='\s+')
                },
                # Strategy 9: Semicolon-separated
                {
                    'name': 'Semicolon-separated',
                    'func': lambda: pd.read_csv(file_path, sep=';')
                },
            ]
            
            # Try each strategy
            last_error = None
            for strategy in strategies:
                try:
                    logger.info(f"Attempting: {strategy['name']}")
                    data = strategy['func']()
                    
                    # Convert to DataFrame if needed
                    if isinstance(data, np.ndarray):
                        logger.info(f"  Converting NumPy array to DataFrame")
                        data = pd.DataFrame(data)
                    elif not isinstance(data, pd.DataFrame):
                        logger.info(f"  Converting {type(data).__name__} to DataFrame")
                        data = pd.DataFrame(data)
                    
                    logger.info(f"[OK] Successfully loaded as {strategy['name']}")
                    logger.info(f"  Shape: {data.shape[0]} rows x {data.shape[1]} columns")
                    logger.info("")
                    return data
                    
                except Exception as e:
                    logger.debug(f"  Failed: {str(e)}")
                    last_error = e
                    continue
            
            # If all strategies failed, raise the last error
            logger.error("="*80)
            logger.error("ALL LOADING STRATEGIES FAILED")
            logger.error("="*80)
            logger.error(f"Last error: {last_error}")
            raise RuntimeError(f"Could not load file with any supported format. Last error: {last_error}")
        
        
        def load_pytorch_format(file_path):
            #Load PyTorch tensors and convert to DataFrame#
            try:
                import torch
            except ImportError:
                raise ImportError("PyTorch not available")
            
            data = torch.load(file_path, map_location='cpu')
            
            # Handle different PyTorch structures
            if isinstance(data, torch.Tensor):
                return data.numpy()
            elif isinstance(data, dict):
                # Try common keys
                for key in ['data', 'X', 'tensor', 'values']:
                    if key in data:
                        tensor = data[key]
                        if isinstance(tensor, torch.Tensor):
                            return tensor.numpy()
                # If no common key, try first tensor
                for value in data.values():
                    if isinstance(value, torch.Tensor):
                        return value.numpy()
            
            raise ValueError("Could not extract tensor from PyTorch file")
        
        
        def load_npz_format(file_path):
            #Load NumPy .npz compressed format#
            npz_data = np.load(file_path, allow_pickle=True)
            
            # Try common keys
            for key in ['data', 'arr_0', 'X', 'values']:
                if key in npz_data:
                    return npz_data[key]
            
            # Return first array
            arrays = list(npz_data.keys())
            if arrays:
                logger.info(f"  Using array: {arrays[0]}")
                return npz_data[arrays[0]]
            
            raise ValueError("No arrays found in .npz file")
        
        
        def fetch_from_cdn(data_source, access_token):
            #Fetch data from CDN URL with multi-format support#
            logger.info("="*80)
            logger.info("FETCHING DATA FROM CDN")
            logger.info("="*80)
            logger.info(f"CDN URL: {data_source}")
            logger.info("")
            
            try:
                # Setup session with retry logic
                session = requests.Session()
                retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
                session.mount("https://", HTTPAdapter(max_retries=retries))
                session.mount("http://", HTTPAdapter(max_retries=retries))
                
                # Setup headers with bearer token if provided
                headers = {}
                if access_token:
                    # Check if token is a file path
                    if os.path.exists(access_token):
                        with open(access_token, 'r') as f:
                            token = f.read().strip()
                            if token:
                                headers["Authorization"] = f"Bearer {token}"
                                logger.info("Using bearer token from file")
                    else:
                        # Use token directly
                        headers["Authorization"] = f"Bearer {access_token}"
                        logger.info("Using provided bearer token")
                
                logger.info("Downloading file from CDN...")
                resp = session.get(data_source, headers=headers, timeout=300)
                resp.raise_for_status()
                
                file_size_mb = len(resp.content) / (1024 * 1024)
                logger.info(f"Downloaded {file_size_mb:.2f} MB")
                logger.info("")
                
                # Save to temporary file for multi-format loading
                fd, temp_file = tempfile.mkstemp()
                os.close(fd)
                
                try:
                    with open(temp_file, 'wb') as f:
                        f.write(resp.content)
                    
                    logger.info(f"Saved to temporary file: {temp_file}")
                    
                    # Use multi-format loader
                    df = load_data_multi_format(temp_file)
                    
                    return df
                    
                finally:
                    # Clean up temp file
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                        logger.info("Cleaned up temporary file")
                
            except requests.exceptions.Timeout:
                logger.error("Request timed out while downloading from CDN")
                raise
            except requests.exceptions.RequestException as e:
                logger.error(f"Network error during CDN download: {e}")
                raise
            except Exception as e:
                logger.error(f"Error loading data from CDN: {e}")
                import traceback
                traceback.print_exc()
                raise
        
        
        def fetch_from_pi_api(api_url, access_token, page_size, db_type):
            #Fetch data from PI API with smart pagination#
            logger.info("="*80)
            logger.info("FETCHING DATA FROM PI API")
            logger.info("="*80)
            logger.info(f"API URL: {api_url}")
            logger.info(f"Requested page size: {page_size}")
            logger.info(f"DB Type: {db_type}")
            
            # Cap page size at PI API maximum limit
            actual_page_size = min(int(page_size), PI_API_MAX_PAGE_SIZE)
            
            if int(page_size) > PI_API_MAX_PAGE_SIZE:
                logger.warning(f"Requested page_size ({page_size}) exceeds PI API limit ({PI_API_MAX_PAGE_SIZE})")
                logger.warning(f"Will use maximum allowed: {actual_page_size}")
            
            logger.info(f"Actual page size: {actual_page_size}")
            logger.info("")
            
            # Read token from file if it's a path
            if access_token and os.path.exists(access_token):
                with open(access_token, 'r') as f:
                    access_token = f.read().strip()
                logger.info("Access token loaded from file")
            
            # Setup session with retry logic
            session = requests.Session()
            retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
            session.mount("https://", HTTPAdapter(max_retries=retries))
            session.mount("http://", HTTPAdapter(max_retries=retries))
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {access_token}"
            }
            
            # Parse URL and prepare pagination
            parsed_url = urlparse.urlparse(api_url)
            url_params = urlparse.parse_qs(parsed_url.query)
            url_params.pop('page', None)
            url_params.pop('size', None)
            
            all_records = []
            current_page = 0
            
            logger.info("Starting pagination...")
            
            while True:
                # Update pagination parameters with ACTUAL page size
                url_params['page'] = [str(current_page)]
                url_params['size'] = [str(actual_page_size)]
                new_query = urlencode(url_params, doseq=True)
                request_url = parsed_url._replace(query=new_query).geturl()
                
                payload = {"dbType": db_type, "limit": actual_page_size, "offset": current_page * actual_page_size}
                
                try:
                    logger.info(f"[Page {current_page}] Requesting {actual_page_size} records (offset: {current_page * actual_page_size})...")
                    
                    resp = session.post(request_url, headers=headers, json=payload, timeout=60)
                    resp.raise_for_status()
                    
                    json_response = resp.json()
                    
                    # Handle different response structures
                    if isinstance(json_response, dict):
                        batch = json_response.get('content', json_response.get('data', json_response.get('instances', [])))
                    else:
                        batch = json_response
                    
                    batch_len = len(batch) if isinstance(batch, list) else 0
                    
                    # Stop if no data
                    if batch_len == 0:
                        logger.info(f"[Page {current_page}] No records returned - reached end of data")
                        break
                    
                    # Extend records and continue
                    all_records.extend(batch)
                    logger.info(f"[Page {current_page}] Fetched {batch_len} records. Total so far: {len(all_records)}")
                    
                    # Compare against ACTUAL page size (not user-requested)
                    if batch_len < actual_page_size:
                        logger.info(f"Received {batch_len} < {actual_page_size} records - last page reached")
                        break
                    
                    current_page += 1
                
                except requests.exceptions.HTTPError as e:
                    logger.error(f"HTTP Error: {e}")
                    logger.error(f"Response: {resp.text if 'resp' in locals() else 'N/A'}")
                    raise
                except requests.exceptions.RequestException as e:
                    logger.error(f"Request Error: {e}")
                    raise
                except Exception as e:
                    logger.error(f"Unexpected Error: {e}")
                    import traceback
                    traceback.print_exc()
                    raise
            
            logger.info("")
            logger.info(f"Pagination complete: {len(all_records)} total records fetched across {current_page + 1} page(s)")
            logger.info("")
            
            if not all_records:
                raise ValueError("No records fetched from PI API")
            
            df = pd.DataFrame(all_records)
            logger.info(f"Created DataFrame: {df.shape[0]} rows x {df.shape[1]} columns")
            
            return df
        
        
        def optimize_dtypes(df):
            #Optimize DataFrame dtypes for Parquet efficiency
            #Converts high-cardinality objects to categories, downcasts numerics
            logger.info("="*80)
            logger.info("OPTIMIZING DTYPES FOR PARQUET")
            logger.info("="*80)
            
            initial_memory = df.memory_usage(deep=True).sum() / 1024**2
            logger.info(f"Initial memory: {initial_memory:.2f} MB")
            
            optimizations = []
            
            # Convert object columns to category (huge space savings in Parquet)
            for col in df.select_dtypes(include=['object']).columns:
                nunique = df[col].nunique()
                if nunique < 0.5 * len(df):  # Less than 50% unique values
                    df[col] = df[col].astype('category')
                    optimizations.append(f"{col}: object -> category ({nunique} unique)")
            
            # Downcast numeric types
            for col in df.select_dtypes(include=['int64']).columns:
                df[col] = pd.to_numeric(df[col], downcast='integer')
            
            for col in df.select_dtypes(include=['float64']).columns:
                df[col] = pd.to_numeric(df[col], downcast='float')
            
            final_memory = df.memory_usage(deep=True).sum() / 1024**2
            savings = ((initial_memory - final_memory) / initial_memory) * 100 if initial_memory > 0 else 0
            
            logger.info(f"Final memory: {final_memory:.2f} MB")
            logger.info(f"Memory saved: {savings:.1f}%")
            
            if optimizations:
                logger.info("Optimizations applied:")
                for opt in optimizations[:10]:  # Show first 10
                    logger.info(f"  [OK] {opt}")
                if len(optimizations) > 10:
                    logger.info(f"  ... and {len(optimizations) - 10} more")
            
            logger.info("")
            
            return df
        
        
        def select_features(df, feature_columns, exclude_columns):
            logger.info("="*80)
            logger.info("FEATURE SELECTION")
            logger.info("="*80)
            
            original_columns = df.columns.tolist()
            logger.info(f"Original columns: {len(original_columns)}")
            
            selection_metadata = {'original_columns': original_columns, 'original_count': len(original_columns)}
            
            # Parse feature columns (whitelist)
            selected_features = []
            if feature_columns and feature_columns.strip():
                selected_features = [col.strip() for col in feature_columns.split(',') if col.strip()]
                logger.info(f"Feature whitelist specified: {len(selected_features)} columns")
                
                missing_cols = [col for col in selected_features if col not in df.columns]
                if missing_cols:
                    available = ', '.join(original_columns)
                    raise ValueError(f"Specified features not found: {missing_cols}. Available columns: {available}")
                
                df = df[selected_features]
                logger.info(f"Selected {len(selected_features)} specified features")
                selection_metadata['selection_type'] = 'whitelist'
                selection_metadata['selected_features'] = selected_features
            
            # Parse exclude columns (blacklist)
            excluded_features = []
            if exclude_columns and exclude_columns.strip():
                excluded_features = [col.strip() for col in exclude_columns.split(',') if col.strip()]
                logger.info(f"Exclude list specified: {len(excluded_features)} columns")
                
                existing_excluded = [col for col in excluded_features if col in df.columns]
                missing_excluded = [col for col in excluded_features if col not in df.columns]
                
                if missing_excluded:
                    logger.warning(f"Some exclude columns not found (skipped): {missing_excluded}")
                
                if existing_excluded:
                    df = df.drop(columns=existing_excluded)
                    logger.info(f"Excluded {len(existing_excluded)} columns: {existing_excluded}")
                    selection_metadata['excluded_features'] = existing_excluded
                else:
                    logger.info("No columns to exclude (none found in data)")
                    selection_metadata['excluded_features'] = []
                
                if not selection_metadata.get('selection_type'):
                    selection_metadata['selection_type'] = 'blacklist'
            
            if not feature_columns and not exclude_columns:
                logger.info("No feature selection specified - using all columns")
                selection_metadata['selection_type'] = 'all'
            
            final_columns = df.columns.tolist()
            selection_metadata['final_columns'] = final_columns
            selection_metadata['final_count'] = len(final_columns)
            
            logger.info(f"Final feature set: {len(final_columns)} columns")
            
            removed_cols = set(original_columns) - set(final_columns)
            if removed_cols:
                logger.info(f"Removed columns: {sorted(removed_cols)}")
            
            logger.info("")
            
            return df, selection_metadata
        
        
        def extract_ground_truth(df, target_column, target_mapping_json):
            if not target_column or target_column == '':
                logger.info("No target column specified - skipping ground truth extraction")
                return df, None, None
            
            logger.info("="*80)
            logger.info("EXTRACTING GROUND TRUTH")
            logger.info("="*80)
            logger.info(f"Target column: '{target_column}'")
            
            if target_column not in df.columns:
                available_cols = ', '.join(df.columns.tolist())
                raise ValueError(f"Target column '{target_column}' not found. Available columns: {available_cols}")
            
            target_series = df[target_column].copy()
            logger.info(f"Found {len(target_series)} target values")
            
            target_mapping = {}
            if target_mapping_json and target_mapping_json != '{}':
                try:
                    target_mapping = json.loads(target_mapping_json)
                    logger.info(f"Using custom mapping: {target_mapping}")
                except json.JSONDecodeError:
                    logger.warning("Invalid target_mapping JSON, will auto-encode")
                    target_mapping = {}
            
            ground_truth_info = {
                'column_name': target_column,
                'n_samples': int(len(target_series)),
                'unique_values': target_series.unique().tolist()[:50],
                'n_unique': int(target_series.nunique()),
                'dtype': str(target_series.dtype)
            }
            
            if pd.api.types.is_numeric_dtype(target_series):
                logger.info("Target column is already numeric")
                ground_truth_encoded = target_series.values
                ground_truth_info['encoding'] = 'none (already numeric)'
                ground_truth_info['mapping'] = {}
            elif target_mapping:
                logger.info("Applying custom mapping")
                ground_truth_encoded = target_series.map(target_mapping).values
                if pd.isna(ground_truth_encoded).any():
                    unmapped = target_series[pd.isna(ground_truth_encoded)].unique()
                    raise ValueError(f"Custom mapping incomplete. Unmapped values: {unmapped.tolist()}")
                ground_truth_info['encoding'] = 'custom'
                ground_truth_info['mapping'] = target_mapping
            else:
                logger.info("Auto-encoding categorical target")
                unique_vals = sorted(target_series.unique())
                auto_mapping = {val: idx for idx, val in enumerate(unique_vals)}
                ground_truth_encoded = target_series.map(auto_mapping).values
                ground_truth_info['encoding'] = 'auto (alphabetical)'
                ground_truth_info['mapping'] = auto_mapping
                logger.info(f"Auto-generated mapping: {auto_mapping}")
            
            ground_truth_encoded = ground_truth_encoded.astype(int)
            ground_truth_info['encoded_range'] = [int(ground_truth_encoded.min()), int(ground_truth_encoded.max())]
            ground_truth_info['encoded_unique'] = int(len(np.unique(ground_truth_encoded)))
            
            logger.info("Ground truth distribution:")
            for label in np.unique(ground_truth_encoded):
                count = np.sum(ground_truth_encoded == label)
                pct = count / len(ground_truth_encoded) * 100
                logger.info(f"  Class {label}: {count:>6} samples ({pct:>5.1f}%)")
            
            df_features = df.drop(columns=[target_column])
            logger.info(f"Removed target column: {df_features.shape[1]} features remain")
            logger.info("")
            
            return df_features, ground_truth_encoded, ground_truth_info
        
        
        def validate_dataset(df):
            logger.info("="*80)
            logger.info("VALIDATING DATASET")
            logger.info("="*80)
            
            validation = {'is_valid': True, 'errors': [], 'warnings': []}
            
            if df.empty:
                validation['is_valid'] = False
                validation['errors'].append("Dataset is empty")
                logger.error("Dataset is empty")
                return validation, {}
            
            if len(df) < 2:
                validation['is_valid'] = False
                validation['errors'].append("Need at least 2 samples for clustering")
                logger.error(f"Insufficient samples: {len(df)}")
                return validation, {}
            
            logger.info(f"Dataset has {len(df)} samples")
            
            if df.columns.duplicated().any():
                validation['warnings'].append("Duplicate column names detected")
                logger.warning("Duplicate column names found")
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            logger.info(f"Numeric columns: {len(numeric_cols)}")
            logger.info(f"Categorical columns: {len(categorical_cols)}")
            
            if len(numeric_cols) == 0:
                validation['warnings'].append("No numeric columns found")
                logger.warning("No numeric columns for clustering")
            
            missing_total = df.isnull().sum().sum()
            missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100 if df.shape[0] * df.shape[1] > 0 else 0
            
            missing_by_col = {}
            if missing_total > 0:
                msg = f"Found {missing_total} missing values ({missing_pct:.2f}%)"
                validation['warnings'].append(msg)
                logger.warning(msg)
                for col in df.columns:
                    miss_count = df[col].isnull().sum()
                    if miss_count > 0:
                        missing_by_col[col] = int(miss_count)
                        miss_pct = miss_count/len(df)*100
                        logger.info(f"  {col}: {miss_count} ({miss_pct:.1f}%)")
            else:
                logger.info("No missing values")
            
            inf_count = 0
            inf_by_col = {}
            for col in numeric_cols:
                col_inf = df[col].isin([np.inf, -np.inf]).sum()
                if col_inf > 0:
                    inf_by_col[col] = int(col_inf)
                    inf_count += col_inf
            
            if inf_count > 0:
                msg = f"Found {inf_count} infinite values"
                validation['warnings'].append(msg)
                logger.warning(msg)
                for col, count in inf_by_col.items():
                    logger.info(f"  {col}: {count}")
            else:
                logger.info("No infinite values")
            
            constant_cols = [col for col in df.columns if df[col].nunique() <= 1]
            if constant_cols:
                msg = f"Constant columns found: {len(constant_cols)}"
                validation['warnings'].append(msg)
                logger.warning(msg)
                logger.info(f"  Constant columns: {constant_cols}")
            else:
                logger.info("No constant columns")
            
            high_cardinality_cols = []
            for col in categorical_cols:
                unique_count = df[col].nunique()
                if unique_count > 0.5 * len(df):
                    high_cardinality_cols.append(col)
            
            if high_cardinality_cols:
                msg = f"High cardinality categorical columns: {high_cardinality_cols}"
                validation['warnings'].append(msg)
                logger.warning(msg)
            
            metadata = {
                'n_samples': int(df.shape[0]),
                'n_features': int(df.shape[1]),
                'columns': list(df.columns),
                'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
                'numeric_columns': numeric_cols,
                'categorical_columns': categorical_cols,
                'missing_values': {'total': int(missing_total), 'percentage': float(missing_pct), 'by_column': missing_by_col},
                'infinite_values': {'total': int(inf_count), 'by_column': inf_by_col},
                'constant_columns': constant_cols,
                'high_cardinality_columns': high_cardinality_cols,
                'memory_mb': float(df.memory_usage(deep=True).sum() / 1024**2)
            }
            
            if numeric_cols:
                metadata['statistics'] = {}
                sample_cols = numeric_cols[:20]
                for col in sample_cols:
                    try:
                        metadata['statistics'][col] = {
                            'mean': float(df[col].mean()),
                            'std': float(df[col].std()),
                            'min': float(df[col].min()),
                            'max': float(df[col].max()),
                            'median': float(df[col].median()),
                            'q25': float(df[col].quantile(0.25)),
                            'q75': float(df[col].quantile(0.75))
                        }
                    except:
                        logger.warning(f"Could not compute statistics for {col}")
                        pass
                
                if len(numeric_cols) > 20:
                    logger.info(f"Statistics computed for first 20 numeric columns (of {len(numeric_cols)})")
            
            metadata['validation'] = validation
            
            logger.info("")
            logger.info("Validation Summary:")
            logger.info(f"  Valid: {validation['is_valid']}")
            logger.info(f"  Errors: {len(validation['errors'])}")
            logger.info(f"  Warnings: {len(validation['warnings'])}")
            logger.info("")
            
            return validation, metadata
        
        
        def main():
            parser = argparse.ArgumentParser(description="Generic Data Loading Component (PARQUET OPTIMIZED)")
            parser.add_argument("--data_source", required=True, help="Data source URL or MinIO path")
            parser.add_argument("--source_type", default='', help="Source type: pi_api, cdn, or minio")
            parser.add_argument("--access_token", default='', help="Bearer access token for PI API or CDN")
            parser.add_argument("--minio_endpoint", default='http://minio-service.kubeflow.svc.cluster.local:9000', help="MinIO endpoint")
            parser.add_argument("--minio_access_key", default='minio', help="MinIO access key")
            parser.add_argument("--minio_secret_key", default='HN1YBS5WHS40M1CJ75FNKH0HEHR42I', help="MinIO secret key")
            parser.add_argument("--target_column", default='', help="Target column for ground truth")
            parser.add_argument("--target_mapping", default='{}', help="JSON mapping for categorical targets")
            parser.add_argument("--page_size", default='2000', help="Records per page for PI API pagination")
            parser.add_argument("--db_type", default='TIDB', help="Database type for PI API")
            parser.add_argument("--feature_columns", default='', help="Comma-separated list of features to select")
            parser.add_argument("--exclude_columns", default='', help="Comma-separated list of columns to exclude")
            parser.add_argument("--output_data", required=True, help="Output path for feature data")
            parser.add_argument("--output_metadata", required=True, help="Output path for metadata JSON")
            parser.add_argument("--output_ground_truth", required=True, help="Output path for ground truth labels")
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("GENERIC DATA LOADING COMPONENT (PARQUET OPTIMIZED)")
            logger.info("PI API + CDN + MinIO | Multi-Format Support -> PARQUET Output")
            logger.info("="*80)
            logger.info(f"Data source: {args.data_source}")
            
            try:
                ensure_directory_exists(args.output_data)
                ensure_directory_exists(args.output_metadata)
                ensure_directory_exists(args.output_ground_truth)
                
                # Determine source type
                if args.source_type and args.source_type.strip():
                    source_type = args.source_type.strip().lower()
                    logger.info(f"Source type (user-specified): {source_type}")
                else:
                    source_type = detect_source_type(args.data_source)
                    logger.info(f"Source type (auto-detected): {source_type}")
                
                logger.info("")
                
                # Fetch data based on source type
                if source_type == 'minio':
                    df = fetch_from_minio(
                        args.data_source,
                        'myminio',
                        args.minio_endpoint,
                        args.minio_access_key,
                        args.minio_secret_key
                    )
                    source_metadata = {
                        'type': 'MinIO',
                        'path': args.data_source,
                        'endpoint': args.minio_endpoint,
                        'format': 'multi-format (auto-detected)'
                    }
                elif source_type == 'cdn':
                    df = fetch_from_cdn(args.data_source, args.access_token)
                    source_metadata = {
                        'type': 'CDN',
                        'url': args.data_source,
                        'format': 'multi-format (auto-detected)'
                    }
                elif source_type == 'pi_api':
                    page_size = int(args.page_size)
                    df = fetch_from_pi_api(args.data_source, args.access_token, page_size, args.db_type)
                    source_metadata = {
                        'type': 'PI_API',
                        'api_url': args.data_source,
                        'db_type': args.db_type,
                        'page_size': page_size
                    }
                else:
                    raise ValueError(f"Unknown source type: {source_type}. Must be 'pi_api', 'cdn', or 'minio'")
                
                # Optimize dtypes for Parquet efficiency
                df = optimize_dtypes(df)
                
                # Select features
                df, selection_metadata = select_features(df, args.feature_columns, args.exclude_columns)
                
                # Extract ground truth
                df_features, ground_truth, ground_truth_info = extract_ground_truth(df, args.target_column, args.target_mapping)
                
                # Validate dataset
                validation, metadata = validate_dataset(df_features)
                
                if not validation['is_valid']:
                    logger.error("="*80)
                    logger.error("VALIDATION FAILED")
                    logger.error("="*80)
                    for error in validation['errors']:
                        logger.error(f"Error: {error}")
                    sys.exit(1)
                
                if validation['warnings']:
                    logger.warning(f"Found {len(validation['warnings'])} warnings")
                    for warning in validation['warnings']:
                        logger.warning(f"  - {warning}")
                
                # Add metadata
                if ground_truth_info:
                    metadata['ground_truth'] = ground_truth_info
                metadata['feature_selection'] = selection_metadata
                metadata['data_source'] = source_metadata
                
                # ============================================================
                # CRITICAL: SAVE AS PARQUET (NOT CSV!)
                # ============================================================
                output_parquet = args.output_data
                if not output_parquet.endswith('.parquet'):
                    output_parquet = output_parquet.replace('.csv', '.parquet')
                
                logger.info("="*80)
                logger.info("SAVING AS PARQUET")
                logger.info("="*80)
                
                df_features.to_parquet(
                    output_parquet,
                    index=False,
                    engine='pyarrow',
                    compression='snappy'
                )
                
                parquet_size = os.path.getsize(output_parquet) / 1024**2
                logger.info(f"[OK] Feature data saved as PARQUET: {parquet_size:.2f} MB")
                logger.info(f"  Shape: {df_features.shape}")
                logger.info(f"  Path: {output_parquet}")
                logger.info(f"  Compression: snappy")
                logger.info(f"  Engine: pyarrow")
                
                # Add Parquet metadata
                metadata['output_format'] = {
                    'format': 'parquet',
                    'engine': 'pyarrow',
                    'compression': 'snappy',
                    'file_size_mb': float(parquet_size)
                }
                
                with open(args.output_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                logger.info(f"[OK] Metadata saved: {args.output_metadata}")
                
                if ground_truth is not None:
                    np.save(args.output_ground_truth, ground_truth, allow_pickle=False)
                    npy_path = args.output_ground_truth + '.npy'
                    if os.path.exists(npy_path):
                        shutil.move(npy_path, args.output_ground_truth)
                    logger.info(f"[OK] Ground truth saved: {args.output_ground_truth}")
                    logger.info(f"  Shape: {ground_truth.shape}")
                else:
                    with open(args.output_ground_truth, 'w') as f:
                        f.write("")
                    logger.info("[OK] No ground truth (empty marker file created)")
                
                logger.info("")
                logger.info("="*80)
                logger.info("DATA LOADING COMPLETED SUCCESSFULLY")
                logger.info("="*80)
                logger.info(f"Source: {source_type.upper()}")
                logger.info(f"Samples: {metadata['n_samples']}")
                logger.info(f"Features: {metadata['n_features']}")
                logger.info(f"  - Numeric: {len(metadata['numeric_columns'])}")
                logger.info(f"  - Categorical: {len(metadata['categorical_columns'])}")
                if ground_truth_info:
                    logger.info(f"Ground truth: {ground_truth_info['n_unique']} classes")
                logger.info(f"Output format: PARQUET (10x faster than CSV)")
                logger.info(f"File size: {parquet_size:.2f} MB")
                logger.info(f"Memory: {metadata['memory_mb']:.2f} MB")
                logger.info("="*80)
                
            except subprocess.CalledProcessError as e:
                logger.error(f"COMMAND ERROR: {str(e)}")
                logger.error(f"stderr: {e.stderr if hasattr(e, 'stderr') else 'N/A'}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except requests.exceptions.RequestException as e:
                logger.error(f"NETWORK ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except ValueError as e:
                logger.error(f"VALIDATION ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
      - --data_source
      - {inputValue: data_source}
      - --source_type
      - {inputValue: source_type}
      - --access_token
      - {inputValue: access_token}
      - --minio_endpoint
      - {inputValue: minio_endpoint}
      - --minio_access_key
      - {inputValue: minio_access_key}
      - --minio_secret_key
      - {inputValue: minio_secret_key}
      - --target_column
      - {inputValue: target_column}
      - --target_mapping
      - {inputValue: target_mapping}
      - --page_size
      - {inputValue: page_size}
      - --db_type
      - {inputValue: db_type}
      - --feature_columns
      - {inputValue: feature_columns}
      - --exclude_columns
      - {inputValue: exclude_columns}
      - --output_data
      - {outputPath: data}
      - --output_metadata
      - {outputPath: metadata}
      - --output_ground_truth
      - {outputPath: ground_truth}
