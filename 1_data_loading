name: Generic Data Loading Component (PARQUET OPTIMIZED)
description: Universal data loader supporting PI API, CDN, and MinIO. Loads any format (PyTorch, NumPy, Parquet, CSV, Excel, TSV) and SAVES AS PARQUET for optimal ML pipeline performance (10x faster, 90% smaller than CSV). Preserves dtypes and optimizes memory.

inputs:
  - name: data_source
    type: String
    description: 'Data source - PI API URL, CDN URL, or MinIO path (bucket/path/to/file)'
  - name: source_type
    type: String
    description: 'Source type: "pi_api", "cdn", or "minio". If empty, auto-detects'
    default: ''
  - name: access_token
    type: String
    description: 'Bearer access token for PI API or CDN (file path or token string)'
    default: ''
  - name: minio_endpoint
    type: String
    description: 'MinIO endpoint URL'
    default: 'http://minio-service.kubeflow.svc.cluster.local:9000'
  - name: minio_access_key
    type: String
    description: 'MinIO access key'
    default: 'minio'
  - name: minio_secret_key
    type: String
    description: 'MinIO secret key'
    default: 'HN1YBS5WHS40M1CJ75FNKH0HEHR42I'
  - name: target_column
    type: String
    description: 'Target column for ground truth extraction (empty = no ground truth)'
    default: ''
  - name: target_mapping
    type: String
    description: 'JSON mapping for categorical targets'
    default: '{}'
  - name: page_size
    type: String
    description: 'Records per page for PI API pagination (max 2000)'
    default: '2000'
  - name: db_type
    type: String
    description: 'Database type for PI API'
    default: 'TIDB'
  - name: feature_columns
    type: String
    description: 'Comma-separated feature columns to select'
    default: ''
  - name: exclude_columns
    type: String
    description: 'Comma-separated columns to exclude'
    default: ''

outputs:
  - name: data
    type: Data
    description: 'Loaded feature data (PARQUET format - optimized for ML)'
  - name: metadata
    type: Data
    description: 'Dataset metadata and validation (JSON)'
  - name: ground_truth
    type: Data
    description: 'Ground truth labels if target_column specified (NPY)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import shutil
        import tempfile
        import subprocess
        import pandas as pd
        import numpy as np
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import urllib.parse as urlparse
        from urllib.parse import urlencode
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('data_loader_parquet')
        
        PI_API_MAX_PAGE_SIZE = 2000
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def detect_source_type(data_source):
            data_source_lower = data_source.lower()
            
            if 'pi-entity-instances-service' in data_source_lower:
                return 'pi_api'
            if '/schemas/' in data_source_lower and '/instances/' in data_source_lower:
                return 'pi_api'
            if not data_source.startswith(('http://', 'https://')) and '/' in data_source:
                return 'minio'
            
            cdn_extensions = ['.csv', '.xlsx', '.xls', '.parquet', '.pq', '.npy', '.npz', '.pt', '.pth', '.tsv']
            if any(data_source_lower.endswith(ext) for ext in cdn_extensions):
                return 'cdn'
            
            cdn_patterns = ['cdn', 's3.', 'blob.core', 'storage.googleapis', 'cloudfront']
            if any(pattern in data_source_lower for pattern in cdn_patterns):
                return 'cdn'
            
            if '/' in data_source and not data_source.startswith('http'):
                return 'minio'
            
            return 'pi_api'
        
        def install_minio_client():
            if shutil.which('mc'):
                return True
            
            logger.info("Installing MinIO client...")
            download_cmd = None
            if shutil.which('wget'):
                download_cmd = ['wget', '-q', 'https://dl.min.io/client/mc/release/linux-amd64/mc', '-O', '/tmp/mc']
            elif shutil.which('curl'):
                download_cmd = ['curl', '-sL', 'https://dl.min.io/client/mc/release/linux-amd64/mc', '-o', '/tmp/mc']
            else:
                return False
            
            try:
                subprocess.run(download_cmd, check=True, capture_output=True)
                os.chmod('/tmp/mc', 0o755)
                shutil.move('/tmp/mc', '/usr/local/bin/mc')
                return True
            except:
                return False
        
        def configure_minio_client(endpoint, access_key, secret_key, alias='myminio'):
            cmd = ['mc', 'alias', 'set', alias, endpoint, access_key, secret_key]
            subprocess.run(cmd, capture_output=True, text=True, check=True)
            return alias
        
        def load_data_multi_format(file_path):
            #Load any format and return DataFrame. Priority: Parquet > PyTorch > NumPy > CSV#
            logger.info("="*80)
            logger.info("MULTI-FORMAT DATA LOADING")
            logger.info("="*80)
            
            strategies = [
                ('Parquet', lambda: pd.read_parquet(file_path, engine="pyarrow")),
                ('PyTorch', lambda: pd.DataFrame(load_pytorch(file_path))),
                ('NumPy .npy', lambda: pd.DataFrame(np.load(file_path, allow_pickle=True))),
                ('NumPy .npz', lambda: pd.DataFrame(load_npz(file_path))),
                ('CSV', lambda: pd.read_csv(file_path)),
                ('Excel', lambda: pd.read_excel(file_path)),
                ('TSV', lambda: pd.read_csv(file_path, sep='\t')),
            ]
            
            for name, func in strategies:
                try:
                    logger.info(f"Attempting: {name}")
                    df = func()
                    if not isinstance(df, pd.DataFrame):
                        df = pd.DataFrame(df)
                    logger.info(f"✓ Loaded as {name}: {df.shape}")
                    return df
                except Exception as e:
                    logger.debug(f"  Failed: {e}")
                    continue
            
            raise RuntimeError("Could not load file with any supported format")
        
        def load_pytorch(file_path):
            import torch
            data = torch.load(file_path, map_location='cpu')
            if isinstance(data, torch.Tensor):
                return data.numpy()
            elif isinstance(data, dict):
                for key in ['data', 'X', 'tensor', 'values']:
                    if key in data and isinstance(data[key], torch.Tensor):
                        return data[key].numpy()
            raise ValueError("No tensor found in PyTorch file")
        
        def load_npz(file_path):
            npz_data = np.load(file_path, allow_pickle=True)
            for key in ['data', 'arr_0', 'X', 'values']:
                if key in npz_data:
                    return npz_data[key]
            return npz_data[list(npz_data.keys())[0]]
        
        def fetch_from_minio(minio_path, alias, endpoint, access_key, secret_key):
            logger.info("FETCHING FROM MINIO")
            
            if not install_minio_client():
                raise RuntimeError("MinIO client unavailable")
            
            alias = configure_minio_client(endpoint, access_key, secret_key, alias)
            
            parts = minio_path.split('/', 1)
            if len(parts) < 2:
                raise ValueError(f"Invalid MinIO path: {minio_path}")
            
            bucket, file_path = parts[0], parts[1]
            source_path = f"{alias}/{bucket}/{file_path}"
            
            # Check exists
            subprocess.run(['mc', 'stat', source_path], capture_output=True, check=True)
            
            fd, temp_file = tempfile.mkstemp()
            os.close(fd)
            
            try:
                subprocess.run(['mc', 'cp', source_path, temp_file], capture_output=True, check=True)
                logger.info(f"Downloaded {os.path.getsize(temp_file)/1024**2:.2f} MB")
                return load_data_multi_format(temp_file)
            finally:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
        
        def fetch_from_cdn(data_source, access_token):
            logger.info("FETCHING FROM CDN")
            
            session = requests.Session()
            retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
            session.mount("https://", HTTPAdapter(max_retries=retries))
            session.mount("http://", HTTPAdapter(max_retries=retries))
            
            headers = {}
            if access_token:
                if os.path.exists(access_token):
                    with open(access_token, 'r') as f:
                        token = f.read().strip()
                        headers["Authorization"] = f"Bearer {token}"
                else:
                    headers["Authorization"] = f"Bearer {access_token}"
            
            resp = session.get(data_source, headers=headers, timeout=300)
            resp.raise_for_status()
            
            logger.info(f"Downloaded {len(resp.content)/1024**2:.2f} MB")
            
            fd, temp_file = tempfile.mkstemp()
            os.close(fd)
            
            try:
                with open(temp_file, 'wb') as f:
                    f.write(resp.content)
                return load_data_multi_format(temp_file)
            finally:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
        
        def fetch_from_pi_api(api_url, access_token, page_size, db_type):
            logger.info("FETCHING FROM PI API")
            
            actual_page_size = min(int(page_size), PI_API_MAX_PAGE_SIZE)
            
            if access_token and os.path.exists(access_token):
                with open(access_token, 'r') as f:
                    access_token = f.read().strip()
            
            session = requests.Session()
            retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
            session.mount("https://", HTTPAdapter(max_retries=retries))
            session.mount("http://", HTTPAdapter(max_retries=retries))
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {access_token}"
            }
            
            parsed_url = urlparse.urlparse(api_url)
            url_params = urlparse.parse_qs(parsed_url.query)
            url_params.pop('page', None)
            url_params.pop('size', None)
            
            all_records = []
            current_page = 0
            
            while True:
                url_params['page'] = [str(current_page)]
                url_params['size'] = [str(actual_page_size)]
                new_query = urlencode(url_params, doseq=True)
                request_url = parsed_url._replace(query=new_query).geturl()
                
                payload = {"dbType": db_type, "limit": actual_page_size, "offset": current_page * actual_page_size}
                
                resp = session.post(request_url, headers=headers, json=payload, timeout=60)
                resp.raise_for_status()
                
                json_response = resp.json()
                
                if isinstance(json_response, dict):
                    batch = json_response.get('content', json_response.get('data', json_response.get('instances', [])))
                else:
                    batch = json_response
                
                batch_len = len(batch) if isinstance(batch, list) else 0
                
                if batch_len == 0:
                    break
                
                all_records.extend(batch)
                logger.info(f"[Page {current_page}] Fetched {batch_len} records. Total: {len(all_records)}")
                
                if batch_len < actual_page_size:
                    break
                
                current_page += 1
            
            if not all_records:
                raise ValueError("No records fetched from PI API")
            
            return pd.DataFrame(all_records)
        
        def optimize_dtypes(df):
            #Optimize DataFrame for Parquet: category conversion + downcast#
            logger.info("OPTIMIZING DTYPES FOR PARQUET")
            
            initial_mem = df.memory_usage(deep=True).sum() / 1024**2
            
            # Convert high-cardinality objects to category
            for col in df.select_dtypes(include=['object']).columns:
                nunique = df[col].nunique()
                if nunique < 0.5 * len(df):
                    df[col] = df[col].astype('category')
                    logger.info(f"  {col}: object → category")
            
            # Downcast numerics
            for col in df.select_dtypes(include=['int64']).columns:
                df[col] = pd.to_numeric(df[col], downcast='integer')
            
            for col in df.select_dtypes(include=['float64']).columns:
                df[col] = pd.to_numeric(df[col], downcast='float')
            
            final_mem = df.memory_usage(deep=True).sum() / 1024**2
            logger.info(f"Memory: {initial_mem:.2f} MB → {final_mem:.2f} MB ({(1-final_mem/initial_mem)*100:.1f}% saved)")
            
            return df
        
        def select_features(df, feature_columns, exclude_columns):
            logger.info("FEATURE SELECTION")
            
            original_columns = df.columns.tolist()
            selection_metadata = {'original_columns': original_columns, 'original_count': len(original_columns)}
            
            if feature_columns and feature_columns.strip():
                selected_features = [col.strip() for col in feature_columns.split(',') if col.strip()]
                missing = [col for col in selected_features if col not in df.columns]
                if missing:
                    raise ValueError(f"Features not found: {missing}")
                df = df[selected_features]
                selection_metadata['selection_type'] = 'whitelist'
                selection_metadata['selected_features'] = selected_features
            
            if exclude_columns and exclude_columns.strip():
                excluded = [col.strip() for col in exclude_columns.split(',') if col.strip()]
                existing_excluded = [col for col in excluded if col in df.columns]
                if existing_excluded:
                    df = df.drop(columns=existing_excluded)
                    selection_metadata['excluded_features'] = existing_excluded
                if not selection_metadata.get('selection_type'):
                    selection_metadata['selection_type'] = 'blacklist'
            
            if not feature_columns and not exclude_columns:
                selection_metadata['selection_type'] = 'all'
            
            selection_metadata['final_columns'] = df.columns.tolist()
            selection_metadata['final_count'] = len(df.columns)
            
            logger.info(f"Selected {len(df.columns)} columns")
            return df, selection_metadata
        
        def extract_ground_truth(df, target_column, target_mapping_json):
            if not target_column:
                return df, None, None
            
            logger.info("EXTRACTING GROUND TRUTH")
            
            if target_column not in df.columns:
                raise ValueError(f"Target '{target_column}' not found")
            
            target_series = df[target_column].copy()
            
            target_mapping = {}
            if target_mapping_json and target_mapping_json != '{}':
                try:
                    target_mapping = json.loads(target_mapping_json)
                except:
                    pass
            
            ground_truth_info = {
                'column_name': target_column,
                'n_samples': int(len(target_series)),
                'n_unique': int(target_series.nunique()),
                'dtype': str(target_series.dtype)
            }
            
            if pd.api.types.is_numeric_dtype(target_series):
                ground_truth_encoded = target_series.values
                ground_truth_info['encoding'] = 'none (numeric)'
                ground_truth_info['mapping'] = {}
            elif target_mapping:
                ground_truth_encoded = target_series.map(target_mapping).values
                if pd.isna(ground_truth_encoded).any():
                    raise ValueError("Custom mapping incomplete")
                ground_truth_info['encoding'] = 'custom'
                ground_truth_info['mapping'] = target_mapping
            else:
                unique_vals = sorted(target_series.unique())
                auto_mapping = {val: idx for idx, val in enumerate(unique_vals)}
                ground_truth_encoded = target_series.map(auto_mapping).values
                ground_truth_info['encoding'] = 'auto'
                ground_truth_info['mapping'] = auto_mapping
            
            ground_truth_encoded = ground_truth_encoded.astype(int)
            ground_truth_info['encoded_range'] = [int(ground_truth_encoded.min()), int(ground_truth_encoded.max())]
            
            df_features = df.drop(columns=[target_column])
            return df_features, ground_truth_encoded, ground_truth_info
        
        def validate_dataset(df):
            logger.info("VALIDATING DATASET")
            
            validation = {'is_valid': True, 'errors': [], 'warnings': []}
            
            if df.empty:
                validation['is_valid'] = False
                validation['errors'].append("Dataset is empty")
                return validation, {}
            
            if len(df) < 2:
                validation['is_valid'] = False
                validation['errors'].append("Need at least 2 samples")
                return validation, {}
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                validation['warnings'].append("No numeric columns")
            
            missing_total = df.isnull().sum().sum()
            missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100 if df.shape[0] * df.shape[1] > 0 else 0
            
            if missing_total > 0:
                validation['warnings'].append(f"Missing: {missing_total} ({missing_pct:.2f}%)")
            
            metadata = {
                'n_samples': int(df.shape[0]),
                'n_features': int(df.shape[1]),
                'columns': list(df.columns),
                'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
                'numeric_columns': numeric_cols,
                'categorical_columns': categorical_cols,
                'missing_values': {'total': int(missing_total), 'percentage': float(missing_pct)},
                'memory_mb': float(df.memory_usage(deep=True).sum() / 1024**2),
                'validation': validation
            }
            
            return validation, metadata
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--data_source", required=True)
            parser.add_argument("--source_type", default='')
            parser.add_argument("--access_token", default='')
            parser.add_argument("--minio_endpoint", default='http://minio-service.kubeflow.svc.cluster.local:9000')
            parser.add_argument("--minio_access_key", default='minio')
            parser.add_argument("--minio_secret_key", default='HN1YBS5WHS40M1CJ75FNKH0HEHR42I')
            parser.add_argument("--target_column", default='')
            parser.add_argument("--target_mapping", default='{}')
            parser.add_argument("--page_size", default='2000')
            parser.add_argument("--db_type", default='TIDB')
            parser.add_argument("--feature_columns", default='')
            parser.add_argument("--exclude_columns", default='')
            parser.add_argument("--output_data", required=True)
            parser.add_argument("--output_metadata", required=True)
            parser.add_argument("--output_ground_truth", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("DATA LOADING COMPONENT (PARQUET OPTIMIZED)")
            logger.info("="*80)
            
            try:
                ensure_directory_exists(args.output_data)
                ensure_directory_exists(args.output_metadata)
                ensure_directory_exists(args.output_ground_truth)
                
                # Detect source type
                source_type = args.source_type.strip().lower() if args.source_type else detect_source_type(args.data_source)
                logger.info(f"Source type: {source_type}")
                
                # Fetch data
                if source_type == 'minio':
                    df = fetch_from_minio(args.data_source, 'myminio', args.minio_endpoint, args.minio_access_key, args.minio_secret_key)
                    source_meta = {'type': 'MinIO', 'path': args.data_source}
                elif source_type == 'cdn':
                    df = fetch_from_cdn(args.data_source, args.access_token)
                    source_meta = {'type': 'CDN', 'url': args.data_source}
                elif source_type == 'pi_api':
                    df = fetch_from_pi_api(args.data_source, args.access_token, args.page_size, args.db_type)
                    source_meta = {'type': 'PI_API', 'api_url': args.data_source}
                else:
                    raise ValueError(f"Unknown source type: {source_type}")
                
                logger.info(f"Loaded: {df.shape}")
                
                # Select features
                df, selection_meta = select_features(df, args.feature_columns, args.exclude_columns)
                
                # Extract ground truth
                df_features, ground_truth, gt_info = extract_ground_truth(df, args.target_column, args.target_mapping)
                
                # Optimize dtypes
                df_features = optimize_dtypes(df_features)
                
                # Validate
                validation, metadata = validate_dataset(df_features)
                
                if not validation['is_valid']:
                    logger.error("VALIDATION FAILED")
                    for error in validation['errors']:
                        logger.error(f"  {error}")
                    sys.exit(1)
                
                # Add metadata
                if gt_info:
                    metadata['ground_truth'] = gt_info
                metadata['feature_selection'] = selection_meta
                metadata['data_source'] = source_meta
                
                # SAVE AS PARQUET (NOT CSV!)
                output_parquet = args.output_data
                if not output_parquet.endswith('.parquet'):
                    output_parquet = output_parquet.replace('.csv', '.parquet')
                
                df_features.to_parquet(
                    output_parquet,
                    index=False,
                    engine='pyarrow',
                    compression='snappy'
                )
                
                parquet_size = os.path.getsize(output_parquet) / 1024**2
                logger.info(f"✓ Saved as PARQUET: {parquet_size:.2f} MB")
                
                # Save metadata
                with open(args.output_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                
                # Save ground truth
                if ground_truth is not None:
                    np.save(args.output_ground_truth, ground_truth, allow_pickle=False)
                    npy_path = args.output_ground_truth + '.npy'
                    if os.path.exists(npy_path):
                        shutil.move(npy_path, args.output_ground_truth)
                else:
                    with open(args.output_ground_truth, 'w') as f:
                        f.write("")
                
                logger.info("="*80)
                logger.info("DATA LOADING COMPLETED")
                logger.info("="*80)
                logger.info(f"Format: PARQUET (optimized for ML pipelines)")
                logger.info(f"Samples: {metadata['n_samples']}")
                logger.info(f"Features: {metadata['n_features']}")
                logger.info(f"File size: {parquet_size:.2f} MB")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {e}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --data_source
      - {inputValue: data_source}
      - --source_type
      - {inputValue: source_type}
      - --access_token
      - {inputValue: access_token}
      - --minio_endpoint
      - {inputValue: minio_endpoint}
      - --minio_access_key
      - {inputValue: minio_access_key}
      - --minio_secret_key
      - {inputValue: minio_secret_key}
      - --target_column
      - {inputValue: target_column}
      - --target_mapping
      - {inputValue: target_mapping}
      - --page_size
      - {inputValue: page_size}
      - --db_type
      - {inputValue: db_type}
      - --feature_columns
      - {inputValue: feature_columns}
      - --exclude_columns
      - {inputValue: exclude_columns}
      - --output_data
      - {outputPath: data}
      - --output_metadata
      - {outputPath: metadata}
      - --output_ground_truth
      - {outputPath: ground_truth}
