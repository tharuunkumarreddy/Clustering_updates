name: Enhanced Data Splitting Component - PARQUET OPTIMIZED (Unified Preprocessing)
description: Splits dataset into train/test and outputs indices for train-aware preprocessing. Maintains combined dataset for unified preprocessing while tracking which rows belong to train vs test. Supports stratified splitting. Loads Parquet/CSV, SAVES AS PARQUET.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset to split (Parquet or CSV)'
  - name: ground_truth
    type: Data
    description: 'Ground truth labels for stratified splitting (NPY or CSV, optional)'
    optional: true
  - name: test_size
    type: String
    description: 'Proportion of test data (0-1). Use 0 for no test split.'
    default: '0.2'
  - name: random_state
    type: String
    description: 'Random seed for reproducibility'
    default: '42'
  - name: stratify
    type: String
    description: 'Use stratified splitting if ground truth available (true/false)'
    default: 'true'

outputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (all rows together) for unified preprocessing (PARQUET format - 10x faster than CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: test_indices
    type: Data
    description: 'Test row indices (JSON array)'
  - name: split_info
    type: Data
    description: 'Split statistics and class distributions (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from sklearn.model_selection import train_test_split
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('data_splitter_enhanced')
        
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        
        def load_data(input_path):
            #Load data from Parquet or CSV#
            logger.info(f"Loading dataset from: {input_path}")
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path, engine='pyarrow')
                    logger.info("✓ Loaded Parquet file")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("✓ Loaded CSV file")
                    
                logger.info(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
                return df
            except Exception as e:
                logger.error(f"Error loading data: {str(e)}")
                raise
        
        
        def load_ground_truth(ground_truth_path):
            #Load ground truth labels from NPY or CSV#
            if not ground_truth_path or ground_truth_path.lower() == 'none':
                return None
            
            try:
                if not os.path.exists(ground_truth_path) or os.path.getsize(ground_truth_path) == 0:
                    return None
                
                ext = Path(ground_truth_path).suffix.lower()
                
                # Try loading as NPY
                if ext == '.npy' or ext == '':
                    try:
                        ground_truth = np.load(ground_truth_path, allow_pickle=False)
                        logger.info(f"Loaded ground truth: {len(ground_truth)} labels")
                        return ground_truth
                    except:
                        pass
                
                # Try loading as CSV
                if ext == '.csv' or ext == '':
                    try:
                        df_gt = pd.read_csv(ground_truth_path)
                        ground_truth = df_gt.iloc[:, 0 if len(df_gt.columns) == 1 else -1].values
                        logger.info(f"Loaded ground truth: {len(ground_truth)} labels")
                        return ground_truth
                    except:
                        pass
                
                return None
            except:
                return None
        
        
        def split_indices(df, ground_truth, test_size, random_state, stratify):
            #Split data and return indices instead of actual data splits#
            logger.info("="*80)
            logger.info("SPLITTING DATA (UNIFIED PREPROCESSING)")
            logger.info("="*80)
            logger.info(f"Test size: {test_size}")
            logger.info(f"Random state: {random_state}")
            logger.info(f"Stratify: {stratify}")
            logger.info("")
            
            # Special case: No test split
            if test_size == 0:
                logger.info("No test split (test_size=0)")
                train_indices = list(range(len(df)))
                test_indices = []
                
                split_info = {
                    'test_size': 0,
                    'train_samples': len(df),
                    'test_samples': 0,
                    'stratified': False,
                    'train_percentage': 100.0,
                    'test_percentage': 0.0
                }
                
                return train_indices, test_indices, split_info
            
            if not (0 < test_size < 1):
                raise ValueError(f"test_size must be between 0 and 1, got {test_size}")
            
            # Create index array
            all_indices = np.arange(len(df))
            
            split_info = {
                'test_size': float(test_size),
                'random_state': int(random_state),
                'stratified': False
            }
            
            # CASE 1: Stratified split with ground truth
            if ground_truth is not None and stratify:
                logger.info("Performing stratified split using ground truth")
                
                if len(ground_truth) != len(df):
                    raise ValueError(
                        f"Ground truth length ({len(ground_truth)}) != dataset length ({len(df)})"
                    )
                
                try:
                    train_idx, test_idx = train_test_split(
                        all_indices,
                        test_size=test_size,
                        random_state=random_state,
                        stratify=ground_truth
                    )
                    
                    split_info['stratified'] = True
                    split_info['n_classes'] = int(len(np.unique(ground_truth)))
                    
                    # Calculate class distributions
                    train_gt = ground_truth[train_idx]
                    test_gt = ground_truth[test_idx]
                    
                    unique_train, counts_train = np.unique(train_gt, return_counts=True)
                    unique_test, counts_test = np.unique(test_gt, return_counts=True)
                    
                    split_info['train_class_distribution'] = {int(k): int(v) for k, v in zip(unique_train, counts_train)}
                    split_info['test_class_distribution'] = {int(k): int(v) for k, v in zip(unique_test, counts_test)}
                    
                    logger.info(f"Stratified split completed")
                    logger.info(f"Number of classes: {split_info['n_classes']}")
                    
                    # Log class distributions
                    logger.info("Train class distribution:")
                    for cls, count in split_info['train_class_distribution'].items():
                        pct = count / len(train_idx) * 100
                        logger.info(f"  Class {cls}: {count} ({pct:.1f}%)")
                    
                    logger.info("Test class distribution:")
                    for cls, count in split_info['test_class_distribution'].items():
                        pct = count / len(test_idx) * 100
                        logger.info(f"  Class {cls}: {count} ({pct:.1f}%)")
                    
                except ValueError as e:
                    logger.warning(f"Stratified split failed: {e}")
                    logger.warning("Falling back to random split")
                    
                    train_idx, test_idx = train_test_split(
                        all_indices,
                        test_size=test_size,
                        random_state=random_state
                    )
                    split_info['stratified'] = False
                    split_info['stratify_fallback'] = True
            
            # CASE 2 & 3: Random split (with or without ground truth)
            else:
                logger.info("Performing random split")
                
                train_idx, test_idx = train_test_split(
                    all_indices,
                    test_size=test_size,
                    random_state=random_state
                )
            
            train_indices = train_idx.tolist()
            test_indices = test_idx.tolist()
            
            split_info['train_samples'] = len(train_indices)
            split_info['test_samples'] = len(test_indices)
            
            total = len(train_indices) + len(test_indices)
            split_info['train_percentage'] = float(len(train_indices) / total * 100)
            split_info['test_percentage'] = float(len(test_indices) / total * 100)
            
            logger.info(f"Train samples: {len(train_indices)} ({split_info['train_percentage']:.1f}%)")
            logger.info(f"Test samples: {len(test_indices)} ({split_info['test_percentage']:.1f}%)")
            logger.info("")
            
            return train_indices, test_indices, split_info
        
        
        def main():
            parser = argparse.ArgumentParser(
                description="Enhanced Data Splitting for Unified Preprocessing (PARQUET OPTIMIZED)"
            )
            parser.add_argument("--input_data", required=True)
            parser.add_argument("--ground_truth", default='')
            parser.add_argument("--test_size", type=str, default='0.2')
            parser.add_argument("--random_state", type=str, default='42')
            parser.add_argument("--stratify", default='true')
            parser.add_argument("--output_combined_data", required=True)
            parser.add_argument("--output_train_indices", required=True)
            parser.add_argument("--output_test_indices", required=True)
            parser.add_argument("--output_split_info", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("ENHANCED DATA SPLITTING (PARQUET OPTIMIZED - UNIFIED)")
            logger.info("="*80)
            logger.info(f"Mode: Unified Preprocessing")
            logger.info(f"Test size: {args.test_size}")
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_combined_data)
                ensure_directory_exists(args.output_train_indices)
                ensure_directory_exists(args.output_test_indices)
                ensure_directory_exists(args.output_split_info)
                
                # Load data
                df = load_data(args.input_data)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Load ground truth
                ground_truth = load_ground_truth(args.ground_truth)
                
                # Parse parameters
                test_size = float(args.test_size)
                random_state = int(args.random_state)
                stratify = args.stratify.lower() == 'true'
                
                # Split indices (not data)
                train_indices, test_indices, split_info = split_indices(
                    df=df,
                    ground_truth=ground_truth,
                    test_size=test_size,
                    random_state=random_state,
                    stratify=stratify
                )
                
                # ============================================================
                # CRITICAL: SAVE COMBINED DATA AS PARQUET (NOT CSV!)
                # ============================================================
                output_parquet = args.output_combined_data
                if not output_parquet.endswith('.parquet'):
                    output_parquet = output_parquet.replace('.csv', '.parquet')
                
                logger.info("="*80)
                logger.info("SAVING COMBINED DATA AS PARQUET")
                logger.info("="*80)
                
                df.to_parquet(
                    output_parquet,
                    index=False,
                    engine='pyarrow',
                    compression='snappy'
                )
                
                parquet_size = os.path.getsize(output_parquet) / 1024**2
                logger.info(f"✓ Combined data saved as PARQUET: {parquet_size:.2f} MB")
                logger.info(f"  Path: {output_parquet}")
                logger.info(f"  Contains ALL {len(df)} rows for unified preprocessing")
                
                # Add Parquet metadata to split info
                split_info['output_format'] = {
                    'format': 'parquet',
                    'engine': 'pyarrow',
                    'compression': 'snappy',
                    'file_size_mb': float(parquet_size)
                }
                
                # Save train indices
                with open(args.output_train_indices, 'w') as f:
                    json.dump(train_indices, f)
                logger.info(f"✓ Train indices saved: {len(train_indices)} indices")
                
                # Save test indices
                with open(args.output_test_indices, 'w') as f:
                    json.dump(test_indices, f)
                logger.info(f"✓ Test indices saved: {len(test_indices)} indices")
                
                # Save split info
                with open(args.output_split_info, 'w') as f:
                    json.dump(split_info, f, indent=2)
                logger.info(f"✓ Split info saved")
                
                logger.info("")
                logger.info("="*80)
                logger.info("ENHANCED SPLITTING COMPLETED")
                logger.info("="*80)
                logger.info("UNIFIED PREPROCESSING MODE:")
                logger.info(f"  - Combined data: {len(df)} rows (train + test together)")
                logger.info(f"  - Train indices: {len(train_indices)} rows")
                logger.info(f"  - Test indices: {len(test_indices)} rows")
                logger.info(f"  - Output format: PARQUET (10x faster than CSV)")
                logger.info(f"  - File size: {parquet_size:.2f} MB")
                logger.info("")
                logger.info("NEXT STEPS:")
                logger.info("  1. Preprocessing components will use train_indices")
                logger.info("  2. They will FIT on train rows only")
                logger.info("  3. They will TRANSFORM all combined_data rows")
                logger.info("  4. Final split component will separate at the end")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --ground_truth
      - {inputPath: ground_truth}
      - --test_size
      - {inputValue: test_size}
      - --random_state
      - {inputValue: random_state}
      - --stratify
      - {inputValue: stratify}
      - --output_combined_data
      - {outputPath: combined_data}
      - --output_train_indices
      - {outputPath: train_indices}
      - --output_test_indices
      - {outputPath: test_indices}
      - --output_split_info
      - {outputPath: split_info}
