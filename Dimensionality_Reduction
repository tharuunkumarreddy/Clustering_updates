name: Unified Dimensionality Reduction Component
description: Train-aware dimensionality reduction for unified preprocessing. Fits reduction on train indices only, transforms all combined data. Supports 7 methods PCA, Kernel PCA, ICA, Factor Analysis, Sparse PCA, Isomap, LLE. Auto-determines optimal components.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: method
    type: String
    description: 'Reduction method: pca, kernel_pca, ica, factor_analysis, sparse_pca, isomap, lle, none'
    default: 'pca'
  - name: method_params
    type: String
    description: 'Method parameters as JSON. Examples: {"n_components":"auto"}, {"kernel":"rbf"}'
    default: '{}'

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Reduced dataset with component features (CSV)'
  - name: metadata
    type: Data
    description: 'Reduction metadata including variance explained (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from sklearn.decomposition import PCA, KernelPCA, FastICA, FactorAnalysis, SparsePCA
        from sklearn.manifold import Isomap, LocallyLinearEmbedding
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('dimreduction_unified')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            ext = Path(input_path).suffix.lower()
            if ext in ['.parquet', '.pq']:
                return pd.read_parquet(input_path)
            return pd.read_csv(input_path)
        
        def load_indices(indices_path):
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def determine_n_components(X_train, method='variance', variance_threshold=0.95):
            n_features = X_train.shape[1]
            n_samples = X_train.shape[0]
            max_components = min(n_samples, n_features)
            
            if method == 'variance':
                pca_temp = PCA(n_components=max_components)
                pca_temp.fit(X_train)
                
                cumsum = np.cumsum(pca_temp.explained_variance_ratio_)
                n_components = int(np.searchsorted(cumsum, variance_threshold) + 1)
                
                logger.info(
                    "Auto: " + str(n_components) + " components for " + str(variance_threshold*100) + "% variance"
                )
            elif method == 'kaiser':
                pca_temp = PCA(n_components=max_components)
                pca_temp.fit(X_train)
                
                n_components = int(np.sum(pca_temp.explained_variance_ > 1))
                n_components = max(1, n_components)
                
                logger.info("Kaiser: " + str(n_components) + " components (eigenvalues > 1)")
            else:
                n_components = max(1, n_features // 2)
                logger.info("Default: " + str(n_components) + " components")
            
            return min(n_components, max_components)
        
        def reduce_dimensions_unified(df, train_indices, method, params):
            logger.info("="*80)
            logger.info("UNIFIED DIMENSIONALITY REDUCTION")
            logger.info("="*80)
            logger.info("Method: " + str(method))
            logger.info("Train indices: " + str(len(train_indices)) + " rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping")
                return df, {
                    'method': 'none',
                    'n_components': 0
                }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns")
                return df, {
                    'method': method,
                    'n_components': 0,
                    'warning': 'no_numeric_columns'
                }
            
            logger.info("Original features: " + str(len(numeric_cols)) + " numeric columns")
            
            train_df = df.iloc[train_indices]
            X_train = train_df[numeric_cols].values
            X_all = df[numeric_cols].values
            
            metadata = {
                'method': method,
                'params': params,
                'input_features': len(numeric_cols),
                'original_columns': numeric_cols[:10]
            }
            
            n_components = params.get('n_components', 'auto')
            random_state = params.get('random_state', 42)
            
            if method == 'pca':
                variance_threshold = params.get('variance_threshold', 0.95)
                
                if n_components == 'auto':
                    n_components = determine_n_components(
                        X_train,
                        method='variance',
                        variance_threshold=variance_threshold
                    )
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                reducer = PCA(n_components=n_components, random_state=random_state)
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['explained_variance_ratio'] = reducer.explained_variance_ratio_.tolist()
                metadata['cumulative_variance'] = np.cumsum(
                    reducer.explained_variance_ratio_
                ).tolist()
                
                logger.info("Components: " + str(n_components))
                logger.info("Variance: " + str(round(metadata['cumulative_variance'][-1]*100, 2)) + "%")
            
            elif method == 'kernel_pca':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1] // 2, 50)
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                kernel = params.get('kernel', 'rbf')
                gamma = params.get('gamma', None)
                
                reducer = KernelPCA(
                    n_components=n_components,
                    kernel=kernel,
                    gamma=gamma,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['kernel'] = kernel
                logger.info("Components: " + str(n_components) + ", Kernel: " + kernel)
            
            elif method == 'ica':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1], X_train.shape[0])
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                reducer = FastICA(
                    n_components=n_components,
                    random_state=random_state,
                    max_iter=1000
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                logger.info("Components: " + str(n_components))
            
            elif method == 'factor_analysis':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1] // 2, X_train.shape[0])
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                reducer = FactorAnalysis(
                    n_components=n_components,
                    random_state=random_state
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                logger.info("Components: " + str(n_components))
            
            elif method == 'sparse_pca':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1] // 2, X_train.shape[0])
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                alpha = params.get('alpha', 1.0)
                
                reducer = SparsePCA(
                    n_components=n_components,
                    alpha=alpha,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['alpha'] = float(alpha)
                logger.info("Components: " + str(n_components) + ", Alpha: " + str(alpha))
            
            elif method == 'isomap':
                n_components = int(params.get('n_components', 2))
                n_neighbors = params.get('n_neighbors', 5)
                
                if n_neighbors >= X_train.shape[0]:
                    n_neighbors = max(2, X_train.shape[0] - 1)
                    logger.warning("Reduced n_neighbors to " + str(n_neighbors))
                
                reducer = Isomap(
                    n_components=n_components,
                    n_neighbors=n_neighbors,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['n_neighbors'] = int(n_neighbors)
                logger.info("Components: " + str(n_components) + ", Neighbors: " + str(n_neighbors))
            
            elif method == 'lle':
                n_components = int(params.get('n_components', 2))
                n_neighbors = params.get('n_neighbors', 5)
                
                if n_neighbors >= X_train.shape[0]:
                    n_neighbors = max(2, X_train.shape[0] - 1)
                    logger.warning("Reduced n_neighbors to " + str(n_neighbors))
                
                reducer = LocallyLinearEmbedding(
                    n_components=n_components,
                    n_neighbors=n_neighbors,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['n_neighbors'] = int(n_neighbors)
                logger.info("Components: " + str(n_components) + ", Neighbors: " + str(n_neighbors))
            
            else:
                raise ValueError(
                    "Unknown method: " + method + ". "
                    "Available: pca, kernel_pca, ica, factor_analysis, sparse_pca, "
                    "isomap, lle, none"
                )
            
            component_names = [("Component_" + str(i+1)) for i in range(X_reduced.shape[1])]
            df_reduced = pd.DataFrame(X_reduced, columns=component_names, index=df.index)
            
            if non_numeric_cols:
                for col in non_numeric_cols:
                    df_reduced[col] = df[col]
            
            metadata['output_features'] = len(component_names)
            metadata['component_names'] = component_names
            metadata['reduction_ratio'] = float(len(component_names) / len(numeric_cols))
            metadata['features_removed'] = len(numeric_cols) - len(component_names)
            
            logger.info("")
            logger.info("Reduction: " + str(len(numeric_cols)) + " to " + str(len(component_names)) + " features")
            logger.info("Features removed: " + str(metadata['features_removed']))
            logger.info("")
            
            return df_reduced, metadata
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--method", default='pca')
            parser.add_argument("--method_params", default='{}')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_metadata", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED DIMENSIONALITY REDUCTION")
            logger.info("="*80)
            logger.info("Method: " + args.method)
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_metadata)
                
                params = json.loads(args.method_params)
                
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                df_reduced, metadata = reduce_dimensions_unified(
                    df=df,
                    train_indices=train_indices,
                    method=args.method,
                    params=params
                )
                
                df_reduced.to_csv(args.output_preprocessed_data, index=False)
                logger.info("Reduced data saved")
                
                with open(args.output_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                logger.info("Metadata saved")
                
                logger.info("")
                logger.info("="*80)
                logger.info("DIMENSIONALITY REDUCTION COMPLETED")
                logger.info("="*80)
                logger.info("Method: " + args.method)
                logger.info("Components: " + str(metadata['output_features']))
                logger.info("Reduction: " + str(round(metadata['reduction_ratio']*100, 1)) + "%")
                logger.info("Final shape: " + str(df_reduced.shape))
                logger.info("="*80)
                
            except Exception as e:
                logger.error("ERROR: " + str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_metadata
      - {outputPath: metadata}
