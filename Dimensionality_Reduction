name: Unified Dimensionality Reduction Component
description: Train-aware dimensionality reduction for unified preprocessing. Fits reduction on train indices only, transforms all combined data. Supports 9 methods: PCA, Kernel PCA, ICA, t-SNE, Isomap, LLE, MDS, Factor Analysis, Sparse PCA. Auto-determines optimal components.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: method
    type: String
    description: 'Reduction method: pca, kernel_pca, ica, factor_analysis, sparse_pca, tsne, isomap, lle, mds, none'
    default: 'pca'
  - name: method_params
    type: String
    description: 'Method parameters as JSON. Examples: {"n_components":"auto"}, {"kernel":"rbf"}, {"perplexity":30}'
    default: '{}'

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Reduced dataset with component features (CSV)'
  - name: metadata
    type: Data
    description: 'Reduction metadata including variance explained (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from sklearn.decomposition import PCA, KernelPCA, FastICA, FactorAnalysis, SparsePCA
        from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, MDS
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('dimreduction_unified')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            ext = Path(input_path).suffix.lower()
            if ext in ['.parquet', '.pq']:
                return pd.read_parquet(input_path)
            return pd.read_csv(input_path)
        
        def load_indices(indices_path):
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def determine_n_components(X_train, method='variance', variance_threshold=0.95):
            #Auto-determine optimal number of components from TRAIN data#
            n_features = X_train.shape[1]
            n_samples = X_train.shape[0]
            max_components = min(n_samples, n_features)
            
            if method == 'variance':
                pca_temp = PCA(n_components=max_components)
                pca_temp.fit(X_train)
                
                cumsum = np.cumsum(pca_temp.explained_variance_ratio_)
                n_components = int(np.searchsorted(cumsum, variance_threshold) + 1)
                
                logger.info(
                    f"Auto: {n_components} components for {variance_threshold*100}% variance"
                )
            elif method == 'kaiser':
                pca_temp = PCA(n_components=max_components)
                pca_temp.fit(X_train)
                
                n_components = int(np.sum(pca_temp.explained_variance_ > 1))
                n_components = max(1, n_components)
                
                logger.info(f"Kaiser: {n_components} components (eigenvalues > 1)")
            else:
                n_components = max(1, n_features // 2)
                logger.info(f"Default: {n_components} components")
            
            return min(n_components, max_components)
        
        def reduce_dimensions_unified(df, train_indices, method, params):
            #Reduce dimensions using train-aware unified approach#
            logger.info("="*80)
            logger.info("UNIFIED DIMENSIONALITY REDUCTION")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            logger.info(f"Train indices: {len(train_indices)} rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping")
                return df, {
                    'method': 'none',
                    'n_components': 0
                }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns")
                return df, {
                    'method': method,
                    'n_components': 0,
                    'warning': 'no_numeric_columns'
                }
            
            logger.info(f"Original features: {len(numeric_cols)} numeric columns")
            
            # Extract TRAIN subset for fitting
            train_df = df.iloc[train_indices]
            X_train = train_df[numeric_cols].values
            X_all = df[numeric_cols].values
            
            metadata = {
                'method': method,
                'params': params,
                'input_features': len(numeric_cols),
                'original_columns': numeric_cols[:10]  # Sample
            }
            
            n_components = params.get('n_components', 'auto')
            random_state = params.get('random_state', 42)
            
            # METHOD 1: PCA
            if method == 'pca':
                variance_threshold = params.get('variance_threshold', 0.95)
                
                if n_components == 'auto':
                    n_components = determine_n_components(
                        X_train,
                        method='variance',
                        variance_threshold=variance_threshold
                    )
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                # Fit on TRAIN only
                reducer = PCA(n_components=n_components, random_state=random_state)
                reducer.fit(X_train)
                
                # Transform ALL data
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['explained_variance_ratio'] = reducer.explained_variance_ratio_.tolist()
                metadata['cumulative_variance'] = np.cumsum(
                    reducer.explained_variance_ratio_
                ).tolist()
                
                logger.info(f"Components: {n_components}")
                logger.info(f"Variance: {metadata['cumulative_variance'][-1]*100:.2f}%")
            
            # METHOD 2: Kernel PCA
            elif method == 'kernel_pca':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1] // 2, 50)
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                kernel = params.get('kernel', 'rbf')
                gamma = params.get('gamma', None)
                
                # Fit on TRAIN only
                reducer = KernelPCA(
                    n_components=n_components,
                    kernel=kernel,
                    gamma=gamma,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                
                # Transform ALL data
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['kernel'] = kernel
                logger.info(f"Components: {n_components}, Kernel: {kernel}")
            
            # METHOD 3: ICA
            elif method == 'ica':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1], X_train.shape[0])
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                # Fit on TRAIN only
                reducer = FastICA(
                    n_components=n_components,
                    random_state=random_state,
                    max_iter=1000
                )
                reducer.fit(X_train)
                
                # Transform ALL data
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                logger.info(f"Components: {n_components}")
            
            # METHOD 4: Factor Analysis
            elif method == 'factor_analysis':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1] // 2, X_train.shape[0])
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                # Fit on TRAIN only
                reducer = FactorAnalysis(
                    n_components=n_components,
                    random_state=random_state
                )
                reducer.fit(X_train)
                
                # Transform ALL data
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                logger.info(f"Components: {n_components}")
            
            # METHOD 5: Sparse PCA
            elif method == 'sparse_pca':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1] // 2, X_train.shape[0])
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                alpha = params.get('alpha', 1.0)
                
                # Fit on TRAIN only
                reducer = SparsePCA(
                    n_components=n_components,
                    alpha=alpha,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                
                # Transform ALL data
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['alpha'] = float(alpha)
                logger.info(f"Components: {n_components}, Alpha: {alpha}")
            
            # METHOD 6: t-SNE (for visualization)
            elif method == 'tsne':
                n_components = min(int(params.get('n_components', 2)), 3)
                perplexity = params.get('perplexity', 30.0)
                n_iter = params.get('n_iter', 1000)
                
                max_perplexity = (X_train.shape[0] - 1) / 3.0
                if perplexity > max_perplexity:
                    perplexity = max_perplexity
                    logger.warning(f"Reduced perplexity to {perplexity:.1f}")
                
                # Fit on TRAIN only
                reducer = TSNE(
                    n_components=n_components,
                    perplexity=perplexity,
                    n_iter=n_iter,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                
                # Transform ALL data
                X_reduced = reducer.fit_transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['perplexity'] = float(perplexity)
                logger.info(f"Components: {n_components}, Perplexity: {perplexity}")
            
            # METHOD 7: Isomap
            elif method == 'isomap':
                n_components = int(params.get('n_components', 2))
                n_neighbors = params.get('n_neighbors', 5)
                
                if n_neighbors >= X_train.shape[0]:
                    n_neighbors = max(2, X_train.shape[0] - 1)
                    logger.warning(f"Reduced n_neighbors to {n_neighbors}")
                
                # Fit on TRAIN only
                reducer = Isomap(
                    n_components=n_components,
                    n_neighbors=n_neighbors,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                
                # Transform ALL data
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['n_neighbors'] = int(n_neighbors)
                logger.info(f"Components: {n_components}, Neighbors: {n_neighbors}")
            
            # METHOD 8: LLE
            elif method == 'lle':
                n_components = int(params.get('n_components', 2))
                n_neighbors = params.get('n_neighbors', 5)
                
                if n_neighbors >= X_train.shape[0]:
                    n_neighbors = max(2, X_train.shape[0] - 1)
                    logger.warning(f"Reduced n_neighbors to {n_neighbors}")
                
                # Fit on TRAIN only
                reducer = LocallyLinearEmbedding(
                    n_components=n_components,
                    n_neighbors=n_neighbors,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                
                # Transform ALL data
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['n_neighbors'] = int(n_neighbors)
                logger.info(f"Components: {n_components}, Neighbors: {n_neighbors}")
            
            # METHOD 9: MDS
            elif method == 'mds':
                n_components = int(params.get('n_components', 2))
                
                # Fit on TRAIN only
                reducer = MDS(
                    n_components=n_components,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                
                # Transform ALL data
                X_reduced = reducer.fit_transform(X_all)
                
                metadata['n_components'] = int(n_components)
                logger.info(f"Components: {n_components}")
            
            else:
                raise ValueError(
                    f"Unknown method: {method}. "
                    f"Available: pca, kernel_pca, ica, factor_analysis, sparse_pca, "
                    f"tsne, isomap, lle, mds, none"
                )
            
            # Create reduced DataFrame
            component_names = [f"Component_{i+1}" for i in range(X_reduced.shape[1])]
            df_reduced = pd.DataFrame(X_reduced, columns=component_names, index=df.index)
            
            # Preserve non-numeric columns
            if non_numeric_cols:
                for col in non_numeric_cols:
                    df_reduced[col] = df[col]
            
            metadata['output_features'] = len(component_names)
            metadata['component_names'] = component_names
            metadata['reduction_ratio'] = float(len(component_names) / len(numeric_cols))
            metadata['features_removed'] = len(numeric_cols) - len(component_names)
            
            logger.info(f"\nReduction: {len(numeric_cols)} â†’ {len(component_names)} features")
            logger.info(f"Features removed: {metadata['features_removed']}")
            logger.info("")
            
            return df_reduced, metadata
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--method", default='pca')
            parser.add_argument("--method_params", default='{}')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_metadata", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED DIMENSIONALITY REDUCTION")
            logger.info("="*80)
            logger.info(f"Method: {args.method}")
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_metadata)
                
                # Parse parameters
                params = json.loads(args.method_params)
                
                # Load data
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Reduce
                df_reduced, metadata = reduce_dimensions_unified(
                    df=df,
                    train_indices=train_indices,
                    method=args.method,
                    params=params
                )
                
                # Save
                df_reduced.to_csv(args.output_preprocessed_data, index=False)
                logger.info(f"Reduced data saved")
                
                with open(args.output_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                logger.info(f"Metadata saved")
                
                logger.info("")
                logger.info("="*80)
                logger.info("DIMENSIONALITY REDUCTION COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.method}")
                logger.info(f"Components: {metadata['output_features']}")
                logger.info(f"Reduction: {metadata['reduction_ratio']:.2%}")
                logger.info(f"Final shape: {df_reduced.shape}")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_metadata
      - {outputPath: metadata}
