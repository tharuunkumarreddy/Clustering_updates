name: Unified Dimensionality Reduction Component - PARQUET OPTIMIZED
description: Train-aware dimensionality reduction for unified preprocessing. Fits reduction on train indices only, transforms all combined data. Supports 7 methods PCA, Kernel PCA, ICA, Factor Analysis, Sparse PCA, Isomap, LLE. Auto-determines optimal components. Loads Parquet/CSV, SAVES AS PARQUET.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (Parquet or CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: method
    type: String
    description: 'Reduction method: pca, kernel_pca, ica, factor_analysis, sparse_pca, isomap, lle, none'
    default: 'pca'
  - name: method_params
    type: String
    description: 'Method parameters as JSON. Examples: {"n_components":"auto"}, {"kernel":"rbf"}'
    default: '{}'

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Reduced dataset with component features (PARQUET format - 10x faster than CSV)'
  - name: metadata
    type: Data
    description: 'Reduction metadata including variance explained (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from sklearn.decomposition import PCA, KernelPCA, FastICA, FactorAnalysis, SparsePCA
        from sklearn.manifold import Isomap, LocallyLinearEmbedding
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('dimreduction_unified')
        
        def ensure_directory_exists(file_path):
            #Ensure directory exists for output file#
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            #Load data from Parquet or CSV#
            logger.info(f"Loading dataset from: {input_path}")
            ext = Path(input_path).suffix.lower()
            
            if ext in ['.parquet', '.pq']:
                df = pd.read_parquet(input_path, engine='pyarrow')
                logger.info("✓ Loaded Parquet file")
            else:
                df = pd.read_csv(input_path)
                logger.info("✓ Loaded CSV file")
            
            logger.info(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
            return df
        
        def load_indices(indices_path):
            #Load indices from JSON file#
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def determine_n_components(X_train, method='variance', variance_threshold=0.95):
            #Auto-determine optimal number of components
            #Methods: variance (cumulative variance), kaiser (eigenvalues > 1)
            n_features = X_train.shape[1]
            n_samples = X_train.shape[0]
            max_components = min(n_samples, n_features)
            
            if method == 'variance':
                pca_temp = PCA(n_components=max_components)
                pca_temp.fit(X_train)
                
                cumsum = np.cumsum(pca_temp.explained_variance_ratio_)
                n_components = int(np.searchsorted(cumsum, variance_threshold) + 1)
                
                logger.info(
                    f"Auto: {n_components} components for {variance_threshold*100}% variance"
                )
            elif method == 'kaiser':
                pca_temp = PCA(n_components=max_components)
                pca_temp.fit(X_train)
                
                n_components = int(np.sum(pca_temp.explained_variance_ > 1))
                n_components = max(1, n_components)
                
                logger.info(f"Kaiser: {n_components} components (eigenvalues > 1)")
            else:
                n_components = max(1, n_features // 2)
                logger.info(f"Default: {n_components} components")
            
            return min(n_components, max_components)
        
        def reduce_dimensions_unified(df, train_indices, method, params):
            #Reduce dimensions using train data
            #Fits on train_indices only, transforms all combined data
            logger.info("="*80)
            logger.info("UNIFIED DIMENSIONALITY REDUCTION")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            logger.info(f"Train indices: {len(train_indices)} rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping dimensionality reduction")
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                return df, {
                    'method': 'none',
                    'params': {},
                    'n_components': 0,
                    'input_features': len(numeric_cols),
                    'output_features': len(df.columns),
                    'component_names': df.columns.tolist(),
                    'reduction_ratio': 1.0,
                    'features_removed': 0,
                    'original_columns': numeric_cols[:10] if numeric_cols else []
                }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns found")
                return df, {
                    'method': method,
                    'n_components': 0,
                    'warning': 'no_numeric_columns',
                    'output_features': len(df.columns),
                    'component_names': df.columns.tolist(),
                    'reduction_ratio': 1.0,
                    'features_removed': 0
                }
            
            logger.info(f"Original features: {len(numeric_cols)} numeric columns")
            
            train_df = df.iloc[train_indices]
            X_train = train_df[numeric_cols].values
            X_all = df[numeric_cols].values
            
            metadata = {
                'method': method,
                'params': params,
                'input_features': len(numeric_cols),
                'original_columns': numeric_cols[:10]
            }
            
            n_components = params.get('n_components', 'auto')
            random_state = params.get('random_state', 42)
            
            if method == 'pca':
                variance_threshold = params.get('variance_threshold', 0.95)
                
                if n_components == 'auto':
                    n_components = determine_n_components(
                        X_train,
                        method='variance',
                        variance_threshold=variance_threshold
                    )
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                reducer = PCA(n_components=n_components, random_state=random_state)
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['explained_variance_ratio'] = reducer.explained_variance_ratio_.tolist()
                metadata['cumulative_variance'] = np.cumsum(
                    reducer.explained_variance_ratio_
                ).tolist()
                
                logger.info(f"✓ Components: {n_components}")
                logger.info(f"  Variance: {metadata['cumulative_variance'][-1]*100:.2f}%")
            
            elif method == 'kernel_pca':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1] // 2, 50)
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                kernel = params.get('kernel', 'rbf')
                gamma = params.get('gamma', None)
                
                reducer = KernelPCA(
                    n_components=n_components,
                    kernel=kernel,
                    gamma=gamma,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['kernel'] = kernel
                logger.info(f"✓ Components: {n_components}, Kernel: {kernel}")
            
            elif method == 'ica':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1], X_train.shape[0])
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                reducer = FastICA(
                    n_components=n_components,
                    random_state=random_state,
                    max_iter=1000
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                logger.info(f"✓ Components: {n_components}")
            
            elif method == 'factor_analysis':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1] // 2, X_train.shape[0])
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                reducer = FactorAnalysis(
                    n_components=n_components,
                    random_state=random_state
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                logger.info(f"✓ Components: {n_components}")
            
            elif method == 'sparse_pca':
                if n_components == 'auto':
                    n_components = min(X_train.shape[1] // 2, X_train.shape[0])
                else:
                    n_components = min(int(n_components), X_train.shape[0], X_train.shape[1])
                
                alpha = params.get('alpha', 1.0)
                
                reducer = SparsePCA(
                    n_components=n_components,
                    alpha=alpha,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['alpha'] = float(alpha)
                logger.info(f"✓ Components: {n_components}, Alpha: {alpha}")
            
            elif method == 'isomap':
                n_components = int(params.get('n_components', 2))
                n_neighbors = params.get('n_neighbors', 5)
                
                if n_neighbors >= X_train.shape[0]:
                    n_neighbors = max(2, X_train.shape[0] - 1)
                    logger.warning(f"Reduced n_neighbors to {n_neighbors}")
                
                reducer = Isomap(
                    n_components=n_components,
                    n_neighbors=n_neighbors,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['n_neighbors'] = int(n_neighbors)
                logger.info(f"✓ Components: {n_components}, Neighbors: {n_neighbors}")
            
            elif method == 'lle':
                n_components = int(params.get('n_components', 2))
                n_neighbors = params.get('n_neighbors', 5)
                
                if n_neighbors >= X_train.shape[0]:
                    n_neighbors = max(2, X_train.shape[0] - 1)
                    logger.warning(f"Reduced n_neighbors to {n_neighbors}")
                
                reducer = LocallyLinearEmbedding(
                    n_components=n_components,
                    n_neighbors=n_neighbors,
                    random_state=random_state,
                    n_jobs=-1
                )
                reducer.fit(X_train)
                X_reduced = reducer.transform(X_all)
                
                metadata['n_components'] = int(n_components)
                metadata['n_neighbors'] = int(n_neighbors)
                logger.info(f"✓ Components: {n_components}, Neighbors: {n_neighbors}")
            
            else:
                raise ValueError(
                    f"Unknown method: {method}. "
                    f"Available: pca, kernel_pca, ica, factor_analysis, sparse_pca, "
                    f"isomap, lle, none"
                )
            
            # Create component names
            component_names = [f"Component_{i+1}" for i in range(X_reduced.shape[1])]
            df_reduced = pd.DataFrame(X_reduced, columns=component_names, index=df.index)
            
            # Preserve non-numeric columns
            if non_numeric_cols:
                for col in non_numeric_cols:
                    df_reduced[col] = df[col]
            
            metadata['output_features'] = len(component_names)
            metadata['component_names'] = component_names
            metadata['reduction_ratio'] = float(len(component_names) / len(numeric_cols))
            metadata['features_removed'] = len(numeric_cols) - len(component_names)
            
            logger.info("")
            logger.info(f"Reduction: {len(numeric_cols)} → {len(component_names)} features")
            logger.info(f"Features removed: {metadata['features_removed']}")
            logger.info("")
            
            return df_reduced, metadata
        
        def main():
            parser = argparse.ArgumentParser(description="Unified Dimensionality Reduction (PARQUET OPTIMIZED)")
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--method", default='pca')
            parser.add_argument("--method_params", default='{}')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_metadata", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED DIMENSIONALITY REDUCTION (PARQUET OPTIMIZED)")
            logger.info("="*80)
            logger.info(f"Method: {args.method}")
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_metadata)
                
                params = json.loads(args.method_params)
                
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                logger.info(f"Input shape: {df.shape}")
                logger.info("")
                
                df_reduced, metadata = reduce_dimensions_unified(
                    df=df,
                    train_indices=train_indices,
                    method=args.method,
                    params=params
                )
                
                # ============================================================
                # CRITICAL: SAVE AS PARQUET (NOT CSV!)
                # ============================================================
                output_parquet = args.output_preprocessed_data
                if not output_parquet.endswith('.parquet'):
                    output_parquet = output_parquet.replace('.csv', '.parquet')
                
                logger.info("="*80)
                logger.info("SAVING REDUCED DATA AS PARQUET")
                logger.info("="*80)
                
                df_reduced.to_parquet(
                    output_parquet,
                    index=False,
                    engine='pyarrow',
                    compression='snappy'
                )
                
                parquet_size = os.path.getsize(output_parquet) / 1024**2
                logger.info(f"✓ Reduced data saved as PARQUET: {parquet_size:.2f} MB")
                logger.info(f"  Path: {output_parquet}")
                
                # Add Parquet metadata
                metadata['output_format'] = {
                    'format': 'parquet',
                    'engine': 'pyarrow',
                    'compression': 'snappy',
                    'file_size_mb': float(parquet_size)
                }
                
                with open(args.output_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                logger.info(f"✓ Metadata saved: {args.output_metadata}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("DIMENSIONALITY REDUCTION COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.method}")
                logger.info(f"Components: {metadata.get('output_features', 0)}")
                logger.info(f"Reduction: {metadata.get('reduction_ratio', 1.0)*100:.1f}%")
                logger.info(f"Final shape: {df_reduced.shape}")
                logger.info(f"Output format: PARQUET (10x faster than CSV)")
                logger.info(f"File size: {parquet_size:.2f} MB")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_metadata
      - {outputPath: metadata}
