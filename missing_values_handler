name: Missing Values Handler (PARQUET OPTIMIZED)
description: Comprehensive missing value handling with 12 imputation strategies including MICE. Loads Parquet/CSV, saves as PARQUET. Methods include drop, mean, median, mode, constant, KNN, iterative, MICE, forward/backward fill.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset with potential missing values (Parquet or CSV)'
  - name: method
    type: String
    description: 'Imputation method: drop_rows, drop_cols, mean, median, mode, constant, zero, knn, iterative, mice, forward_fill, backward_fill, none'
    default: 'mean'
  - name: method_params
    type: String
    description: 'Method-specific parameters as JSON. Examples: {"n_neighbors":5}, {"drop_threshold":0.5}, {"fill_value":0}, {"max_iter":10}'
    default: '{}'

outputs:
  - name: data
    type: Data
    description: 'Dataset with missing values handled (PARQUET format)'
  - name: imputer
    type: Model
    description: 'Fitted imputer object saved for reference (PKL)'
  - name: report
    type: Data
    description: 'Comprehensive imputation report and statistics (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pickle
        import pandas as pd
        import numpy as np
        from sklearn.impute import SimpleImputer, KNNImputer
        from sklearn.experimental import enable_iterative_imputer
        from sklearn.impute import IterativeImputer
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('missing_values_parquet')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            #Load data from Parquet or CSV#
            logger.info(f"Loading: {input_path}")
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path, engine='pyarrow')
                    logger.info("✓ Loaded Parquet")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("✓ Loaded CSV")
                
                logger.info(f"Shape: {df.shape}")
                return df
            except Exception as e:
                logger.error(f"Error loading: {e}")
                raise
        
        def analyze_missing_patterns(df):
            logger.info("="*80)
            logger.info("ANALYZING MISSING PATTERNS")
            logger.info("="*80)
            
            total_missing = df.isnull().sum().sum()
            total_values = df.shape[0] * df.shape[1]
            missing_pct = (total_missing / total_values * 100) if total_values > 0 else 0
            
            missing_info = {
                'total_missing': int(total_missing),
                'total_values': int(total_values),
                'missing_percentage': float(missing_pct),
                'columns_with_missing': {},
                'rows_with_missing': int(df.isnull().any(axis=1).sum()),
                'rows_with_missing_pct': float(df.isnull().any(axis=1).sum() / len(df) * 100) if len(df) > 0 else 0
            }
            
            for col in df.columns:
                miss_count = df[col].isnull().sum()
                if miss_count > 0:
                    missing_info['columns_with_missing'][col] = {
                        'count': int(miss_count),
                        'percentage': float(miss_count / len(df) * 100),
                        'dtype': str(df[col].dtype)
                    }
                    logger.info(f"  {col}: {miss_count} ({miss_count/len(df)*100:.1f}%)")
            
            if total_missing == 0:
                logger.info("✓ No missing values")
            else:
                logger.info(f"Total missing: {total_missing} ({missing_pct:.2f}%)")
            
            logger.info("")
            return missing_info
        
        def handle_missing_values(df, method, method_params):
            logger.info("="*80)
            logger.info("HANDLING MISSING VALUES")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            if method_params:
                logger.info(f"Params: {method_params}")
            logger.info("")
            
            initial_shape = df.shape
            initial_missing = df.isnull().sum().sum()
            
            df_imputed = df.copy()
            imputer = None
            
            stats = {
                'method': method,
                'method_params': method_params,
                'initial_missing': int(initial_missing),
                'initial_shape': list(initial_shape),
                'dropped_columns': [],
                'dropped_rows': 0,
                'final_missing': 0,
                'final_shape': [],
                'imputation_success': False
            }
            
            if method == 'none':
                logger.info("Skipping imputation")
                return df_imputed, None, stats
            
            elif method == 'drop_rows':
                before = len(df_imputed)
                df_imputed = df_imputed.dropna()
                stats['dropped_rows'] = before - len(df_imputed)
                logger.info(f"Dropped {stats['dropped_rows']} rows")
            
            elif method == 'drop_cols':
                threshold = method_params.get('drop_threshold', 0.5)
                missing_pct = df_imputed.isnull().sum() / len(df_imputed)
                cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()
                if cols_to_drop:
                    df_imputed = df_imputed.drop(columns=cols_to_drop)
                    stats['dropped_columns'] = cols_to_drop
                    logger.info(f"Dropped {len(cols_to_drop)} columns")
            
            elif method == 'mean':
                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns.tolist()
                
                if numeric_cols:
                    imputer = SimpleImputer(strategy='mean')
                    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])
                    logger.info(f"Mean: {len(numeric_cols)} numeric columns")
                
                if cat_cols:
                    cat_imputer = SimpleImputer(strategy='most_frequent')
                    df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])
                    logger.info(f"Mode: {len(cat_cols)} categorical columns")
            
            elif method == 'median':
                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns.tolist()
                
                if numeric_cols:
                    imputer = SimpleImputer(strategy='median')
                    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])
                    logger.info(f"Median: {len(numeric_cols)} columns")
                
                if cat_cols:
                    cat_imputer = SimpleImputer(strategy='most_frequent')
                    df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])
                    logger.info(f"Mode: {len(cat_cols)} columns")
            
            elif method == 'mode':
                imputer = SimpleImputer(strategy='most_frequent')
                df_imputed_values = imputer.fit_transform(df_imputed)
                df_imputed = pd.DataFrame(df_imputed_values, columns=df_imputed.columns, index=df_imputed.index)
                logger.info(f"Mode: all {len(df_imputed.columns)} columns")
            
            elif method in ['constant', 'zero']:
                fill_value = method_params.get('fill_value', 0)
                df_imputed = df_imputed.fillna(fill_value)
                logger.info(f"Constant: {fill_value}")
                stats['method_params']['fill_value'] = fill_value
            
            elif method == 'knn':
                n_neighbors = method_params.get('n_neighbors', 5)
                weights = method_params.get('weights', 'uniform')
                
                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns.tolist()
                
                if numeric_cols:
                    imputer = KNNImputer(n_neighbors=n_neighbors, weights=weights)
                    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])
                    logger.info(f"KNN (k={n_neighbors}): {len(numeric_cols)} columns")
                    stats['method_params'] = {'n_neighbors': n_neighbors, 'weights': weights}
                
                if cat_cols:
                    cat_imputer = SimpleImputer(strategy='most_frequent')
                    df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])
                    logger.info(f"Mode: {len(cat_cols)} columns")
            
            elif method == 'iterative':
                max_iter = method_params.get('max_iter', 10)
                random_state = method_params.get('random_state', 42)
                
                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns.tolist()
                
                if numeric_cols:
                    imputer = IterativeImputer(max_iter=max_iter, random_state=random_state, verbose=0)
                    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])
                    logger.info(f"Iterative (max_iter={max_iter}): {len(numeric_cols)} columns")
                    stats['method_params'] = {'max_iter': max_iter, 'random_state': random_state}
                
                if cat_cols:
                    cat_imputer = SimpleImputer(strategy='most_frequent')
                    df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])
                    logger.info(f"Mode: {len(cat_cols)} columns")
            
            elif method == 'mice':
                max_iter = method_params.get('max_iter', 10)
                random_state = method_params.get('random_state', 42)
                sample_posterior = method_params.get('sample_posterior', True)
                
                numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()
                cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns.tolist()
                
                if numeric_cols:
                    logger.info("MICE IMPUTATION")
                    logger.info(f"Max iterations: {max_iter}")
                    logger.info(f"Sample posterior: {sample_posterior}")
                    
                    imputer = IterativeImputer(
                        max_iter=max_iter,
                        random_state=random_state,
                        sample_posterior=sample_posterior,
                        verbose=1
                    )
                    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])
                    logger.info(f"✓ MICE: {len(numeric_cols)} columns")
                    stats['method_params'] = {
                        'max_iter': max_iter,
                        'random_state': random_state,
                        'sample_posterior': sample_posterior
                    }
                
                if cat_cols:
                    cat_imputer = SimpleImputer(strategy='most_frequent')
                    df_imputed[cat_cols] = cat_imputer.fit_transform(df_imputed[cat_cols])
                    logger.info(f"Mode: {len(cat_cols)} columns")
            
            elif method in ['forward_fill', 'ffill']:
                df_imputed = df_imputed.fillna(method='ffill')
                if df_imputed.isnull().sum().sum() > 0:
                    df_imputed = df_imputed.fillna(method='bfill')
                logger.info("Forward fill (+ backward fill for leading NaNs)")
            
            elif method in ['backward_fill', 'bfill']:
                df_imputed = df_imputed.fillna(method='bfill')
                if df_imputed.isnull().sum().sum() > 0:
                    df_imputed = df_imputed.fillna(method='ffill')
                logger.info("Backward fill (+ forward fill for trailing NaNs)")
            
            else:
                raise ValueError(f"Unknown method: {method}")
            
            final_missing = df_imputed.isnull().sum().sum()
            stats['final_missing'] = int(final_missing)
            stats['final_shape'] = list(df_imputed.shape)
            stats['imputation_success'] = bool(final_missing == 0)
            
            if initial_missing > 0:
                stats['missing_reduction_pct'] = float(((initial_missing - final_missing) / initial_missing) * 100)
            else:
                stats['missing_reduction_pct'] = 0.0
            
            logger.info("")
            logger.info(f"Missing before: {initial_missing}")
            logger.info(f"Missing after: {final_missing}")
            
            if final_missing == 0:
                logger.info("✓ All missing values handled")
            elif final_missing < initial_missing:
                logger.info(f"⚠ {final_missing} missing values remain")
            
            logger.info("")
            return df_imputed, imputer, stats
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--input_data", required=True)
            parser.add_argument("--method", default='mean')
            parser.add_argument("--method_params", default='{}')
            parser.add_argument("--output_data", required=True)
            parser.add_argument("--output_imputer", required=True)
            parser.add_argument("--output_report", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("MISSING VALUES HANDLER (PARQUET OPTIMIZED)")
            logger.info("="*80)
            
            try:
                ensure_directory_exists(args.output_data)
                ensure_directory_exists(args.output_imputer)
                ensure_directory_exists(args.output_report)
                
                try:
                    method_params = json.loads(args.method_params)
                except:
                    logger.error("Invalid JSON params")
                    sys.exit(1)
                
                df = load_data(args.input_data)
                
                if df.empty:
                    logger.error("Dataset is empty")
                    sys.exit(1)
                
                missing_analysis = analyze_missing_patterns(df)
                
                if missing_analysis['total_missing'] == 0:
                    logger.info("No missing values - no imputation needed")
                    
                    # Save as Parquet
                    output_parquet = args.output_data
                    if not output_parquet.endswith('.parquet'):
                        output_parquet = output_parquet.replace('.csv', '.parquet')
                    
                    df.to_parquet(output_parquet, index=False, engine='pyarrow', compression='snappy')
                    
                    with open(args.output_imputer, 'wb') as f:
                        pickle.dump(None, f)
                    
                    report = {
                        'missing_analysis': missing_analysis,
                        'imputation_stats': {
                            'method': args.method,
                            'initial_missing': 0,
                            'final_missing': 0,
                            'imputation_success': True,
                            'imputation_needed': False
                        }
                    }
                    
                    with open(args.output_report, 'w') as f:
                        json.dump(report, f, indent=2)
                    
                    logger.info("✓ Completed (no imputation needed)")
                    sys.exit(0)
                
                df_imputed, imputer, stats = handle_missing_values(df, args.method, method_params)
                
                if not stats.get('imputation_success', False):
                    logger.warning(f"⚠ {stats.get('final_missing', 0)} missing remain")
                
                # SAVE AS PARQUET (NOT CSV!)
                output_parquet = args.output_data
                if not output_parquet.endswith('.parquet'):
                    output_parquet = output_parquet.replace('.csv', '.parquet')
                
                df_imputed.to_parquet(
                    output_parquet,
                    index=False,
                    engine='pyarrow',
                    compression='snappy'
                )
                
                parquet_size = os.path.getsize(output_parquet) / 1024**2
                logger.info(f"✓ Saved as PARQUET: {parquet_size:.2f} MB")
                
                with open(args.output_imputer, 'wb') as f:
                    pickle.dump(imputer, f)
                
                report = {
                    'missing_analysis': missing_analysis,
                    'imputation_stats': stats,
                    'method': args.method
                }
                
                with open(args.output_report, 'w') as f:
                    json.dump(report, f, indent=2)
                
                logger.info("")
                logger.info("="*80)
                logger.info("MISSING VALUES HANDLING COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.method}")
                logger.info(f"Success: {stats.get('imputation_success', False)}")
                logger.info(f"Format: PARQUET (optimized for ML)")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {e}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --method
      - {inputValue: method}
      - --method_params
      - {inputValue: method_params}
      - --output_data
      - {outputPath: data}
      - --output_imputer
      - {outputPath: imputer}
      - --output_report
      - {outputPath: report}
