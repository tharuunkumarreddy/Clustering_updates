name: Unified Outlier Detection & Handling Component
description: Train-aware outlier detection for unified preprocessing. Fits detection on train indices only, transforms all combined data. Supports 4 detection methods (Z-score, IQR, LOF, Isolation Forest) and 7 handling strategies. Prevents data leakage.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: detection_method
    type: String
    description: 'Outlier detection method: zscore, iqr, lof, isolation_forest, none'
    default: 'zscore'
  - name: detection_params
    type: String
    description: 'Detection parameters as JSON. Examples: {"threshold":3.0}, {"factor":1.5}, {"n_neighbors":20}, {"contamination":0.1}'
    default: '{}'
  - name: handling_method
    type: String
    description: 'Outlier handling method: remove, clip, cap, winsorize, log_transform, sqrt_transform, flag, none'
    default: 'clip'
  - name: handling_params
    type: String
    description: 'Handling parameters as JSON. Examples: {"use_iqr":true,"factor":1.5}, {"limits":[0.05,0.05]}'
    default: '{}'

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Dataset with outliers handled (CSV)'
  - name: report
    type: Data
    description: 'Outlier detection and handling report (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from scipy import stats
        from scipy.stats import mstats
        from sklearn.neighbors import LocalOutlierFactor
        from sklearn.ensemble import IsolationForest
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('outlier_unified')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            ext = Path(input_path).suffix.lower()
            if ext in ['.parquet', '.pq']:
                return pd.read_parquet(input_path)
            return pd.read_csv(input_path)
        
        def load_indices(indices_path):
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def detect_outliers_unified(df, train_indices, method, params):
            #Detect outliers using train-aware unified approach#
            logger.info("="*80)
            logger.info("UNIFIED OUTLIER DETECTION")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            logger.info(f"Train indices: {len(train_indices)} rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping")
                return pd.Series(False, index=df.index), None, {
                    'method': 'none',
                    'n_outliers': 0
                }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns")
                return pd.Series(False, index=df.index), None, {
                    'method': method,
                    'n_outliers': 0,
                    'warning': 'no_numeric_columns'
                }
            
            logger.info(f"Analyzing {len(numeric_cols)} numeric columns")
            
            # Extract TRAIN subset for fitting
            train_df = df.iloc[train_indices]
            
            detection_info = {
                'method': method,
                'params': params,
                'numeric_columns': numeric_cols,
                'n_numeric_cols': len(numeric_cols)
            }
            
            detector = None
            outlier_mask = pd.Series(False, index=df.index)
            
            # METHOD 1: Z-Score
            if method == 'zscore':
                threshold = params.get('threshold', 3.0)
                logger.info(f"Z-score threshold: {threshold}")
                
                # Calculate mean/std from TRAIN only
                train_mean = train_df[numeric_cols].mean()
                train_std = train_df[numeric_cols].std()
                
                # Apply to ALL data
                z_scores = np.abs((df[numeric_cols] - train_mean) / train_std)
                outlier_mask = (z_scores > threshold).any(axis=1)
                
                detector = {
                    'method': 'zscore',
                    'threshold': threshold,
                    'mean': train_mean.to_dict(),
                    'std': train_std.to_dict()
                }
                
                detection_info['threshold'] = threshold
                logger.info(f"Fitted on train, applied to all {len(df)} rows")
            
            # METHOD 2: IQR
            elif method == 'iqr':
                factor = params.get('factor', 1.5)
                logger.info(f"IQR factor: {factor}")
                
                bounds = {}
                
                # Calculate IQR bounds from TRAIN only
                for col in numeric_cols:
                    Q1 = train_df[col].quantile(0.25)
                    Q3 = train_df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    
                    lower = Q1 - factor * IQR
                    upper = Q3 + factor * IQR
                    
                    bounds[col] = {
                        'lower': float(lower),
                        'upper': float(upper),
                        'Q1': float(Q1),
                        'Q3': float(Q3),
                        'IQR': float(IQR)
                    }
                    
                    # Apply to ALL data
                    col_outliers = (df[col] < lower) | (df[col] > upper)
                    outlier_mask = outlier_mask | col_outliers
                    
                    n_out = col_outliers.sum()
                    if n_out > 0:
                        logger.info(f"  {col}: {n_out} outliers ([{lower:.2f}, {upper:.2f}])")
                
                detector = {
                    'method': 'iqr',
                    'bounds': bounds,
                    'factor': factor
                }
                
                detection_info['bounds'] = bounds
            
            # METHOD 3: LOF
            elif method == 'lof':
                n_neighbors = params.get('n_neighbors', 20)
                contamination = params.get('contamination', 0.1)
                
                logger.info(f"LOF: n_neighbors={n_neighbors}, contamination={contamination}")
                
                # Fit on TRAIN only
                X_train = train_df[numeric_cols].values
                
                if len(train_df) < n_neighbors:
                    n_neighbors = max(2, len(train_df) - 1)
                    logger.warning(f"Reduced n_neighbors to {n_neighbors}")
                
                lof = LocalOutlierFactor(
                    n_neighbors=n_neighbors,
                    contamination=contamination,
                    novelty=True
                )
                lof.fit(X_train)
                
                # Apply to ALL data
                X_all = df[numeric_cols].values
                outlier_labels = lof.predict(X_all)
                outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                
                detector = lof
                detection_info['n_neighbors'] = n_neighbors
                detection_info['contamination'] = contamination
                logger.info(f"Fitted on train, predicted on all {len(df)} rows")
            
            # METHOD 4: Isolation Forest
            elif method == 'isolation_forest':
                contamination = params.get('contamination', 0.1)
                n_estimators = params.get('n_estimators', 100)
                random_state = params.get('random_state', 42)
                
                logger.info(f"IsolationForest: contamination={contamination}, n_estimators={n_estimators}")
                
                # Fit on TRAIN only
                X_train = train_df[numeric_cols].values
                
                iso = IsolationForest(
                    contamination=contamination,
                    n_estimators=n_estimators,
                    random_state=random_state,
                    n_jobs=-1
                )
                iso.fit(X_train)
                
                # Apply to ALL data
                X_all = df[numeric_cols].values
                outlier_labels = iso.predict(X_all)
                outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                
                detector = iso
                detection_info['contamination'] = contamination
                detection_info['n_estimators'] = n_estimators
                logger.info(f"Fitted on train, predicted on all {len(df)} rows")
            
            else:
                raise ValueError(
                    f"Unknown method: {method}. "
                    f"Available: zscore, iqr, lof, isolation_forest, none"
                )
            
            n_outliers = outlier_mask.sum()
            pct = (n_outliers / len(df) * 100) if len(df) > 0 else 0
            detection_info['n_outliers'] = int(n_outliers)
            detection_info['outlier_percentage'] = float(pct)
            
            logger.info(f"\nDetected {n_outliers} outliers ({pct:.2f}%)")
            logger.info("")
            
            return outlier_mask, detector, detection_info
        
        def handle_outliers_unified(df, outlier_mask, method, params, detection_info):
            #Handle detected outliers#
            logger.info("="*80)
            logger.info("HANDLING OUTLIERS")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping")
                return df, {'method': 'none', 'n_rows_removed': 0}
            
            df_handled = df.copy()
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            stats = {
                'method': method,
                'params': params,
                'n_outliers_detected': int(outlier_mask.sum()),
                'n_rows_removed': 0
            }
            
            # METHOD 1: Remove
            if method == 'remove':
                df_handled = df[~outlier_mask].reset_index(drop=True)
                stats['n_rows_removed'] = outlier_mask.sum()
                logger.info(f"Removed {outlier_mask.sum()} outlier rows")
            
            # METHOD 2: Clip/Cap
            elif method in ['clip', 'cap']:
                use_iqr = params.get('use_iqr', True)
                factor = params.get('factor', 1.5)
                
                if detection_info and 'bounds' in detection_info:
                    bounds = detection_info['bounds']
                elif use_iqr:
                    bounds = {}
                    for col in numeric_cols:
                        Q1 = df[col].quantile(0.25)
                        Q3 = df[col].quantile(0.75)
                        IQR = Q3 - Q1
                        bounds[col] = {
                            'lower': Q1 - factor * IQR,
                            'upper': Q3 + factor * IQR
                        }
                else:
                    bounds = {col: {'lower': df[col].min(), 'upper': df[col].max()} for col in numeric_cols}
                
                for col in numeric_cols:
                    if col in bounds:
                        df_handled[col] = df_handled[col].clip(
                            lower=bounds[col]['lower'],
                            upper=bounds[col]['upper']
                        )
                logger.info(f"Clipped {len(numeric_cols)} columns")
            
            # METHOD 3: Winsorize
            elif method == 'winsorize':
                limits = params.get('limits', [0.05, 0.05])
                for col in numeric_cols:
                    df_handled[col] = mstats.winsorize(df_handled[col], limits=limits)
                logger.info(f"Winsorized with limits {limits}")
            
            # METHOD 4: Log transform
            elif method == 'log_transform':
                for col in numeric_cols:
                    if (df_handled[col] > 0).all():
                        df_handled[col] = np.log1p(df_handled[col])
                    else:
                        logger.warning(f"  {col}: skipped (non-positive)")
                logger.info("Log transform applied")
            
            # METHOD 5: Sqrt transform
            elif method == 'sqrt_transform':
                for col in numeric_cols:
                    if (df_handled[col] >= 0).all():
                        df_handled[col] = np.sqrt(df_handled[col])
                    else:
                        logger.warning(f"  {col}: skipped (negative)")
                logger.info("Sqrt transform applied")
            
            # METHOD 6: Flag
            elif method == 'flag':
                flag_col = params.get('flag_column', 'is_outlier')
                df_handled[flag_col] = outlier_mask.astype(int)
                logger.info(f"Added '{flag_col}' flag column")
            
            else:
                raise ValueError(
                    f"Unknown handling method: {method}. "
                    f"Available: remove, clip, cap, winsorize, log_transform, sqrt_transform, flag, none"
                )
            
            stats['initial_shape'] = list(df.shape)
            stats['final_shape'] = list(df_handled.shape)
            
            logger.info(f"Final shape: {df_handled.shape}")
            logger.info("")
            
            return df_handled, stats
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--detection_method", default='zscore')
            parser.add_argument("--detection_params", default='{}')
            parser.add_argument("--handling_method", default='clip')
            parser.add_argument("--handling_params", default='{}')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_report", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED OUTLIER DETECTION & HANDLING")
            logger.info("="*80)
            logger.info(f"Detection: {args.detection_method}")
            logger.info(f"Handling: {args.handling_method}")
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_report)
                
                # Parse parameters
                detection_params = json.loads(args.detection_params)
                handling_params = json.loads(args.handling_params)
                
                # Load data
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Detect outliers (unified approach)
                outlier_mask, detector, detection_info = detect_outliers_unified(
                    df=df,
                    train_indices=train_indices,
                    method=args.detection_method,
                    params=detection_params
                )
                
                # Handle outliers
                df_handled, handling_stats = handle_outliers_unified(
                    df=df,
                    outlier_mask=outlier_mask,
                    method=args.handling_method,
                    params=handling_params,
                    detection_info=detection_info
                )
                
                # Save
                df_handled.to_csv(args.output_preprocessed_data, index=False)
                logger.info(f"Preprocessed data saved")
                
                report = {
                    'detection': detection_info,
                    'handling': handling_stats
                }
                
                with open(args.output_report, 'w') as f:
                    json.dump(report, f, indent=2)
                logger.info(f"Report saved")
                
                logger.info("")
                logger.info("="*80)
                logger.info("OUTLIER PROCESSING COMPLETED")
                logger.info("="*80)
                logger.info(f"Detection: {args.detection_method}")
                logger.info(f"Outliers: {detection_info['n_outliers']}")
                logger.info(f"Final shape: {df_handled.shape}")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --detection_method
      - {inputValue: detection_method}
      - --detection_params
      - {inputValue: detection_params}
      - --handling_method
      - {inputValue: handling_method}
      - --handling_params
      - {inputValue: handling_params}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_report
      - {outputPath: report}
