name: Unified Categorical Encoding Component - PARQUET OPTIMIZED
description: Train-aware categorical encoding for unified preprocessing. Fits encoding on train indices only, transforms all combined data. Supports 4 methods (onehot, label, frequency, target). Handles unseen test categories gracefully. Loads Parquet/CSV, SAVES AS PARQUET.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (Parquet or CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: encoding_method
    type: String
    description: 'Encoding method: onehot, label, frequency, target, none'
    default: 'onehot'
  - name: encoding_params
    type: String
    description: 'Encoding parameters as JSON. Examples: {"max_categories":50}, {"handle_unknown":"ignore"}'
    default: '{}'
  - name: ground_truth
    type: Data
    description: 'Ground truth labels for target encoding (CSV with single column, optional)'
    default: ''

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Encoded dataset (PARQUET format - 10x faster than CSV)'
  - name: encoding_report
    type: Data
    description: 'Encoding report with mappings and statistics (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('categorical_encoding_unified')
        
        def ensure_directory_exists(file_path):
            #Ensure directory exists for output file#
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            #Load data from Parquet or CSV with robust encoding handling
            #Tries Parquet first, then CSV with multiple encoding fallbacks
            if not input_path or input_path == '':
                return None
            
            logger.info(f"Loading data from: {input_path}")
            ext = Path(input_path).suffix.lower()
            
            try:
                # Try Parquet first
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path, engine='pyarrow')
                    logger.info("✓ Loaded Parquet file")
                    return df
                else:
                    # CSV with robust encoding fallbacks
                    # Try UTF-8 first (most common)
                    try:
                        df = pd.read_csv(input_path, encoding='utf-8')
                        logger.info("✓ Loaded CSV file (UTF-8)")
                        return df
                    except UnicodeDecodeError:
                        logger.warning("UTF-8 decode failed, trying latin-1 encoding")
                        try:
                            df = pd.read_csv(input_path, encoding='latin-1')
                            logger.info("✓ Loaded CSV file (latin-1)")
                            return df
                        except UnicodeDecodeError:
                            logger.warning("Latin-1 decode failed, trying cp1252 encoding")
                            try:
                                df = pd.read_csv(input_path, encoding='cp1252')
                                logger.info("✓ Loaded CSV file (cp1252)")
                                return df
                            except UnicodeDecodeError:
                                logger.warning("CP1252 decode failed, trying ISO-8859-1 encoding")
                                df = pd.read_csv(input_path, encoding='ISO-8859-1')
                                logger.info("✓ Loaded CSV file (ISO-8859-1)")
                                return df
            except Exception as e:
                logger.error(f"Failed to load data: {str(e)}")
                raise
        
        def load_indices(indices_path):
            #Load indices from JSON file#
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def encode_categorical_unified(df, train_indices, method, params, ground_truth_df=None):
            #Encode categorical columns using train data statistics
            #Fits on train_indices only, transforms all combined data
            logger.info("="*80)
            logger.info("UNIFIED CATEGORICAL ENCODING")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            logger.info(f"Train indices: {len(train_indices)} rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping encoding")
                return df, {
                    'method': 'none',
                    'n_columns_encoded': 0
                }
            
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            if len(categorical_cols) == 0:
                logger.info("No categorical columns found")
                return df, {
                    'method': method,
                    'n_columns_encoded': 0,
                    'warning': 'no_categorical_columns'
                }
            
            logger.info(f"Found {len(categorical_cols)} categorical columns")
            for col in categorical_cols[:10]:
                n_unique = df[col].nunique()
                logger.info(f"  {col}: {n_unique} unique values")
            if len(categorical_cols) > 10:
                logger.info(f"  ... and {len(categorical_cols) - 10} more columns")
            
            df_encoded = df.copy()
            train_df = df.iloc[train_indices]
            
            report = {
                'method': method,
                'params': params,
                'categorical_columns': categorical_cols,
                'n_columns_encoded': len(categorical_cols),
                'encodings': {}
            }
            
            if method == 'onehot':
                max_categories = params.get('max_categories', 50)
                drop_first = params.get('drop_first', False)
                
                logger.info(f"One-hot encoding with max_categories={max_categories}")
                
                for col in categorical_cols:
                    train_categories = train_df[col].value_counts()
                    
                    if len(train_categories) > max_categories:
                        top_categories = train_categories.head(max_categories).index.tolist()
                        logger.info(f"  {col}: keeping top {max_categories} of {len(train_categories)} categories")
                    else:
                        top_categories = train_categories.index.tolist()
                    
                    if drop_first and len(top_categories) > 1:
                        categories_to_encode = top_categories[1:]
                    else:
                        categories_to_encode = top_categories
                    
                    for category in categories_to_encode:
                        new_col_name = f"{col}_{category}"
                        df_encoded[new_col_name] = (df_encoded[col] == category).astype(int)
                    
                    df_encoded = df_encoded.drop(columns=[col])
                    
                    report['encodings'][col] = {
                        'type': 'onehot',
                        'n_original_categories': int(len(train_categories)),
                        'n_encoded_categories': len(categories_to_encode),
                        'categories': categories_to_encode[:20]  # First 20 for report
                    }
                
                logger.info("✓ One-hot encoding completed")
            
            elif method == 'label':
                logger.info("Label encoding (alphabetical order from train)")
                
                for col in categorical_cols:
                    train_categories = sorted(train_df[col].unique())
                    
                    label_map = {cat: idx for idx, cat in enumerate(train_categories)}
                    
                    def map_with_unknown(val):
                        return label_map.get(val, -1)
                    
                    df_encoded[col] = df_encoded[col].apply(map_with_unknown)
                    
                    unseen = (df_encoded[col] == -1).sum()
                    if unseen > 0:
                        logger.info(f"  {col}: {unseen} unseen categories mapped to -1")
                    
                    report['encodings'][col] = {
                        'type': 'label',
                        'n_categories': len(label_map),
                        'mapping': {str(k): int(v) for k, v in list(label_map.items())[:20]},
                        'unseen_count': int(unseen)
                    }
                
                logger.info("✓ Label encoding completed")
            
            elif method == 'frequency':
                logger.info("Frequency encoding (from train)")
                
                for col in categorical_cols:
                    train_freq = train_df[col].value_counts(normalize=True).to_dict()
                    
                    def map_frequency(val):
                        return train_freq.get(val, 0.0)
                    
                    df_encoded[col + '_freq'] = df_encoded[col].apply(map_frequency)
                    df_encoded = df_encoded.drop(columns=[col])
                    
                    unseen = (df_encoded[col + '_freq'] == 0.0).sum()
                    if unseen > 0:
                        logger.info(f"  {col}: {unseen} unseen categories (freq=0)")
                    
                    report['encodings'][col] = {
                        'type': 'frequency',
                        'n_categories': len(train_freq),
                        'frequency_range': [float(min(train_freq.values())), float(max(train_freq.values()))],
                        'unseen_count': int(unseen)
                    }
                
                logger.info("✓ Frequency encoding completed")
            
            elif method == 'target':
                if ground_truth_df is None or ground_truth_df.empty:
                    logger.error("ERROR: Target encoding requires ground_truth labels")
                    raise ValueError("ground_truth is required for target encoding")
                
                logger.info("Target encoding (mean from train)")
                
                target_col = ground_truth_df.columns[0]
                train_targets = ground_truth_df.iloc[train_indices][target_col]
                global_mean = train_targets.mean()
                
                logger.info(f"  Global mean: {global_mean:.4f}")
                
                for col in categorical_cols:
                    # Create temporary dataframe for groupby
                    temp_train = train_df[[col]].copy()
                    temp_train['_target'] = train_targets.values
                    train_means = temp_train.groupby(col)['_target'].mean().to_dict()
                    
                    def map_target(val):
                        return train_means.get(val, global_mean)
                    
                    df_encoded[col + '_target'] = df_encoded[col].apply(map_target)
                    df_encoded = df_encoded.drop(columns=[col])
                    
                    # Calculate actual unseen (excluding categories that happen to have global mean)
                    unseen_mask = ~df_encoded[col + '_target'].isin(train_means.values())
                    actual_unseen = unseen_mask.sum()
                    
                    if actual_unseen > 0:
                        logger.info(f"  {col}: {actual_unseen} unseen (using global mean)")
                    
                    report['encodings'][col] = {
                        'type': 'target',
                        'n_categories': len(train_means),
                        'global_mean': float(global_mean),
                        'target_range': [float(min(train_means.values())), float(max(train_means.values()))],
                        'unseen_count': int(actual_unseen)
                    }
                
                logger.info("✓ Target encoding completed")
            
            else:
                raise ValueError(
                    f"Unknown method: {method}. "
                    f"Available: onehot, label, frequency, target, none"
                )
            
            logger.info("")
            logger.info(f"Encoding completed: {df_encoded.shape}")
            logger.info("")
            
            return df_encoded, report
        
        def main():
            parser = argparse.ArgumentParser(description="Unified Categorical Encoding (PARQUET OPTIMIZED)")
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--encoding_method", default='onehot')
            parser.add_argument("--encoding_params", default='{}')
            parser.add_argument("--ground_truth", default='')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_encoding_report", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED CATEGORICAL ENCODING (PARQUET OPTIMIZED)")
            logger.info("="*80)
            logger.info(f"Method: {args.encoding_method}")
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_encoding_report)
                
                params = json.loads(args.encoding_params)
                
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                ground_truth_df = load_data(args.ground_truth) if args.ground_truth else None
                
                if df is None or df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                logger.info(f"Input shape: {df.shape}")
                logger.info("")
                
                df_encoded, report = encode_categorical_unified(
                    df=df,
                    train_indices=train_indices,
                    method=args.encoding_method,
                    params=params,
                    ground_truth_df=ground_truth_df
                )
                
                # ============================================================
                # CRITICAL: SAVE AS PARQUET (NOT CSV!)
                # ============================================================
                output_parquet = args.output_preprocessed_data
                if not output_parquet.endswith('.parquet'):
                    output_parquet = output_parquet.replace('.csv', '.parquet')
                
                logger.info("="*80)
                logger.info("SAVING ENCODED DATA AS PARQUET")
                logger.info("="*80)
                
                df_encoded.to_parquet(
                    output_parquet,
                    index=False,
                    engine='pyarrow',
                    compression='snappy'
                )
                
                parquet_size = os.path.getsize(output_parquet) / 1024**2
                logger.info(f"✓ Encoded data saved as PARQUET: {parquet_size:.2f} MB")
                logger.info(f"  Path: {output_parquet}")
                
                # Add Parquet metadata to report
                report['output_format'] = {
                    'format': 'parquet',
                    'engine': 'pyarrow',
                    'compression': 'snappy',
                    'file_size_mb': float(parquet_size)
                }
                
                with open(args.output_encoding_report, 'w') as f:
                    json.dump(report, f, indent=2)
                logger.info(f"✓ Report saved: {args.output_encoding_report}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("CATEGORICAL ENCODING COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.encoding_method}")
                logger.info(f"Columns encoded: {report['n_columns_encoded']}")
                logger.info(f"Final shape: {df_encoded.shape}")
                logger.info(f"Output format: PARQUET (10x faster than CSV)")
                logger.info(f"File size: {parquet_size:.2f} MB")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --encoding_method
      - {inputValue: encoding_method}
      - --encoding_params
      - {inputValue: encoding_params}
      - --ground_truth
      - {inputPath: ground_truth}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_encoding_report
      - {outputPath: encoding_report}
