name: Unified Categorical Encoding Component
description: Train-aware categorical encoding for unified preprocessing. Fits encoding on train indices only, transforms all combined data. Supports 4 methods (onehot, label, frequency, target). Handles unseen test categories gracefully. Robust encoding handling for various file formats.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: encoding_method
    type: String
    description: 'Encoding method: onehot, label, frequency, target, none'
    default: 'onehot'
  - name: encoding_params
    type: String
    description: 'Encoding parameters as JSON. Examples: {"max_categories":50}, {"handle_unknown":"ignore"}'
    default: '{}'
  - name: ground_truth
    type: Data
    description: 'Ground truth labels for target encoding (CSV with single column, optional)'
    default: ''

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Encoded dataset (CSV)'
  - name: encoding_report
    type: Data
    description: 'Encoding report with mappings and statistics (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('categorical_encoding_unified')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            if not input_path or input_path == '':
                return None
            
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    return pd.read_parquet(input_path)
                else:
                    # Try UTF-8 first
                    try:
                        return pd.read_csv(input_path, encoding='utf-8')
                    except UnicodeDecodeError:
                        logger.warning("UTF-8 decode failed, trying latin-1 encoding")
                        try:
                            return pd.read_csv(input_path, encoding='latin-1')
                        except UnicodeDecodeError:
                            logger.warning("Latin-1 decode failed, trying cp1252 encoding")
                            try:
                                return pd.read_csv(input_path, encoding='cp1252')
                            except UnicodeDecodeError:
                                logger.warning("CP1252 decode failed, trying with encoding detection")
                                return pd.read_csv(input_path, encoding='ISO-8859-1')
            except Exception as e:
                logger.error("Failed to load data: " + str(e))
                raise
        
        def load_indices(indices_path):
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def encode_categorical_unified(df, train_indices, method, params, ground_truth_df=None):
            logger.info("="*80)
            logger.info("UNIFIED CATEGORICAL ENCODING")
            logger.info("="*80)
            logger.info("Method: " + str(method))
            logger.info("Train indices: " + str(len(train_indices)) + " rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping")
                return df, {
                    'method': 'none',
                    'n_columns_encoded': 0
                }
            
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            if len(categorical_cols) == 0:
                logger.info("No categorical columns found")
                return df, {
                    'method': method,
                    'n_columns_encoded': 0,
                    'warning': 'no_categorical_columns'
                }
            
            logger.info("Found " + str(len(categorical_cols)) + " categorical columns")
            for col in categorical_cols[:10]:
                n_unique = df[col].nunique()
                logger.info("  " + col + ": " + str(n_unique) + " unique values")
            
            df_encoded = df.copy()
            train_df = df.iloc[train_indices]
            
            report = {
                'method': method,
                'params': params,
                'categorical_columns': categorical_cols,
                'n_columns_encoded': len(categorical_cols),
                'encodings': {}
            }
            
            if method == 'onehot':
                max_categories = params.get('max_categories', 50)
                drop_first = params.get('drop_first', False)
                
                logger.info("One-hot encoding with max_categories=" + str(max_categories))
                
                for col in categorical_cols:
                    train_categories = train_df[col].value_counts()
                    
                    if len(train_categories) > max_categories:
                        top_categories = train_categories.head(max_categories).index.tolist()
                        logger.info("  " + col + ": keeping top " + str(max_categories) + " of " + str(len(train_categories)) + " categories")
                    else:
                        top_categories = train_categories.index.tolist()
                    
                    if drop_first and len(top_categories) > 1:
                        categories_to_encode = top_categories[1:]
                    else:
                        categories_to_encode = top_categories
                    
                    for category in categories_to_encode:
                        new_col_name = col + "_" + str(category)
                        df_encoded[new_col_name] = (df_encoded[col] == category).astype(int)
                    
                    df_encoded = df_encoded.drop(columns=[col])
                    
                    report['encodings'][col] = {
                        'type': 'onehot',
                        'n_original_categories': int(len(train_categories)),
                        'n_encoded_categories': len(categories_to_encode),
                        'categories': categories_to_encode[:20]
                    }
                
                logger.info("One-hot encoding completed")
            
            elif method == 'label':
                logger.info("Label encoding (alphabetical order from train)")
                
                for col in categorical_cols:
                    train_categories = sorted(train_df[col].unique())
                    
                    label_map = {cat: idx for idx, cat in enumerate(train_categories)}
                    
                    def map_with_unknown(val):
                        return label_map.get(val, -1)
                    
                    df_encoded[col] = df_encoded[col].apply(map_with_unknown)
                    
                    unseen = (df_encoded[col] == -1).sum()
                    if unseen > 0:
                        logger.info("  " + col + ": " + str(unseen) + " unseen categories mapped to -1")
                    
                    report['encodings'][col] = {
                        'type': 'label',
                        'n_categories': len(label_map),
                        'mapping': {str(k): int(v) for k, v in list(label_map.items())[:20]},
                        'unseen_count': int(unseen)
                    }
                
                logger.info("Label encoding completed")
            
            elif method == 'frequency':
                logger.info("Frequency encoding (from train)")
                
                for col in categorical_cols:
                    train_freq = train_df[col].value_counts(normalize=True).to_dict()
                    
                    def map_frequency(val):
                        return train_freq.get(val, 0.0)
                    
                    df_encoded[col + '_freq'] = df_encoded[col].apply(map_frequency)
                    df_encoded = df_encoded.drop(columns=[col])
                    
                    unseen = (df_encoded[col + '_freq'] == 0.0).sum()
                    if unseen > 0:
                        logger.info("  " + col + ": " + str(unseen) + " unseen categories (freq=0)")
                    
                    report['encodings'][col] = {
                        'type': 'frequency',
                        'n_categories': len(train_freq),
                        'frequency_range': [float(min(train_freq.values())), float(max(train_freq.values()))],
                        'unseen_count': int(unseen)
                    }
                
                logger.info("Frequency encoding completed")
            
            elif method == 'target':
                if ground_truth_df is None or ground_truth_df.empty:
                    logger.error("ERROR: Target encoding requires ground_truth labels")
                    raise ValueError("ground_truth is required for target encoding")
                
                logger.info("Target encoding (mean from train)")
                
                target_col = ground_truth_df.columns[0]
                train_targets = ground_truth_df.iloc[train_indices][target_col]
                global_mean = train_targets.mean()
                
                logger.info("  Global mean: " + str(round(global_mean, 4)))
                
                for col in categorical_cols:
                    train_means = train_df.groupby(col)[target_col].mean().to_dict()
                    
                    def map_target(val):
                        return train_means.get(val, global_mean)
                    
                    df_encoded[col + '_target'] = df_encoded[col].apply(map_target)
                    df_encoded = df_encoded.drop(columns=[col])
                    
                    unseen = (df_encoded[col + '_target'] == global_mean).sum()
                    actual_unseen = unseen - train_df[col].value_counts().get(train_df[col].mode()[0], 0)
                    
                    if actual_unseen > 0:
                        logger.info("  " + col + ": " + str(int(actual_unseen)) + " unseen (using global mean)")
                    
                    report['encodings'][col] = {
                        'type': 'target',
                        'n_categories': len(train_means),
                        'global_mean': float(global_mean),
                        'target_range': [float(min(train_means.values())), float(max(train_means.values()))],
                        'unseen_count': int(actual_unseen)
                    }
                
                logger.info("Target encoding completed")
            
            else:
                raise ValueError(
                    "Unknown method: " + method + ". "
                    "Available: onehot, label, frequency, target, none"
                )
            
            logger.info("")
            logger.info("Encoding completed: " + str(df_encoded.shape))
            logger.info("")
            
            return df_encoded, report
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--encoding_method", default='onehot')
            parser.add_argument("--encoding_params", default='{}')
            parser.add_argument("--ground_truth", default='')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_encoding_report", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED CATEGORICAL ENCODING")
            logger.info("="*80)
            logger.info("Method: " + args.encoding_method)
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_encoding_report)
                
                params = json.loads(args.encoding_params)
                
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                ground_truth_df = load_data(args.ground_truth) if args.ground_truth else None
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                df_encoded, report = encode_categorical_unified(
                    df=df,
                    train_indices=train_indices,
                    method=args.encoding_method,
                    params=params,
                    ground_truth_df=ground_truth_df
                )
                
                df_encoded.to_csv(args.output_preprocessed_data, index=False, encoding='utf-8')
                logger.info("Encoded data saved")
                
                with open(args.output_encoding_report, 'w') as f:
                    json.dump(report, f, indent=2)
                logger.info("Report saved")
                
                logger.info("")
                logger.info("="*80)
                logger.info("CATEGORICAL ENCODING COMPLETED")
                logger.info("="*80)
                logger.info("Method: " + args.encoding_method)
                logger.info("Columns encoded: " + str(report['n_columns_encoded']))
                logger.info("Final shape: " + str(df_encoded.shape))
                logger.info("="*80)
                
            except Exception as e:
                logger.error("ERROR: " + str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --encoding_method
      - {inputValue: encoding_method}
      - --encoding_params
      - {inputValue: encoding_params}
      - --ground_truth
      - {inputPath: ground_truth}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_encoding_report
      - {outputPath: encoding_report}
