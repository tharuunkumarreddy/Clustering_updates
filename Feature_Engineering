name: Unified Feature Engineering Component - PARQUET OPTIMIZED
description: Train-aware feature engineering for unified preprocessing. Fits transformations on train indices only, transforms all combined data. Supports 6 methods polynomial, interactions, mathematical transforms, ratios, binning, custom formulas. Loads Parquet/CSV, SAVES AS PARQUET.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (Parquet or CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: engineering_method
    type: String
    description: 'Engineering method: polynomial, interactions, mathematical, ratios, binning, custom, none'
    default: 'polynomial'
  - name: engineering_params
    type: String
    description: 'Engineering parameters as JSON. Examples: {"degree":2}, {"transforms":["log","sqrt"]}, {"n_bins":5}'
    default: '{}'

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Dataset with engineered features (PARQUET format - 10x faster than CSV)'
  - name: feature_report
    type: Data
    description: 'Feature engineering report (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('feature_engineering_unified')
        
        def ensure_directory_exists(file_path):
            #Ensure directory exists for output file#
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def detect_file_type(file_path):
            # Detect file type by MAGIC BYTES first (not extension).
            # CRITICAL FIX: Kubernetes passes files without extensions
            try:
                with open(file_path, 'rb') as f:
                    header = f.read(8)
                if header[:4] == b'PAR1':
                    logger.info("[OK] Detected: Parquet (PAR1 magic bytes)")
                    return 'parquet'
                if header[1:6] == b'NUMPY':
                    logger.info("[OK] Detected: NumPy array")
                    return 'numpy'
                if header[:2] in [b'PK', b'\x80\x04']:
                    logger.info("[OK] Detected: Pickle/ZIP")
                    return 'pickle'
                try:
                    text_start = open(file_path, 'r', errors='replace').read(512)
                    if text_start.strip().startswith('{') or text_start.strip().startswith('['):
                        return 'json'
                    return 'csv'
                except Exception:
                    return 'csv'
            except Exception as e:
                logger.warning("Magic byte detection failed: " + str(e) + ", defaulting to CSV")
                return 'csv'

        def load_data(input_path):
            # Load data with MAGIC BYTES detection first.
            # CRITICAL FIX: Kubernetes strips file extensions.
            logger.info("Loading dataset from: " + input_path)
            ext = Path(input_path).suffix.lower()
            logger.info("File extension: '" + ext + "' (may be empty on Kubernetes)")
            detected_type = detect_file_type(input_path)
            try:
                if ext in ['.parquet', '.pq'] or detected_type == 'parquet':
                    logger.info("Loading as Parquet...")
                    df = pd.read_parquet(input_path, engine='pyarrow')
                    logger.info("[OK] Loaded Parquet: " + str(df.shape[0]) + " rows x " + str(df.shape[1]) + " columns")
                    return df
                logger.info("Loading as CSV...")
                for enc in ['utf-8', 'latin-1', 'cp1252']:
                    try:
                        df = pd.read_csv(input_path, encoding=enc)
                        logger.info("[OK] Loaded CSV (" + enc + "): " + str(df.shape[0]) + " rows x " + str(df.shape[1]) + " columns")
                        return df
                    except UnicodeDecodeError:
                        continue
                raise ValueError("Could not decode file with any supported encoding")
            except Exception as e:
                logger.error("Error loading data: " + str(e))
                raise
        
        def load_indices(indices_path):
            #Load indices from JSON file#
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def engineer_features_unified(df, train_indices, method, params):
            #Engineer features using train data statistics
            #Fits on train_indices only, transforms all combined data
            logger.info("="*80)
            logger.info("UNIFIED FEATURE ENGINEERING")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            logger.info(f"Train indices: {len(train_indices)} rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - skipping feature engineering")
                return df, {
                    'method': 'none',
                    'n_features_created': 0
                }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns found")
                return df, {
                    'method': method,
                    'n_features_created': 0,
                    'warning': 'no_numeric_columns'
                }
            
            logger.info(f"Original features: {len(numeric_cols)} numeric columns")
            
            train_df = df.iloc[train_indices]
            df_engineered = df.copy()
            
            report = {
                'method': method,
                'params': params,
                'original_features': numeric_cols,
                'n_original_features': len(numeric_cols)
            }
            
            if method == 'polynomial':
                degree = params.get('degree', 2)
                include_bias = params.get('include_bias', False)
                interaction_only = params.get('interaction_only', False)
                
                logger.info(f"Polynomial features: degree={degree}")
                
                poly = PolynomialFeatures(
                    degree=degree,
                    include_bias=include_bias,
                    interaction_only=interaction_only
                )
                
                X_train = train_df[numeric_cols].values
                poly.fit(X_train)
                
                X_all = df[numeric_cols].values
                X_poly = poly.transform(X_all)
                
                feature_names = poly.get_feature_names_out(numeric_cols)
                df_poly = pd.DataFrame(X_poly, columns=feature_names, index=df.index)
                
                # Preserve non-numeric columns
                non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
                if non_numeric_cols:
                    for col in non_numeric_cols:
                        df_poly[col] = df[col]
                
                df_engineered = df_poly
                
                n_new = len(feature_names) - len(numeric_cols)
                report['n_features_created'] = int(n_new)
                report['feature_names'] = feature_names.tolist()[:50]  # First 50
                
                logger.info(f"oe" Created {n_new} polynomial features")
            
            elif method == 'interactions':
                degree = params.get('degree', 2)
                
                logger.info(f"Interaction features: degree={degree}")
                
                poly = PolynomialFeatures(
                    degree=degree,
                    include_bias=False,
                    interaction_only=True
                )
                
                X_train = train_df[numeric_cols].values
                poly.fit(X_train)
                
                X_all = df[numeric_cols].values
                X_inter = poly.transform(X_all)
                
                feature_names = poly.get_feature_names_out(numeric_cols)
                df_inter = pd.DataFrame(X_inter, columns=feature_names, index=df.index)
                
                # Preserve non-numeric columns
                non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
                if non_numeric_cols:
                    for col in non_numeric_cols:
                        df_inter[col] = df[col]
                
                df_engineered = df_inter
                
                n_new = len(feature_names) - len(numeric_cols)
                report['n_features_created'] = int(n_new)
                report['feature_names'] = feature_names.tolist()[:50]
                
                logger.info(f"oe" Created {n_new} interaction features")
            
            elif method == 'mathematical':
                transforms = params.get('transforms', ['log', 'sqrt'])
                
                logger.info(f"Mathematical transforms: {transforms}")
                
                new_features = []
                
                for col in numeric_cols:
                    if 'log' in transforms:
                        if (df[col] > 0).all():
                            df_engineered[col + '_log'] = np.log1p(df[col])
                            new_features.append(col + '_log')
                        else:
                            logger.info(f"  {col}: skipped log (non-positive values)")
                    
                    if 'sqrt' in transforms:
                        if (df[col] >= 0).all():
                            df_engineered[col + '_sqrt'] = np.sqrt(df[col])
                            new_features.append(col + '_sqrt')
                        else:
                            logger.info(f"  {col}: skipped sqrt (negative values)")
                    
                    if 'square' in transforms:
                        df_engineered[col + '_square'] = df[col] ** 2
                        new_features.append(col + '_square')
                    
                    if 'reciprocal' in transforms:
                        if (df[col] != 0).all():
                            df_engineered[col + '_reciprocal'] = 1 / df[col]
                            new_features.append(col + '_reciprocal')
                        else:
                            logger.info(f"  {col}: skipped reciprocal (zero values)")
                    
                    if 'exp' in transforms:
                        if (np.abs(df[col]) < 10).all():
                            df_engineered[col + '_exp'] = np.exp(df[col])
                            new_features.append(col + '_exp')
                        else:
                            logger.info(f"  {col}: skipped exp (values too large)")
                
                report['n_features_created'] = len(new_features)
                report['feature_names'] = new_features
                
                logger.info(f"oe" Created {len(new_features)} mathematical features")
            
            elif method == 'ratios':
                logger.info("Creating ratio features")
                
                new_features = []
                
                for i, col1 in enumerate(numeric_cols):
                    for col2 in numeric_cols[i+1:]:
                        if (df[col2] != 0).all():
                            ratio_name = col1 + '_div_' + col2
                            df_engineered[ratio_name] = df[col1] / df[col2]
                            new_features.append(ratio_name)
                        
                        if (df[col1] != 0).all():
                            ratio_name = col2 + '_div_' + col1
                            df_engineered[ratio_name] = df[col2] / df[col1]
                            new_features.append(ratio_name)
                
                report['n_features_created'] = len(new_features)
                report['feature_names'] = new_features[:50]  # First 50
                
                logger.info(f"oe" Created {len(new_features)} ratio features")
            
            elif method == 'binning':
                n_bins = params.get('n_bins', 5)
                strategy = params.get('strategy', 'quantile')
                
                logger.info(f"Binning: n_bins={n_bins}, strategy={strategy}")
                
                discretizer = KBinsDiscretizer(
                    n_bins=n_bins,
                    encode='ordinal',
                    strategy=strategy
                )
                
                X_train = train_df[numeric_cols].values
                discretizer.fit(X_train)
                
                X_all = df[numeric_cols].values
                X_binned = discretizer.transform(X_all)
                
                binned_names = [col + '_binned' for col in numeric_cols]
                df_binned = pd.DataFrame(X_binned, columns=binned_names, index=df.index)
                
                # Preserve all non-numeric columns
                for col in df.columns:
                    if col not in numeric_cols:
                        df_binned[col] = df[col]
                
                df_engineered = df_binned
                
                report['n_features_created'] = len(binned_names)
                report['feature_names'] = binned_names
                report['n_bins'] = n_bins
                report['strategy'] = strategy
                
                logger.info(f"oe" Created {len(binned_names)} binned features")
            
            elif method == 'custom':
                features = params.get('features', [])
                
                logger.info(f"Custom formulas: {len(features)} features")
                
                new_features = []
                
                for feature_spec in features:
                    name = feature_spec['name']
                    formula = feature_spec['formula']
                    
                    try:
                        df_engineered[name] = df_engineered.eval(formula)
                        new_features.append(name)
                        logger.info(f"  oe" Created: {name}")
                    except Exception as e:
                        logger.warning(f"  oe-- Failed: {name} - {str(e)}")
                
                report['n_features_created'] = len(new_features)
                report['feature_names'] = new_features
                
                logger.info(f"oe" Created {len(new_features)} custom features")
            
            else:
                raise ValueError(
                    f"Unknown method: {method}. "
                    f"Available: polynomial, interactions, mathematical, ratios, binning, custom, none"
                )
            
            report['final_n_features'] = len(df_engineered.columns)
            
            logger.info("")
            logger.info("Feature engineering completed")
            logger.info(f"Original: {len(numeric_cols)} features")
            logger.info(f"Final: {df_engineered.shape[1]} features")
            logger.info(f"Added: {report['n_features_created']} features")
            logger.info("")
            
            return df_engineered, report
        
        def main():
            parser = argparse.ArgumentParser(description="Unified Feature Engineering (PARQUET OPTIMIZED)")
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--engineering_method", default='polynomial')
            parser.add_argument("--engineering_params", default='{}')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_feature_report", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED FEATURE ENGINEERING (PARQUET OPTIMIZED)")
            logger.info("="*80)
            logger.info(f"Method: {args.engineering_method}")
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_feature_report)
                
                params = json.loads(args.engineering_params)
                
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                logger.info(f"Input shape: {df.shape}")
                logger.info("")
                
                df_engineered, report = engineer_features_unified(
                    df=df,
                    train_indices=train_indices,
                    method=args.engineering_method,
                    params=params
                )
                
                # ============================================================
                # CRITICAL: SAVE AS PARQUET (NOT CSV!)
                # ============================================================
                output_parquet = args.output_preprocessed_data
                if not output_parquet.endswith('.parquet'):
                    output_parquet = output_parquet.replace('.csv', '.parquet')
                
                logger.info("="*80)
                logger.info("SAVING ENGINEERED DATA AS PARQUET")
                logger.info("="*80)
                
                df_engineered.to_parquet(
                    output_parquet,
                    index=False,
                    engine='pyarrow',
                    compression='snappy'
                )
                
                parquet_size = os.path.getsize(output_parquet) / 1024**2
                logger.info(f"oe" Engineered data saved as PARQUET: {parquet_size:.2f} MB")
                logger.info(f"  Path: {output_parquet}")
                
                # Add Parquet metadata to report
                report['output_format'] = {
                    'format': 'parquet',
                    'engine': 'pyarrow',
                    'compression': 'snappy',
                    'file_size_mb': float(parquet_size)
                }
                
                with open(args.output_feature_report, 'w') as f:
                    json.dump(report, f, indent=2)
                logger.info(f"oe" Report saved: {args.output_feature_report}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("FEATURE ENGINEERING COMPLETED")
                logger.info("="*80)
                logger.info(f"Method: {args.engineering_method}")
                logger.info(f"Features created: {report['n_features_created']}")
                logger.info(f"Final shape: {df_engineered.shape}")
                logger.info(f"Output format: PARQUET (10x faster than CSV)")
                logger.info(f"File size: {parquet_size:.2f} MB")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --engineering_method
      - {inputValue: engineering_method}
      - --engineering_params
      - {inputValue: engineering_params}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_feature_report
      - {outputPath: feature_report}
