name: Data Quality Validation Component - PARQUET OPTIMIZED (FIXED v2)
description: Validates data quality before preprocessing. Performs schema validation, missing value analysis, duplicate detection, outlier checks. Loads Parquet/CSV, SAVES AS PARQUET. FIXED v2 - Magic-bytes detection handles Kubernetes extensionless file paths.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset to validate (Parquet or CSV)'
  - name: quality_thresholds
    type: String
    description: 'Quality thresholds as JSON. Example: {"max_missing_pct":50,"min_samples":10,"max_duplicates_pct":20}'
    default: "{}"

outputs:
  - name: validated_data
    type: Data
    description: 'Cleaned dataset with severe quality issues removed (PARQUET format - 10x faster than CSV)'
  - name: quality_report
    type: Data
    description: 'Comprehensive quality validation report with score breakdown (JSON)'
  - name: alerts
    type: Data
    description: 'Critical quality alerts and recommendations (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from pathlib import Path
        from scipy import stats
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('data_quality_validator')
        
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info(f"Created directory: {directory}")
        
        
        def detect_file_type(file_path):
            #Detect file type by MAGIC BYTES first (not extension).
            #CRITICAL FIX: Kubernetes passes files without extensions
            #e.g. /tmp/inputs/input_data/data (no .parquet, no .csv)
            #Magic bytes are reliable regardless of filename.
            try:
                with open(file_path, 'rb') as f:
                    header = f.read(8)
                
                # Parquet magic: PAR1 at start (bytes 0-3)
                if header[:4] == b'PAR1':
                    logger.info("✓ Detected: Parquet (PAR1 magic bytes)")
                    return 'parquet'
                
                # Parquet footer magic: PAR1 sometimes at end (older writers)
                # Also check byte 4 since some implementations vary
                if b'PAR1' in header:
                    logger.info("✓ Detected: Parquet (PAR1 in header)")
                    return 'parquet'
                
                # Pickle protocol marker (0x80 followed by protocol byte)
                if header[0:1] == b'\x80':
                    logger.info("✓ Detected: Pickle binary")
                    return 'pickle'
                
                # NumPy .npy magic: \x93NUMPY
                if header[:6] == b'\x93NUMPY':
                    logger.info("✓ Detected: NumPy array")
                    return 'numpy'
                
                # Text-based: try UTF-8 read to distinguish JSON vs CSV
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='strict') as f:
                        text_start = f.read(512).strip()
                    if text_start.startswith('{') or text_start.startswith('['):
                        logger.info("✓ Detected: JSON (UTF-8)")
                        return 'json'
                    logger.info("✓ Detected: CSV (UTF-8 text)")
                    return 'csv'
                except UnicodeDecodeError:
                    pass
                
                # Binary file that is NOT parquet/pickle/numpy - unknown
                logger.warning("⚠ Could not detect type by magic bytes, trying Parquet as fallback")
                return 'parquet_fallback'
                
            except Exception as e:
                logger.warning(f"Magic byte detection failed: {e}, defaulting to CSV")
                return 'csv'
        
        
        def load_data(input_path):
            #Load data with MAGIC BYTES detection first.
            #CRITICAL FIX: Kubernetes strips file extensions.
            #Path like /tmp/inputs/input_data/data has NO extension,
            #so extension-based detection silently falls through to pd.read_csv()
            #which then fails with UnicodeDecodeError on binary Parquet files.
            #Solution: detect format by file content (magic bytes), not filename.
            logger.info(f"Loading dataset from: {input_path}")
            
            # Step 1: Check extension as a hint (may be empty on Kubernetes)
            ext = Path(input_path).suffix.lower()
            logger.info(f"File extension: '{ext}' (may be empty on Kubernetes)")
            
            # Step 2: Always verify with magic bytes (authoritative)
            detected_type = detect_file_type(input_path)
            
            try:
                # Load Parquet (extension match OR magic bytes match)
                if ext in ['.parquet', '.pq'] or detected_type in ['parquet', 'parquet_fallback']:
                    logger.info("Loading as Parquet...")
                    df = pd.read_parquet(input_path, engine='pyarrow')
                    logger.info(f"✓ Loaded Parquet: {df.shape[0]} rows x {df.shape[1]} columns")
                    return df
                
                # Load CSV with encoding fallbacks
                logger.info("Loading as CSV...")
                for encoding in ['utf-8', 'latin-1', 'cp1252', 'ISO-8859-1']:
                    try:
                        df = pd.read_csv(input_path, encoding=encoding)
                        logger.info(f"✓ Loaded CSV ({encoding}): {df.shape[0]} rows x {df.shape[1]} columns")
                        return df
                    except UnicodeDecodeError:
                        logger.warning(f"  {encoding} failed, trying next encoding...")
                        continue
                
                raise ValueError(f"Could not load file with any known encoding: {input_path}")
                
            except Exception as e:
                logger.error(f"Error loading data: {str(e)}")
                raise
        
        
        def validate_schema(df, thresholds):
            #Validate basic schema and structure#
            logger.info("="*80)
            logger.info("SCHEMA VALIDATION")
            logger.info("="*80)
            
            validation = {
                'is_valid': True,
                'errors': [],
                'warnings': [],
                'checks': {}
            }
            
            min_samples = thresholds.get('min_samples', 10)
            min_features = thresholds.get('min_features', 2)
            
            # Check 1: Non-empty dataset
            if df.empty:
                validation['is_valid'] = False
                validation['errors'].append("Dataset is empty")
                logger.error("✗ Dataset is empty")
                return validation
            
            logger.info(f"✓ Dataset is not empty: {len(df)} rows")
            
            # Check 2: Minimum samples
            if len(df) < min_samples:
                validation['is_valid'] = False
                validation['errors'].append(
                    f"Insufficient samples: {len(df)} < {min_samples} required"
                )
                logger.error(f"✗ Insufficient samples: {len(df)} < {min_samples}")
            else:
                logger.info(f"✓ Sufficient samples: {len(df)} >= {min_samples}")
            
            validation['checks']['n_samples'] = {
                'value': int(len(df)),
                'threshold': int(min_samples),
                'passed': len(df) >= min_samples
            }
            
            # Check 3: Minimum features
            if df.shape[1] < min_features:
                validation['is_valid'] = False
                validation['errors'].append(
                    f"Insufficient features: {df.shape[1]} < {min_features} required"
                )
                logger.error(f"✗ Insufficient features: {df.shape[1]} < {min_features}")
            else:
                logger.info(f"✓ Sufficient features: {df.shape[1]} >= {min_features}")
            
            validation['checks']['n_features'] = {
                'value': int(df.shape[1]),
                'threshold': int(min_features),
                'passed': df.shape[1] >= min_features
            }
            
            # Check 4: Duplicate column names
            if df.columns.duplicated().any():
                dup_cols = df.columns[df.columns.duplicated()].tolist()
                validation['warnings'].append(f"Duplicate column names: {dup_cols}")
                logger.warning(f"⚠ Duplicate column names: {dup_cols}")
            else:
                logger.info("✓ No duplicate column names")
            
            validation['checks']['duplicate_columns'] = {
                'count': int(df.columns.duplicated().sum()),
                'passed': not df.columns.duplicated().any()
            }
            
            logger.info("")
            return validation
        
        
        def check_completeness(df, thresholds):
            #Check data completeness (missing values)#
            logger.info("="*80)
            logger.info("COMPLETENESS CHECK")
            logger.info("="*80)
            
            max_missing_pct = thresholds.get('max_missing_pct', 50)
            
            checks = {
                'is_valid': True,
                'errors': [],
                'warnings': [],
                'details': {}
            }
            
            total_missing = df.isnull().sum().sum()
            total_values = df.shape[0] * df.shape[1]
            missing_pct = (total_missing / total_values * 100) if total_values > 0 else 0
            
            logger.info(f"Total missing values: {total_missing} ({missing_pct:.2f}%)")
            
            checks['details']['total_missing'] = int(total_missing)
            checks['details']['missing_percentage'] = float(missing_pct)
            checks['details']['threshold'] = float(max_missing_pct)
            
            if missing_pct > max_missing_pct:
                checks['errors'].append(
                    f"Excessive missing values: {missing_pct:.1f}% > {max_missing_pct}%"
                )
                logger.error(f"✗ Excessive missing: {missing_pct:.1f}% > {max_missing_pct}%")
                checks['is_valid'] = False
            elif missing_pct > max_missing_pct * 0.7:
                checks['warnings'].append(
                    f"High missing values: {missing_pct:.1f}% (threshold: {max_missing_pct}%)"
                )
                logger.warning(f"⚠ High missing: {missing_pct:.1f}%")
            else:
                logger.info(f"✓ Acceptable missing: {missing_pct:.2f}%")
            
            completely_empty_cols = []
            high_missing_cols = []
            
            for col in df.columns:
                miss_count = df[col].isnull().sum()
                miss_pct = (miss_count / len(df) * 100) if len(df) > 0 else 0
                
                if miss_count == len(df):
                    completely_empty_cols.append(col)
                elif miss_pct > max_missing_pct:
                    high_missing_cols.append((col, miss_pct))
            
            if completely_empty_cols:
                checks['warnings'].append(f"Completely empty columns: {completely_empty_cols}")
                logger.warning(f"⚠ Empty columns: {completely_empty_cols}")
            
            if high_missing_cols:
                checks['warnings'].append(
                    f"Columns with >{max_missing_pct}% missing: {len(high_missing_cols)}"
                )
                for col, pct in high_missing_cols[:5]:
                    logger.warning(f"  - {col}: {pct:.1f}% missing")
            
            checks['details']['completely_empty_columns'] = completely_empty_cols
            checks['details']['high_missing_columns'] = len(high_missing_cols)
            
            logger.info("")
            return checks
        
        
        def check_uniqueness(df, thresholds):
            #Check for duplicates and uniqueness issues#
            logger.info("="*80)
            logger.info("UNIQUENESS CHECK")
            logger.info("="*80)
            
            max_duplicates_pct = thresholds.get('max_duplicates_pct', 20)
            
            checks = {
                'is_valid': True,
                'errors': [],
                'warnings': [],
                'details': {}
            }
            
            n_duplicates = df.duplicated().sum()
            dup_pct = (n_duplicates / len(df) * 100) if len(df) > 0 else 0
            
            logger.info(f"Duplicate rows: {n_duplicates} ({dup_pct:.2f}%)")
            
            checks['details']['duplicate_rows'] = int(n_duplicates)
            checks['details']['duplicate_percentage'] = float(dup_pct)
            checks['details']['threshold'] = float(max_duplicates_pct)
            
            if dup_pct > max_duplicates_pct:
                checks['warnings'].append(
                    f"High duplicate rate: {dup_pct:.1f}% > {max_duplicates_pct}%"
                )
                logger.warning(f"⚠ High duplicates: {dup_pct:.1f}%")
            else:
                logger.info(f"✓ Acceptable duplicates: {dup_pct:.2f}%")
            
            constant_cols = []
            for col in df.columns:
                if df[col].nunique(dropna=True) <= 1:
                    constant_cols.append(col)
            
            if constant_cols:
                checks['warnings'].append(
                    f"Constant columns (no variance): {len(constant_cols)} columns"
                )
                logger.warning(f"⚠ Constant columns: {len(constant_cols)}")
                for col in constant_cols[:5]:
                    logger.warning(f"  - {col}")
            else:
                logger.info("✓ No constant columns")
            
            checks['details']['constant_columns'] = constant_cols
            
            high_cardinality = []
            cat_cols = df.select_dtypes(exclude=[np.number]).columns
            
            for col in cat_cols:
                unique_count = df[col].nunique()
                cardinality_ratio = unique_count / len(df)
                if cardinality_ratio > 0.5:
                    high_cardinality.append((col, unique_count))
            
            if high_cardinality:
                checks['warnings'].append(
                    f"High cardinality categorical: {len(high_cardinality)} columns"
                )
                for col, count in high_cardinality[:3]:
                    logger.warning(f"  - {col}: {count} unique values")
            
            checks['details']['high_cardinality_columns'] = len(high_cardinality)
            
            logger.info("")
            return checks
        
        
        def check_distribution(df):
            #Check data distributions and outliers#
            logger.info("="*80)
            logger.info("DISTRIBUTION CHECK")
            logger.info("="*80)
            
            checks = {
                'is_valid': True,
                'warnings': [],
                'details': {}
            }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) == 0:
                logger.info("No numeric columns to analyze")
                return checks
            
            outlier_cols = []
            for col in numeric_cols:
                try:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    outliers = ((df[col] < (Q1 - 3 * IQR)) | (df[col] > (Q3 + 3 * IQR))).sum()
                    outlier_pct = (outliers / len(df) * 100) if len(df) > 0 else 0
                    if outlier_pct > 10:
                        outlier_cols.append((col, outlier_pct))
                except:
                    pass
            
            if outlier_cols:
                checks['warnings'].append(f"Columns with >10% outliers: {len(outlier_cols)}")
                for col, pct in outlier_cols[:5]:
                    logger.warning(f"  - {col}: {pct:.1f}% outliers")
            else:
                logger.info("✓ No excessive outliers detected")
            
            checks['details']['outlier_columns'] = len(outlier_cols)
            
            inf_cols = []
            for col in numeric_cols:
                inf_count = df[col].isin([np.inf, -np.inf]).sum()
                if inf_count > 0:
                    inf_cols.append((col, inf_count))
            
            if inf_cols:
                checks['warnings'].append(f"Infinite values found in {len(inf_cols)} columns")
                for col, count in inf_cols:
                    logger.warning(f"  - {col}: {count} infinite values")
            else:
                logger.info("✓ No infinite values")
            
            checks['details']['infinite_value_columns'] = len(inf_cols)
            
            logger.info("")
            return checks
        
        
        def clean_severe_issues(df):
            #Remove completely empty rows/columns#
            logger.info("="*80)
            logger.info("CLEANING SEVERE ISSUES")
            logger.info("="*80)
            
            initial_shape = df.shape
            cleaning_actions = []
            
            empty_cols = df.columns[df.isnull().all()].tolist()
            if empty_cols:
                df = df.drop(columns=empty_cols)
                msg = f"Removed {len(empty_cols)} completely empty columns"
                cleaning_actions.append(msg)
                logger.info(f"✓ {msg}")
            
            empty_rows = df.isnull().all(axis=1).sum()
            if empty_rows > 0:
                df = df[~df.isnull().all(axis=1)]
                msg = f"Removed {empty_rows} completely empty rows"
                cleaning_actions.append(msg)
                logger.info(f"✓ {msg}")
            
            if not cleaning_actions:
                logger.info("✓ No severe issues to clean")
            
            final_shape = df.shape
            logger.info(f"Shape change: {initial_shape} -> {final_shape}")
            logger.info("")
            
            return df, cleaning_actions, initial_shape, final_shape
        
        
        def calculate_quality_score_with_breakdown(validations):
            #Calculate overall quality score (0-100) with detailed breakdown.
            #FIXED: Capped penalties prevent negative scores for high-dimensional data.
            breakdown = {
                'starting_score': 100.0,
                'penalties': {},
                'total_penalty': 0.0
            }
            
            score = 100.0
            
            # 1. Schema errors (-20 per error, uncapped - critical)
            schema_errors = len(validations['schema']['errors'])
            schema_penalty = schema_errors * 20
            score -= schema_penalty
            breakdown['penalties']['schema_errors'] = {
                'count': schema_errors,
                'penalty': -schema_penalty,
                'note': 'Critical structural issues'
            }
            
            # 2. Missing values (max -30)
            missing_pct = validations['completeness']['details'].get('missing_percentage', 0)
            missing_penalty = min(missing_pct / 2, 30) if missing_pct > 0 else 0
            score -= missing_penalty
            breakdown['penalties']['missing_values'] = {
                'percentage': round(missing_pct, 2),
                'penalty': -round(missing_penalty, 2),
                'max_penalty': -30,
                'note': 'Sparse data expected in some domains'
            }
            
            # 3. Duplicate rows (max -15)
            dup_pct = validations['uniqueness']['details'].get('duplicate_percentage', 0)
            dup_penalty = min(dup_pct / 2, 15)
            score -= dup_penalty
            breakdown['penalties']['duplicate_rows'] = {
                'percentage': round(dup_pct, 2),
                'penalty': -round(dup_penalty, 2),
                'max_penalty': -15,
                'note': 'Some duplication acceptable'
            }
            
            # 4. Constant columns (max -20) - FIXED: capped to prevent score destruction
            const_cols = len(validations['uniqueness']['details'].get('constant_columns', []))
            const_penalty = min(const_cols * 2, 20)
            score -= const_penalty
            breakdown['penalties']['constant_columns'] = {
                'count': const_cols,
                'penalty': -round(const_penalty, 2),
                'max_penalty': -20,
                'note': 'Common in high-dimensional data'
            }
            
            # 5. High cardinality (max -10)
            high_card = validations['uniqueness']['details'].get('high_cardinality_columns', 0)
            card_penalty = min(high_card * 1, 10)
            score -= card_penalty
            breakdown['penalties']['high_cardinality'] = {
                'count': high_card,
                'penalty': -round(card_penalty, 2),
                'max_penalty': -10,
                'note': 'Increases encoding complexity'
            }
            
            # 6. Outlier columns (max -10)
            outlier_cols = validations['distribution']['details'].get('outlier_columns', 0)
            outlier_penalty = min(outlier_cols * 2, 10)
            score -= outlier_penalty
            breakdown['penalties']['outlier_columns'] = {
                'count': outlier_cols,
                'penalty': -round(outlier_penalty, 2),
                'max_penalty': -10,
                'note': 'Outliers often meaningful in real data'
            }
            
            # 7. Infinite values (max -15)
            inf_cols = validations['distribution']['details'].get('infinite_value_columns', 0)
            inf_penalty = min(inf_cols * 3, 15)
            score -= inf_penalty
            breakdown['penalties']['infinite_values'] = {
                'count': inf_cols,
                'penalty': -round(inf_penalty, 2),
                'max_penalty': -15,
                'note': 'Serious data quality issue'
            }
            
            total_penalty = sum(p['penalty'] for p in breakdown['penalties'].values())
            breakdown['total_penalty'] = round(total_penalty, 2)
            
            final_score = max(0, min(100, score))
            breakdown['final_score'] = round(final_score, 2)
            
            if final_score >= 90:
                breakdown['interpretation'] = 'Excellent - Minimal preprocessing needed'
            elif final_score >= 70:
                breakdown['interpretation'] = 'Good - Standard preprocessing sufficient'
            elif final_score >= 50:
                breakdown['interpretation'] = 'Moderate - Careful preprocessing needed'
            elif final_score >= 30:
                breakdown['interpretation'] = 'Poor - Extensive cleaning required'
            else:
                breakdown['interpretation'] = 'Critical - Review data collection'
            
            return final_score, breakdown
        
        
        def generate_recommendations(validations, quality_score):
            #Generate actionable recommendations#
            recommendations = []
            
            if quality_score < 50:
                recommendations.append(
                    "CRITICAL: Quality score is very low. Consider data collection review."
                )
            
            missing_pct = validations['completeness']['details'].get('missing_percentage', 0)
            if missing_pct > 30:
                recommendations.append(
                    f"High missing values ({missing_pct:.1f}%). "
                    "Consider using advanced imputation (MICE or Iterative)."
                )
            elif missing_pct > 10:
                recommendations.append(
                    f"Moderate missing values ({missing_pct:.1f}%). "
                    "Use KNN or mean/median imputation."
                )
            
            dup_pct = validations['uniqueness']['details'].get('duplicate_percentage', 0)
            if dup_pct > 10:
                recommendations.append(
                    f"High duplicate rate ({dup_pct:.1f}%). "
                    "Review data collection for duplicates."
                )
            
            const_cols = validations['uniqueness']['details'].get('constant_columns', [])
            if len(const_cols) > 10:
                recommendations.append(
                    f"Remove {len(const_cols)} constant columns (showing first 10): "
                    f"{const_cols[:10]}"
                )
            elif const_cols:
                recommendations.append(
                    f"Remove {len(const_cols)} constant columns: {const_cols}"
                )
            
            outlier_cols = validations['distribution']['details'].get('outlier_columns', 0)
            if outlier_cols > 5:
                recommendations.append(
                    f"{outlier_cols} columns have excessive outliers. "
                    "Consider robust scaling or outlier handling."
                )
            
            inf_cols = validations['distribution']['details'].get('infinite_value_columns', 0)
            if inf_cols > 0:
                recommendations.append(
                    f"{inf_cols} columns contain infinite values. "
                    "Use outlier handling with method='clip' or 'remove'."
                )
            
            if validations['schema']['checks'].get('n_features', {}).get('value', 0) > 0:
                recommendations.append(
                    "Ensure categorical columns are encoded before clustering."
                )
            
            return recommendations
        
        
        def main():
            parser = argparse.ArgumentParser(
                description="Data Quality Validation Component (PARQUET OPTIMIZED - FIXED v2)"
            )
            parser.add_argument("--input_data", required=True)
            parser.add_argument("--quality_thresholds", default='{}')
            parser.add_argument("--output_validated_data", required=True)
            parser.add_argument("--output_quality_report", required=True)
            parser.add_argument("--output_alerts", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("DATA QUALITY VALIDATION (PARQUET OPTIMIZED - FIXED v2)")
            logger.info("="*80)
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_validated_data)
                ensure_directory_exists(args.output_quality_report)
                ensure_directory_exists(args.output_alerts)
                
                try:
                    if args.quality_thresholds and args.quality_thresholds != '{}':
                        thresholds = json.loads(args.quality_thresholds)
                    else:
                        thresholds = {}
                    
                    thresholds.setdefault('max_missing_pct', 50)
                    thresholds.setdefault('min_samples', 10)
                    thresholds.setdefault('max_duplicates_pct', 20)
                    thresholds.setdefault('min_features', 2)
                    
                    logger.info(f"Using thresholds: {thresholds}")
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON in quality_thresholds: {e}")
                    logger.info("Using default thresholds")
                    thresholds = {
                        'max_missing_pct': 50,
                        'min_samples': 10,
                        'max_duplicates_pct': 20,
                        'min_features': 2
                    }
                
                logger.info("")
                
                # Load data (magic-bytes detection handles extensionless Kubernetes paths)
                df = load_data(args.input_data)
                
                validations = {
                    'schema': validate_schema(df, thresholds),
                    'completeness': check_completeness(df, thresholds),
                    'uniqueness': check_uniqueness(df, thresholds),
                    'distribution': check_distribution(df)
                }
                
                df_cleaned, cleaning_actions, initial_shape, final_shape = \
                    clean_severe_issues(df)
                
                quality_score, score_breakdown = calculate_quality_score_with_breakdown(validations)
                recommendations = generate_recommendations(validations, quality_score)
                
                overall_valid = validations['schema']['is_valid'] and \
                               validations['completeness']['is_valid']
                
                quality_report = {
                    'overall_valid': overall_valid,
                    'quality_score': float(quality_score),
                    'score_breakdown': score_breakdown,
                    'validations': validations,
                    'cleaning_actions': cleaning_actions,
                    'shape_change': {
                        'initial': list(initial_shape),
                        'final': list(final_shape)
                    },
                    'recommendations': recommendations,
                    'thresholds': thresholds
                }
                
                all_errors = []
                all_warnings = []
                for category, results in validations.items():
                    all_errors.extend(results.get('errors', []))
                    all_warnings.extend(results.get('warnings', []))
                
                alerts = {
                    'critical_errors': all_errors,
                    'warnings': all_warnings,
                    'quality_score': float(quality_score),
                    'score_breakdown': score_breakdown,
                    'recommendations': recommendations
                }
                
                # SAVE AS PARQUET
                output_parquet = args.output_validated_data
                if not output_parquet.endswith('.parquet'):
                    output_parquet = output_parquet.replace('.csv', '.parquet')
                
                logger.info("="*80)
                logger.info("SAVING VALIDATED DATA AS PARQUET")
                logger.info("="*80)
                
                df_cleaned.to_parquet(
                    output_parquet,
                    index=False,
                    engine='pyarrow',
                    compression='snappy'
                )
                
                parquet_size = os.path.getsize(output_parquet) / 1024**2
                logger.info(f"✓ Saved PARQUET: {parquet_size:.2f} MB")
                
                quality_report['output_format'] = {
                    'format': 'parquet',
                    'engine': 'pyarrow',
                    'compression': 'snappy',
                    'file_size_mb': float(parquet_size)
                }
                
                with open(args.output_quality_report, 'w') as f:
                    json.dump(quality_report, f, indent=2)
                logger.info(f"✓ Quality report saved: {args.output_quality_report}")
                
                with open(args.output_alerts, 'w') as f:
                    json.dump(alerts, f, indent=2)
                logger.info(f"✓ Alerts saved: {args.output_alerts}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("VALIDATION SUMMARY")
                logger.info("="*80)
                logger.info(f"Overall Valid: {overall_valid}")
                logger.info(f"Quality Score: {quality_score:.1f}/100")
                logger.info(f"  Interpretation: {score_breakdown['interpretation']}")
                logger.info(f"  Total Penalty: {score_breakdown['total_penalty']}")
                logger.info(f"Critical Errors: {len(all_errors)}")
                logger.info(f"Warnings: {len(all_warnings)}")
                logger.info(f"Recommendations: {len(recommendations)}")
                logger.info(f"Output format: PARQUET (10x faster than CSV)")
                logger.info("")
                logger.info("Penalty Breakdown:")
                for category, details in score_breakdown['penalties'].items():
                    logger.info(f"  {category}: {details['penalty']} points")
                
                if quality_score < 50:
                    logger.warning("⚠ LOW QUALITY SCORE - Review data carefully!")
                elif quality_score < 70:
                    logger.warning("⚠ MODERATE QUALITY - Some improvements needed")
                else:
                    logger.info("✓ GOOD QUALITY")
                
                logger.info("="*80)
                
                if not overall_valid:
                    logger.error("CRITICAL: Validation failed")
                    sys.exit(1)
                
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --quality_thresholds
      - {inputValue: quality_thresholds}
      - --output_validated_data
      - {outputPath: validated_data}
      - --output_quality_report
      - {outputPath: quality_report}
      - --output_alerts
      - {outputPath: alerts}
