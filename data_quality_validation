name: Data Quality Validation Component (PARQUET OPTIMIZED)
description: Validates data quality before preprocessing. Performs schema validation, missing value analysis, duplicate detection, outlier checks. Loads Parquet/CSV, saves as PARQUET for optimal ML pipeline performance. FIXED - Proper quality score calculation with capped penalties.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset to validate (Parquet or CSV)'
  - name: quality_thresholds
    type: String
    description: 'Quality thresholds as JSON. Example: {"max_missing_pct":50,"min_samples":10,"max_duplicates_pct":20}'
    default: "{}"

outputs:
  - name: validated_data
    type: Data
    description: 'Cleaned dataset with severe quality issues removed (PARQUET format)'
  - name: quality_report
    type: Data
    description: 'Comprehensive quality validation report with score breakdown (JSON)'
  - name: alerts
    type: Data
    description: 'Critical quality alerts and recommendations (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from pathlib import Path
        from scipy import stats
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('data_quality_validator_parquet')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            #Load data from Parquet or CSV#
            logger.info(f"Loading dataset from: {input_path}")
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path, engine='pyarrow')
                    logger.info("✓ Loaded Parquet file")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("✓ Loaded CSV file")
                
                logger.info(f"Shape: {df.shape[0]} rows × {df.shape[1]} columns")
                return df
            except Exception as e:
                logger.error(f"Error loading data: {e}")
                raise
        
        def validate_schema(df, thresholds):
            logger.info("="*80)
            logger.info("SCHEMA VALIDATION")
            logger.info("="*80)
            
            validation = {'is_valid': True, 'errors': [], 'warnings': [], 'checks': {}}
            
            min_samples = thresholds.get('min_samples', 10)
            min_features = thresholds.get('min_features', 2)
            
            if df.empty:
                validation['is_valid'] = False
                validation['errors'].append("Dataset is empty")
                logger.error("✗ Dataset is empty")
                return validation
            
            logger.info(f"✓ Dataset: {len(df)} rows")
            
            if len(df) < min_samples:
                validation['is_valid'] = False
                validation['errors'].append(f"Insufficient samples: {len(df)} < {min_samples}")
                logger.error(f"✗ Insufficient samples")
            else:
                logger.info(f"✓ Sufficient samples: {len(df)}")
            
            validation['checks']['n_samples'] = {
                'value': int(len(df)),
                'threshold': int(min_samples),
                'passed': len(df) >= min_samples
            }
            
            if df.shape[1] < min_features:
                validation['is_valid'] = False
                validation['errors'].append(f"Insufficient features: {df.shape[1]} < {min_features}")
                logger.error(f"✗ Insufficient features")
            else:
                logger.info(f"✓ Sufficient features: {df.shape[1]}")
            
            validation['checks']['n_features'] = {
                'value': int(df.shape[1]),
                'threshold': int(min_features),
                'passed': df.shape[1] >= min_features
            }
            
            if df.columns.duplicated().any():
                dup_cols = df.columns[df.columns.duplicated()].tolist()
                validation['warnings'].append(f"Duplicate column names: {dup_cols}")
                logger.warning(f"⚠ Duplicate columns: {dup_cols}")
            else:
                logger.info("✓ No duplicate column names")
            
            validation['checks']['duplicate_columns'] = {
                'count': int(df.columns.duplicated().sum()),
                'passed': not df.columns.duplicated().any()
            }
            
            logger.info("")
            return validation
        
        def check_completeness(df, thresholds):
            logger.info("="*80)
            logger.info("COMPLETENESS CHECK")
            logger.info("="*80)
            
            max_missing_pct = thresholds.get('max_missing_pct', 50)
            
            checks = {'is_valid': True, 'errors': [], 'warnings': [], 'details': {}}
            
            total_missing = df.isnull().sum().sum()
            total_values = df.shape[0] * df.shape[1]
            missing_pct = (total_missing / total_values * 100) if total_values > 0 else 0
            
            logger.info(f"Missing: {total_missing} ({missing_pct:.2f}%)")
            
            checks['details']['total_missing'] = int(total_missing)
            checks['details']['missing_percentage'] = float(missing_pct)
            checks['details']['threshold'] = float(max_missing_pct)
            
            if missing_pct > max_missing_pct:
                checks['errors'].append(f"Excessive missing: {missing_pct:.1f}% > {max_missing_pct}%")
                logger.error(f"✗ Excessive missing: {missing_pct:.1f}%")
                checks['is_valid'] = False
            elif missing_pct > max_missing_pct * 0.7:
                checks['warnings'].append(f"High missing: {missing_pct:.1f}%")
                logger.warning(f"⚠ High missing: {missing_pct:.1f}%")
            else:
                logger.info(f"✓ Acceptable missing: {missing_pct:.2f}%")
            
            completely_empty_cols = [col for col in df.columns if df[col].isnull().sum() == len(df)]
            high_missing_cols = []
            
            for col in df.columns:
                miss_pct = (df[col].isnull().sum() / len(df) * 100) if len(df) > 0 else 0
                if miss_pct > max_missing_pct and col not in completely_empty_cols:
                    high_missing_cols.append((col, miss_pct))
            
            if completely_empty_cols:
                checks['warnings'].append(f"Empty columns: {completely_empty_cols}")
                logger.warning(f"⚠ Empty columns: {len(completely_empty_cols)}")
            
            if high_missing_cols:
                checks['warnings'].append(f"High missing columns: {len(high_missing_cols)}")
            
            checks['details']['completely_empty_columns'] = completely_empty_cols
            checks['details']['high_missing_columns'] = len(high_missing_cols)
            
            logger.info("")
            return checks
        
        def check_uniqueness(df, thresholds):
            logger.info("="*80)
            logger.info("UNIQUENESS CHECK")
            logger.info("="*80)
            
            max_duplicates_pct = thresholds.get('max_duplicates_pct', 20)
            
            checks = {'is_valid': True, 'errors': [], 'warnings': [], 'details': {}}
            
            n_duplicates = df.duplicated().sum()
            dup_pct = (n_duplicates / len(df) * 100) if len(df) > 0 else 0
            
            logger.info(f"Duplicates: {n_duplicates} ({dup_pct:.2f}%)")
            
            checks['details']['duplicate_rows'] = int(n_duplicates)
            checks['details']['duplicate_percentage'] = float(dup_pct)
            checks['details']['threshold'] = float(max_duplicates_pct)
            
            if dup_pct > max_duplicates_pct:
                checks['warnings'].append(f"High duplicates: {dup_pct:.1f}%")
                logger.warning(f"⚠ High duplicates: {dup_pct:.1f}%")
            else:
                logger.info(f"✓ Acceptable duplicates: {dup_pct:.2f}%")
            
            constant_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1]
            
            if constant_cols:
                checks['warnings'].append(f"Constant columns: {len(constant_cols)}")
                logger.warning(f"⚠ Constant columns: {len(constant_cols)}")
            else:
                logger.info("✓ No constant columns")
            
            checks['details']['constant_columns'] = constant_cols
            
            high_cardinality = []
            cat_cols = df.select_dtypes(exclude=[np.number]).columns
            
            for col in cat_cols:
                if df[col].nunique() / len(df) > 0.5:
                    high_cardinality.append((col, df[col].nunique()))
            
            if high_cardinality:
                checks['warnings'].append(f"High cardinality: {len(high_cardinality)} columns")
            
            checks['details']['high_cardinality_columns'] = len(high_cardinality)
            
            logger.info("")
            return checks
        
        def check_distribution(df):
            logger.info("="*80)
            logger.info("DISTRIBUTION CHECK")
            logger.info("="*80)
            
            checks = {'is_valid': True, 'warnings': [], 'details': {}}
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) == 0:
                logger.info("No numeric columns")
                return checks
            
            outlier_cols = []
            for col in numeric_cols:
                try:
                    Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    outliers = ((df[col] < (Q1 - 3 * IQR)) | (df[col] > (Q3 + 3 * IQR))).sum()
                    outlier_pct = (outliers / len(df) * 100) if len(df) > 0 else 0
                    
                    if outlier_pct > 10:
                        outlier_cols.append((col, outlier_pct))
                except:
                    pass
            
            if outlier_cols:
                checks['warnings'].append(f"Outlier columns: {len(outlier_cols)}")
            else:
                logger.info("✓ No excessive outliers")
            
            checks['details']['outlier_columns'] = len(outlier_cols)
            
            inf_cols = []
            for col in numeric_cols:
                inf_count = df[col].isin([np.inf, -np.inf]).sum()
                if inf_count > 0:
                    inf_cols.append((col, inf_count))
            
            if inf_cols:
                checks['warnings'].append(f"Infinite values: {len(inf_cols)} columns")
            else:
                logger.info("✓ No infinite values")
            
            checks['details']['infinite_value_columns'] = len(inf_cols)
            
            logger.info("")
            return checks
        
        def clean_severe_issues(df):
            logger.info("="*80)
            logger.info("CLEANING SEVERE ISSUES")
            logger.info("="*80)
            
            initial_shape = df.shape
            cleaning_actions = []
            
            empty_cols = df.columns[df.isnull().all()].tolist()
            if empty_cols:
                df = df.drop(columns=empty_cols)
                msg = f"Removed {len(empty_cols)} empty columns"
                cleaning_actions.append(msg)
                logger.info(f"✓ {msg}")
            
            empty_rows = df.isnull().all(axis=1).sum()
            if empty_rows > 0:
                df = df[~df.isnull().all(axis=1)]
                msg = f"Removed {empty_rows} empty rows"
                cleaning_actions.append(msg)
                logger.info(f"✓ {msg}")
            
            if not cleaning_actions:
                logger.info("✓ No severe issues")
            
            final_shape = df.shape
            logger.info(f"Shape: {initial_shape} → {final_shape}")
            logger.info("")
            
            return df, cleaning_actions, initial_shape, final_shape
        
        def calculate_quality_score_with_breakdown(validations):
            breakdown = {'starting_score': 100.0, 'penalties': {}, 'total_penalty': 0.0}
            score = 100.0
            
            schema_errors = len(validations['schema']['errors'])
            schema_penalty = schema_errors * 20
            score -= schema_penalty
            breakdown['penalties']['schema_errors'] = {
                'count': schema_errors,
                'penalty': -schema_penalty,
                'note': 'Critical structural issues'
            }
            
            missing_pct = validations['completeness']['details'].get('missing_percentage', 0)
            missing_penalty = min(missing_pct / 2, 30) if missing_pct > 0 else 0
            score -= missing_penalty
            breakdown['penalties']['missing_values'] = {
                'percentage': round(missing_pct, 2),
                'penalty': -round(missing_penalty, 2),
                'max_penalty': -30
            }
            
            dup_pct = validations['uniqueness']['details'].get('duplicate_percentage', 0)
            dup_penalty = min(dup_pct / 2, 15)
            score -= dup_penalty
            breakdown['penalties']['duplicate_rows'] = {
                'percentage': round(dup_pct, 2),
                'penalty': -round(dup_penalty, 2),
                'max_penalty': -15
            }
            
            const_cols = len(validations['uniqueness']['details'].get('constant_columns', []))
            const_penalty = min(const_cols * 2, 20)
            score -= const_penalty
            breakdown['penalties']['constant_columns'] = {
                'count': const_cols,
                'penalty': -round(const_penalty, 2),
                'max_penalty': -20
            }
            
            high_card = validations['uniqueness']['details'].get('high_cardinality_columns', 0)
            card_penalty = min(high_card * 1, 10)
            score -= card_penalty
            breakdown['penalties']['high_cardinality'] = {
                'count': high_card,
                'penalty': -round(card_penalty, 2),
                'max_penalty': -10
            }
            
            outlier_cols = validations['distribution']['details'].get('outlier_columns', 0)
            outlier_penalty = min(outlier_cols * 2, 10)
            score -= outlier_penalty
            breakdown['penalties']['outlier_columns'] = {
                'count': outlier_cols,
                'penalty': -round(outlier_penalty, 2),
                'max_penalty': -10
            }
            
            inf_cols = validations['distribution']['details'].get('infinite_value_columns', 0)
            inf_penalty = min(inf_cols * 3, 15)
            score -= inf_penalty
            breakdown['penalties']['infinite_values'] = {
                'count': inf_cols,
                'penalty': -round(inf_penalty, 2),
                'max_penalty': -15
            }
            
            breakdown['total_penalty'] = round(sum(p['penalty'] for p in breakdown['penalties'].values()), 2)
            
            final_score = max(0, min(100, score))
            breakdown['final_score'] = round(final_score, 2)
            
            if final_score >= 90:
                breakdown['interpretation'] = 'Excellent'
            elif final_score >= 70:
                breakdown['interpretation'] = 'Good'
            elif final_score >= 50:
                breakdown['interpretation'] = 'Moderate'
            elif final_score >= 30:
                breakdown['interpretation'] = 'Poor'
            else:
                breakdown['interpretation'] = 'Critical'
            
            return final_score, breakdown
        
        def generate_recommendations(validations, quality_score):
            recommendations = []
            
            if quality_score < 50:
                recommendations.append("CRITICAL: Quality score very low")
            
            missing_pct = validations['completeness']['details'].get('missing_percentage', 0)
            if missing_pct > 30:
                recommendations.append(f"High missing ({missing_pct:.1f}%) - use MICE/Iterative")
            elif missing_pct > 10:
                recommendations.append(f"Moderate missing ({missing_pct:.1f}%) - use KNN/mean")
            
            dup_pct = validations['uniqueness']['details'].get('duplicate_percentage', 0)
            if dup_pct > 10:
                recommendations.append(f"High duplicates ({dup_pct:.1f}%)")
            
            const_cols = validations['uniqueness']['details'].get('constant_columns', [])
            if const_cols:
                recommendations.append(f"Remove {len(const_cols)} constant columns")
            
            return recommendations
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--input_data", required=True)
            parser.add_argument("--quality_thresholds", default='{}')
            parser.add_argument("--output_validated_data", required=True)
            parser.add_argument("--output_quality_report", required=True)
            parser.add_argument("--output_alerts", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("DATA QUALITY VALIDATION (PARQUET OPTIMIZED)")
            logger.info("="*80)
            
            try:
                ensure_directory_exists(args.output_validated_data)
                ensure_directory_exists(args.output_quality_report)
                ensure_directory_exists(args.output_alerts)
                
                try:
                    thresholds = json.loads(args.quality_thresholds) if args.quality_thresholds != '{}' else {}
                except:
                    thresholds = {}
                
                thresholds.setdefault('max_missing_pct', 50)
                thresholds.setdefault('min_samples', 10)
                thresholds.setdefault('max_duplicates_pct', 20)
                thresholds.setdefault('min_features', 2)
                
                logger.info(f"Thresholds: {thresholds}")
                logger.info("")
                
                df = load_data(args.input_data)
                
                validations = {
                    'schema': validate_schema(df, thresholds),
                    'completeness': check_completeness(df, thresholds),
                    'uniqueness': check_uniqueness(df, thresholds),
                    'distribution': check_distribution(df)
                }
                
                df_cleaned, cleaning_actions, initial_shape, final_shape = clean_severe_issues(df)
                
                quality_score, score_breakdown = calculate_quality_score_with_breakdown(validations)
                recommendations = generate_recommendations(validations, quality_score)
                
                overall_valid = validations['schema']['is_valid'] and validations['completeness']['is_valid']
                
                quality_report = {
                    'overall_valid': overall_valid,
                    'quality_score': float(quality_score),
                    'score_breakdown': score_breakdown,
                    'validations': validations,
                    'cleaning_actions': cleaning_actions,
                    'shape_change': {'initial': list(initial_shape), 'final': list(final_shape)},
                    'recommendations': recommendations,
                    'thresholds': thresholds
                }
                
                all_errors = []
                all_warnings = []
                for category, results in validations.items():
                    all_errors.extend(results.get('errors', []))
                    all_warnings.extend(results.get('warnings', []))
                
                alerts = {
                    'critical_errors': all_errors,
                    'warnings': all_warnings,
                    'quality_score': float(quality_score),
                    'score_breakdown': score_breakdown,
                    'recommendations': recommendations
                }
                
                # SAVE AS PARQUET (NOT CSV!)
                output_parquet = args.output_validated_data
                if not output_parquet.endswith('.parquet'):
                    output_parquet = output_parquet.replace('.csv', '.parquet')
                
                df_cleaned.to_parquet(
                    output_parquet,
                    index=False,
                    engine='pyarrow',
                    compression='snappy'
                )
                
                parquet_size = os.path.getsize(output_parquet) / 1024**2
                logger.info(f"✓ Saved as PARQUET: {parquet_size:.2f} MB")
                
                with open(args.output_quality_report, 'w') as f:
                    json.dump(quality_report, f, indent=2)
                
                with open(args.output_alerts, 'w') as f:
                    json.dump(alerts, f, indent=2)
                
                logger.info("")
                logger.info("="*80)
                logger.info("VALIDATION SUMMARY")
                logger.info("="*80)
                logger.info(f"Valid: {overall_valid}")
                logger.info(f"Score: {quality_score:.1f}/100 ({score_breakdown['interpretation']})")
                logger.info(f"Errors: {len(all_errors)}")
                logger.info(f"Warnings: {len(all_warnings)}")
                logger.info(f"Format: PARQUET (optimized for ML)")
                logger.info("="*80)
                
                if not overall_valid:
                    logger.error("CRITICAL: Validation failed")
                    sys.exit(1)
                
            except Exception as e:
                logger.error(f"ERROR: {e}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --quality_thresholds
      - {inputValue: quality_thresholds}
      - --output_validated_data
      - {outputPath: validated_data}
      - --output_quality_report
      - {outputPath: quality_report}
      - --output_alerts
      - {outputPath: alerts}
