name: Comprehensive Preprocessing Data Upload to CDN
description: Uploads ALL preprocessing outputs (data, metadata, indices, ground truth) from the entire pipeline to CDN for complete traceability and debugging. Returns a structured manifest with all CDN URLs organized by pipeline stage.

inputs:
  # ============================================================================
  # ORIGINAL LOADED DATA
  # ============================================================================
  - name: loaded_data
    type: Data
    description: "Original loaded data from Component 02"
    optional: true
  - name: loaded_metadata
    type: Data
    description: "Original metadata from Component 02"
    optional: true
  - name: loaded_ground_truth
    type: Data
    description: "Original ground truth labels from Component 02"
    optional: true
  
  # ============================================================================
  # VALIDATION OUTPUTS (Component 03)
  # ============================================================================
  - name: validation_data
    type: Data
    description: "Data after quality validation"
    optional: true
  - name: validation_metadata
    type: Data
    description: "Metadata after quality validation"
    optional: true
  - name: validation_report
    type: Data
    description: "Quality validation report (JSON)"
    optional: true
  
  # ============================================================================
  # MISSING VALUES HANDLING (Component 04)
  # ============================================================================
  - name: missing_handled_data
    type: Data
    description: "Data after missing value handling"
    optional: true
  - name: missing_handled_metadata
    type: Data
    description: "Metadata after missing value handling"
    optional: true
  - name: missing_report
    type: Data
    description: "Missing values handling report (JSON)"
    optional: true
  
  # ============================================================================
  # OUTLIER DETECTION (Component 06)
  # ============================================================================
  - name: outlier_handled_data
    type: Data
    description: "Data after outlier detection/handling"
    optional: true
  - name: outlier_handled_metadata
    type: Data
    description: "Metadata after outlier handling"
    optional: true
  - name: outlier_report
    type: Data
    description: "Outlier detection report (JSON)"
    optional: true
  
  # ============================================================================
  # CATEGORICAL ENCODING (Component 07)
  # ============================================================================
  - name: encoded_data
    type: Data
    description: "Data after categorical encoding"
    optional: true
  - name: encoded_metadata
    type: Data
    description: "Metadata after encoding"
    optional: true
  - name: encoding_report
    type: Data
    description: "Encoding transformation report (JSON)"
    optional: true
  
  # ============================================================================
  # FEATURE ENGINEERING (Component 08)
  # ============================================================================
  - name: engineered_data
    type: Data
    description: "Data after feature engineering"
    optional: true
  - name: engineered_metadata
    type: Data
    description: "Metadata after feature engineering"
    optional: true
  - name: engineering_report
    type: Data
    description: "Feature engineering report (JSON)"
    optional: true
  
  # ============================================================================
  # SCALING (Component 09)
  # ============================================================================
  - name: scaled_data
    type: Data
    description: "Data after scaling"
    optional: true
  - name: scaled_metadata
    type: Data
    description: "Metadata after scaling"
    optional: true
  - name: scaling_report
    type: Data
    description: "Scaling transformation report (JSON)"
    optional: true
  
  # ============================================================================
  # DIMENSIONALITY REDUCTION (Component 10)
  # ============================================================================
  - name: reduced_data
    type: Data
    description: "Data after dimensionality reduction"
    optional: true
  - name: reduced_metadata
    type: Data
    description: "Metadata after dimensionality reduction"
    optional: true
  - name: reduction_report
    type: Data
    description: "Dimensionality reduction report (JSON)"
    optional: true
  
  # ============================================================================
  # FINAL TRAIN/TEST SPLIT (Component 11)
  # ============================================================================
  - name: train_data
    type: Data
    description: "Final training dataset"
    optional: true
  - name: test_data
    type: Data
    description: "Final test dataset"
    optional: true
  - name: train_indices
    type: Data
    description: "Training indices (NPY)"
    optional: true
  - name: test_indices
    type: Data
    description: "Test indices (NPY)"
    optional: true
  - name: split_metadata
    type: Data
    description: "Train/test split metadata"
    optional: true
  
  # ============================================================================
  # VALIDATION OUTPUTS (Components 12-13)
  # ============================================================================
  - name: distance_statistics
    type: Data
    description: "Distance matrix validation statistics (JSON)"
    optional: true
  - name: validation_report_distance
    type: Data
    description: "Distance validation report (JSON)"
    optional: true
  - name: parameter_report
    type: Data
    description: "Parameter validation report (JSON)"
    optional: true
  - name: validated_params
    type: Data
    description: "Validated algorithm parameters (JSON)"
    optional: true
  
  # ============================================================================
  # CDN CONFIGURATION
  # ============================================================================
  - name: bearer_token
    type: String
    description: "Bearer token for CDN authentication"
  - name: domain
    type: String
    description: "Upload service base domain"
  - name: get_cdn
    type: String
    description: "Public CDN base domain"
  - name: upload_prefix
    type: String
    description: "Optional prefix for organized storage (e.g., 'experiment_001/', 'dataset_xyz/')"
    default: ""

outputs:
  - name: cdn_manifest
    type: Data
    description: "Complete manifest of all uploaded files with CDN URLs, organized by pipeline stage (JSON)"
  - name: upload_summary
    type: String
    description: "Human-readable summary of upload results"

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - sh
      - -ec
      - |
        if ! command -v curl >/dev/null 2>&1; then
          apt-get update >/dev/null && apt-get install -y curl >/dev/null
        fi
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import subprocess
        import json
        import os
        import time
        import tempfile
        import shutil
        from pathlib import Path
        
        parser = argparse.ArgumentParser(
            description="Comprehensive Preprocessing Data Upload to CDN"
        )
        
        # Original loaded data
        parser.add_argument('--loaded_data', help="Original loaded data")
        parser.add_argument('--loaded_metadata', help="Original metadata")
        parser.add_argument('--loaded_ground_truth', help="Original ground truth")
        
        # Validation outputs
        parser.add_argument('--validation_data', help="Validated data")
        parser.add_argument('--validation_metadata', help="Validated metadata")
        parser.add_argument('--validation_report', help="Validation report")
        
        # Missing values handling
        parser.add_argument('--missing_handled_data', help="Data after missing handling")
        parser.add_argument('--missing_handled_metadata', help="Metadata after missing handling")
        parser.add_argument('--missing_report', help="Missing values report")
        
        # Outlier detection
        parser.add_argument('--outlier_handled_data', help="Data after outlier handling")
        parser.add_argument('--outlier_handled_metadata', help="Metadata after outlier handling")
        parser.add_argument('--outlier_report', help="Outlier detection report")
        
        # Categorical encoding
        parser.add_argument('--encoded_data', help="Encoded data")
        parser.add_argument('--encoded_metadata', help="Encoded metadata")
        parser.add_argument('--encoding_report', help="Encoding report")
        
        # Feature engineering
        parser.add_argument('--engineered_data', help="Engineered data")
        parser.add_argument('--engineered_metadata', help="Engineered metadata")
        parser.add_argument('--engineering_report', help="Engineering report")
        
        # Scaling
        parser.add_argument('--scaled_data', help="Scaled data")
        parser.add_argument('--scaled_metadata', help="Scaled metadata")
        parser.add_argument('--scaling_report', help="Scaling report")
        
        # Dimensionality reduction
        parser.add_argument('--reduced_data', help="Reduced data")
        parser.add_argument('--reduced_metadata', help="Reduced metadata")
        parser.add_argument('--reduction_report', help="Reduction report")
        
        # Train/test split
        parser.add_argument('--train_data', help="Training data")
        parser.add_argument('--test_data', help="Test data")
        parser.add_argument('--train_indices', help="Training indices")
        parser.add_argument('--test_indices', help="Test indices")
        parser.add_argument('--split_metadata', help="Split metadata")
        
        # Validation components
        parser.add_argument('--distance_statistics', help="Distance statistics")
        parser.add_argument('--validation_report_distance', help="Distance validation report")
        parser.add_argument('--parameter_report', help="Parameter report")
        parser.add_argument('--validated_params', help="Validated parameters")
        
        # CDN config
        parser.add_argument('--bearer_token', required=True)
        parser.add_argument('--domain', required=True)
        parser.add_argument('--get_cdn', required=True)
        parser.add_argument('--upload_prefix', default="")
        
        # Outputs
        parser.add_argument('--output_cdn_manifest', required=True)
        parser.add_argument('--output_upload_summary', required=True)
        
        args = parser.parse_args()
        
        print("="*80)
        print("COMPREHENSIVE CDN UPLOAD - PREPROCESSING PIPELINE")
        print("="*80)
        print("Domain:", args.domain)
        print("CDN Base:", args.get_cdn)
        print("Upload Prefix:", args.upload_prefix if args.upload_prefix else "(none)")
        print("")
        
        bearer_token = args.bearer_token
        
        # Build upload URL with optional prefix
        upload_url = (
            args.domain +
            "/mobius-content-service/v1.0/content/upload"
            "?filePathAccess=private&filePath=%2Fbottle%2Flimka%2Fsoda%2F"
        )
        
        if args.upload_prefix:
            encoded_prefix = args.upload_prefix.replace("/", "%2F")
            upload_url += encoded_prefix
        
        def encode_special_chars(text):
            replacements = {
                "$": "%24", "(": "%28", ")": "%29",
                "[": "%5B", "]": "%5D", "{": "%7B",
                "}": "%7D", " ": "%20"
            }
            for char, encoded in replacements.items():
                text = text.replace(char, encoded)
            return text
        
        def get_file_extension(file_path):
            return os.path.splitext(file_path)[1]
        
        def detect_file_type(file_path):
            #Detect file type from extension or content#
            ext = get_file_extension(file_path).lower()
            
            if ext in ['.csv', '.tsv']:
                return 'csv', ext
            elif ext in ['.npy', '.npz']:
                return 'numpy', ext
            elif ext in ['.json']:
                return 'json', ext
            elif ext in ['.pkl', '.pickle']:
                return 'pickle', ext
            else:
                # Try to detect from content
                try:
                    with open(file_path, 'r') as f:
                        first_line = f.readline()
                        if ',' in first_line or '\t' in first_line:
                            return 'csv', '.csv'
                        elif first_line.strip().startswith('{') or first_line.strip().startswith('['):
                            return 'json', '.json'
                except:
                    pass
                
                return 'unknown', ext if ext else '.dat'
        
        def ensure_proper_filename(file_path, stage_name):
            #Ensure file has proper extension based on content#
            original_name = os.path.basename(file_path)
            file_type, detected_ext = detect_file_type(file_path)
            current_ext = get_file_extension(original_name)
            
            if not current_ext or current_ext.lower() != detected_ext.lower():
                # Create temp file with proper extension
                temp_dir = tempfile.gettempdir()
                base_name = os.path.splitext(original_name)[0]
                if not base_name:
                    base_name = stage_name.lower().replace(' ', '_')
                
                new_name = base_name + detected_ext
                new_path = os.path.join(temp_dir, new_name)
                
                shutil.copy2(file_path, new_path)
                return new_path, new_path, detected_ext
            
            return file_path, None, current_ext
        
        def upload_file(file_path, stage_name, file_description):
            #Upload a single file to CDN#
            if not file_path or not os.path.exists(file_path):
                return None
            
            print('Uploading:', file_description)
            print('  Local path:', file_path)
            
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                print('  ⚠ WARNING: File is empty, skipping')
                return None
            
            print('  Size:', str(round(file_size/1024, 2)), 'KB')
            
            proper_path, temp_file, extension = ensure_proper_filename(file_path, stage_name)
            
            try:
                cmd = [
                    "curl",
                    "--location", upload_url,
                    "--header", "Authorization: Bearer " + bearer_token,
                    "--form", "file=@" + proper_path,
                    "--fail",
                    "--show-error",
                    "--silent"
                ]
                
                result = subprocess.run(cmd, capture_output=True, check=True)
                response = json.loads(result.stdout.decode())
                
                relative_url = response.get("cdnUrl")
                if not relative_url:
                    raise RuntimeError('cdnUrl missing in response')
                
                relative_url = encode_special_chars(relative_url)
                full_url = args.get_cdn + relative_url
                
                print('  ✓ Success:', full_url)
                print("")
                
                return {
                    'cdn_url': full_url,
                    'local_path': file_path,
                    'filename': os.path.basename(proper_path),
                    'extension': extension,
                    'size_bytes': file_size,
                    'size_kb': round(file_size / 1024, 2),
                    'size_mb': round(file_size / (1024*1024), 2),
                    'upload_timestamp': int(time.time())
                }
            
            except subprocess.CalledProcessError as e:
                print('  ✗ Upload failed:', e.stderr.decode())
                return None
            
            finally:
                if temp_file and os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                    except:
                        pass
        
        # Define all file mappings organized by stage
        file_mappings = {
            'original_data': {
                'loaded_data': ('Original Loaded Data', args.loaded_data),
                'loaded_metadata': ('Original Metadata', args.loaded_metadata),
                'loaded_ground_truth': ('Original Ground Truth', args.loaded_ground_truth)
            },
            'validation': {
                'validation_data': ('Quality Validated Data', args.validation_data),
                'validation_metadata': ('Quality Validated Metadata', args.validation_metadata),
                'validation_report': ('Quality Validation Report', args.validation_report)
            },
            'missing_values': {
                'missing_handled_data': ('Missing Values Handled Data', args.missing_handled_data),
                'missing_handled_metadata': ('Missing Values Handled Metadata', args.missing_handled_metadata),
                'missing_report': ('Missing Values Report', args.missing_report)
            },
            'outlier_detection': {
                'outlier_handled_data': ('Outlier Handled Data', args.outlier_handled_data),
                'outlier_handled_metadata': ('Outlier Handled Metadata', args.outlier_handled_metadata),
                'outlier_report': ('Outlier Detection Report', args.outlier_report)
            },
            'categorical_encoding': {
                'encoded_data': ('Categorically Encoded Data', args.encoded_data),
                'encoded_metadata': ('Encoded Metadata', args.encoded_metadata),
                'encoding_report': ('Encoding Report', args.encoding_report)
            },
            'feature_engineering': {
                'engineered_data': ('Feature Engineered Data', args.engineered_data),
                'engineered_metadata': ('Feature Engineered Metadata', args.engineered_metadata),
                'engineering_report': ('Feature Engineering Report', args.engineering_report)
            },
            'scaling': {
                'scaled_data': ('Scaled Data', args.scaled_data),
                'scaled_metadata': ('Scaled Metadata', args.scaled_metadata),
                'scaling_report': ('Scaling Report', args.scaling_report)
            },
            'dimensionality_reduction': {
                'reduced_data': ('Dimensionality Reduced Data', args.reduced_data),
                'reduced_metadata': ('Reduced Metadata', args.reduced_metadata),
                'reduction_report': ('Dimensionality Reduction Report', args.reduction_report)
            },
            'train_test_split': {
                'train_data': ('Training Data', args.train_data),
                'test_data': ('Test Data', args.test_data),
                'train_indices': ('Training Indices', args.train_indices),
                'test_indices': ('Test Indices', args.test_indices),
                'split_metadata': ('Split Metadata', args.split_metadata)
            },
            'final_validation': {
                'distance_statistics': ('Distance Statistics', args.distance_statistics),
                'validation_report_distance': ('Distance Validation Report', args.validation_report_distance),
                'parameter_report': ('Parameter Validation Report', args.parameter_report),
                'validated_params': ('Validated Parameters', args.validated_params)
            }
        }
        
        # Upload all files
        manifest = {
            'upload_metadata': {
                'timestamp': int(time.time()),
                'upload_prefix': args.upload_prefix,
                'cdn_base': args.get_cdn
            },
            'stages': {},
            'statistics': {
                'total_files_attempted': 0,
                'total_files_uploaded': 0,
                'total_files_skipped': 0,
                'total_size_bytes': 0,
                'total_size_mb': 0
            }
        }
        
        print("="*80)
        print("UPLOADING FILES BY STAGE")
        print("="*80)
        print("")
        
        for stage_name, files in file_mappings.items():
            print("-" * 80)
            print("STAGE:", stage_name.upper().replace('_', ' '))
            print("-" * 80)
            
            stage_results = {}
            files_uploaded_in_stage = 0
            
            for file_key, (file_desc, file_path) in files.items():
                manifest['statistics']['total_files_attempted'] += 1
                
                result = upload_file(file_path, stage_name, file_desc)
                
                if result:
                    stage_results[file_key] = result
                    files_uploaded_in_stage += 1
                    manifest['statistics']['total_files_uploaded'] += 1
                    manifest['statistics']['total_size_bytes'] += result['size_bytes']
                else:
                    manifest['statistics']['total_files_skipped'] += 1
            
            if stage_results:
                manifest['stages'][stage_name] = {
                    'files': stage_results,
                    'files_uploaded': files_uploaded_in_stage
                }
                print("✓ Stage complete:", files_uploaded_in_stage, "files uploaded")
            else:
                print("○ Stage skipped: No files available")
            
            print("")
        
        manifest['statistics']['total_size_mb'] = round(
            manifest['statistics']['total_size_bytes'] / (1024*1024), 2
        )
        
        # Save manifest
        Path(args.output_cdn_manifest).parent.mkdir(parents=True, exist_ok=True)
        with open(args.output_cdn_manifest, 'w') as f:
            json.dump(manifest, f, indent=2)
        
        # Create human-readable summary
        summary_lines = [
            "="*80,
            "CDN UPLOAD SUMMARY",
            "="*80,
            "",
            "Upload Statistics:",
            f"  Total files attempted: {manifest['statistics']['total_files_attempted']}",
            f"  Successfully uploaded: {manifest['statistics']['total_files_uploaded']}",
            f"  Skipped (missing/empty): {manifest['statistics']['total_files_skipped']}",
            f"  Total size: {manifest['statistics']['total_size_mb']} MB",
            "",
            "Files by Stage:",
            ""
        ]
        
        for stage_name, stage_data in manifest['stages'].items():
            summary_lines.append(f"  {stage_name.upper().replace('_', ' ')}:")
            summary_lines.append(f"    Files: {stage_data['files_uploaded']}")
            for file_key, file_data in stage_data['files'].items():
                summary_lines.append(f"      - {file_key}: {file_data['size_kb']} KB")
            summary_lines.append("")
        
        summary_lines.extend([
            "="*80,
            "Complete manifest saved to cdn_manifest output",
            "="*80
        ])
        
        summary_text = "\n".join(summary_lines)
        print(summary_text)
        
        Path(args.output_upload_summary).parent.mkdir(parents=True, exist_ok=True)
        with open(args.output_upload_summary, 'w') as f:
            f.write(summary_text)
    
    args:
      # Original data
      - --loaded_data
      - {inputPath: loaded_data}
      - --loaded_metadata
      - {inputPath: loaded_metadata}
      - --loaded_ground_truth
      - {inputPath: loaded_ground_truth}
      
      # Validation
      - --validation_data
      - {inputPath: validation_data}
      - --validation_metadata
      - {inputPath: validation_metadata}
      - --validation_report
      - {inputPath: validation_report}
      
      # Missing values
      - --missing_handled_data
      - {inputPath: missing_handled_data}
      - --missing_handled_metadata
      - {inputPath: missing_handled_metadata}
      - --missing_report
      - {inputPath: missing_report}
      
      # Outliers
      - --outlier_handled_data
      - {inputPath: outlier_handled_data}
      - --outlier_handled_metadata
      - {inputPath: outlier_handled_metadata}
      - --outlier_report
      - {inputPath: outlier_report}
      
      # Encoding
      - --encoded_data
      - {inputPath: encoded_data}
      - --encoded_metadata
      - {inputPath: encoded_metadata}
      - --encoding_report
      - {inputPath: encoding_report}
      
      # Feature engineering
      - --engineered_data
      - {inputPath: engineered_data}
      - --engineered_metadata
      - {inputPath: engineered_metadata}
      - --engineering_report
      - {inputPath: engineering_report}
      
      # Scaling
      - --scaled_data
      - {inputPath: scaled_data}
      - --scaled_metadata
      - {inputPath: scaled_metadata}
      - --scaling_report
      - {inputPath: scaling_report}
      
      # Dimensionality reduction
      - --reduced_data
      - {inputPath: reduced_data}
      - --reduced_metadata
      - {inputPath: reduced_metadata}
      - --reduction_report
      - {inputPath: reduction_report}
      
      # Train/test split
      - --train_data
      - {inputPath: train_data}
      - --test_data
      - {inputPath: test_data}
      - --train_indices
      - {inputPath: train_indices}
      - --test_indices
      - {inputPath: test_indices}
      - --split_metadata
      - {inputPath: split_metadata}
      
      # Validation components
      - --distance_statistics
      - {inputPath: distance_statistics}
      - --validation_report_distance
      - {inputPath: validation_report_distance}
      - --parameter_report
      - {inputPath: parameter_report}
      - --validated_params
      - {inputPath: validated_params}
      
      # CDN config
      - --bearer_token
      - {inputValue: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      - --upload_prefix
      - {inputValue: upload_prefix}
      
      # Outputs
      - --output_cdn_manifest
      - {outputPath: cdn_manifest}
      - --output_upload_summary
      - {outputPath: upload_summary}
