name: Comprehensive CDN Upload - All Preprocessing Outputs
description: Uploads ALL 27 preprocessing outputs to CDN across 12 pipeline components with complete traceability

inputs:
  - {name: loaded_data, type: Data, optional: true}
  - {name: loaded_metadata, type: Data, optional: true}
  - {name: loaded_ground_truth, type: Data, optional: true}
  - {name: validation_data, type: Data, optional: true}
  - {name: validation_report, type: Data, optional: true}
  - {name: missing_handled_data, type: Data, optional: true}
  - {name: missing_handled_metadata, type: Data, optional: true}
  - {name: train_indices, type: Data, optional: true}
  - {name: test_indices, type: Data, optional: true}
  - {name: split_metadata, type: Data, optional: true}
  - {name: outlier_handled_data, type: Data, optional: true}
  - {name: outlier_handled_metadata, type: Data, optional: true}
  - {name: encoded_data, type: Data, optional: true}
  - {name: encoded_metadata, type: Data, optional: true}
  - {name: engineered_data, type: Data, optional: true}
  - {name: engineered_metadata, type: Data, optional: true}
  - {name: scaled_data, type: Data, optional: true}
  - {name: scaled_metadata, type: Data, optional: true}
  - {name: fitted_scaler, type: Data, optional: true}
  - {name: reduced_data, type: Data, optional: true}
  - {name: reduced_metadata, type: Data, optional: true}
  - {name: train_data, type: Data, optional: true}
  - {name: test_data, type: Data, optional: true}
  - {name: distance_statistics, type: Data, optional: true}
  - {name: validation_report_distance, type: Data, optional: true}
  - {name: parameter_report, type: Data, optional: true}
  - {name: validated_params, type: Data, optional: true}
  - {name: bearer_token, type: String}
  - {name: domain, type: String}
  - {name: get_cdn, type: String}
  - {name: upload_prefix, type: String, default: ""}

outputs:
  - {name: cdn_manifest, type: Data}
  - {name: upload_summary, type: String}

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - sh
      - -ec
      - |
        if ! command -v curl >/dev/null 2>&1; then
          apt-get update >/dev/null && apt-get install -y curl >/dev/null
        fi
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, time, tempfile, shutil
        from pathlib import Path
        
        p = argparse.ArgumentParser()
        p.add_argument('--loaded_data');p.add_argument('--loaded_metadata');p.add_argument('--loaded_ground_truth')
        p.add_argument('--validation_data');p.add_argument('--validation_report')
        p.add_argument('--missing_handled_data');p.add_argument('--missing_handled_metadata')
        p.add_argument('--train_indices');p.add_argument('--test_indices');p.add_argument('--split_metadata')
        p.add_argument('--outlier_handled_data');p.add_argument('--outlier_handled_metadata')
        p.add_argument('--encoded_data');p.add_argument('--encoded_metadata')
        p.add_argument('--engineered_data');p.add_argument('--engineered_metadata')
        p.add_argument('--scaled_data');p.add_argument('--scaled_metadata');p.add_argument('--fitted_scaler')
        p.add_argument('--reduced_data');p.add_argument('--reduced_metadata')
        p.add_argument('--train_data');p.add_argument('--test_data')
        p.add_argument('--distance_statistics');p.add_argument('--validation_report_distance')
        p.add_argument('--parameter_report');p.add_argument('--validated_params')
        p.add_argument('--bearer_token',required=True);p.add_argument('--domain',required=True);p.add_argument('--get_cdn',required=True);p.add_argument('--upload_prefix',default='')
        p.add_argument('--output_cdn_manifest',required=True);p.add_argument('--output_upload_summary',required=True)
        a = p.parse_args()
        
        print('='*80)
        print('CDN UPLOAD - CLUSTERING PIPELINE')
        print('='*80)
        url = a.domain+'/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fbottle%2Flimka%2Fsoda%2F'
        if a.upload_prefix: url += a.upload_prefix.replace('/','%2F')
        
        def enc(t):
          for c,e in [('$','%24'),('(','%28'),(')','%29'),('[','%5B'),(']','%5D'),('{','%7B'),('}','%7D'),(' ','%20')]: t=t.replace(c,e)
          return t
        
        def up(fp,desc):
          if not fp or not os.path.exists(fp): return None
          print('Uploading:', desc)
          sz=os.path.getsize(fp)
          if sz==0: print('  Skipped (empty)'); return None
          print('  Size:', round(sz/1024,2), 'KB')
          try:
            r=subprocess.run(['curl','--location',url,'--header','Authorization: Bearer %s' % a.bearer_token,'--form','file=@%s' % fp,'--fail','--show-error','--silent'],capture_output=True,check=True)
            resp=json.loads(r.stdout.decode())
            rel=resp.get('cdnUrl')
            if not rel: raise RuntimeError('cdnUrl missing')
            full='%s%s' % (a.get_cdn,enc(rel))
            print('  Success:', full)
            return dict(cdn_url=full,filename=os.path.basename(fp),size_kb=round(sz/1024,2),size_mb=round(sz/(1024*1024),2))
          except Exception as e: print('  Failed:', str(e)); return None
        
        files=dict()
        files['component_02']=dict(loaded_data=('Original Data',a.loaded_data),loaded_metadata=('Original Metadata',a.loaded_metadata),loaded_ground_truth=('Ground Truth',a.loaded_ground_truth))
        files['component_03']=dict(validation_data=('Validated Data',a.validation_data),validation_report=('Validation Report',a.validation_report))
        files['component_04']=dict(missing_handled_data=('Missing Handled Data',a.missing_handled_data),missing_handled_metadata=('Missing Metadata',a.missing_handled_metadata))
        files['component_05']=dict(train_indices=('Train Indices',a.train_indices),test_indices=('Test Indices',a.test_indices),split_metadata=('Split Metadata',a.split_metadata))
        files['component_06']=dict(outlier_handled_data=('Outlier Data',a.outlier_handled_data),outlier_handled_metadata=('Outlier Metadata',a.outlier_handled_metadata))
        files['component_07']=dict(encoded_data=('Encoded Data',a.encoded_data),encoded_metadata=('Encoding Metadata',a.encoded_metadata))
        files['component_08']=dict(engineered_data=('Engineered Data',a.engineered_data),engineered_metadata=('Engineering Metadata',a.engineered_metadata))
        files['component_09']=dict(scaled_data=('Scaled Data',a.scaled_data),scaled_metadata=('Scaling Metadata',a.scaled_metadata),fitted_scaler=('Fitted Scaler',a.fitted_scaler))
        files['component_10']=dict(reduced_data=('Reduced Data',a.reduced_data),reduced_metadata=('Reduction Metadata',a.reduced_metadata))
        files['component_11']=dict(train_data=('Training Data',a.train_data),test_data=('Test Data',a.test_data))
        files['component_12']=dict(distance_statistics=('Distance Stats',a.distance_statistics),validation_report_distance=('Distance Report',a.validation_report_distance))
        files['component_13']=dict(parameter_report=('Parameter Report',a.parameter_report),validated_params=('Validated Params',a.validated_params))
        
        m=dict()
        m['upload_metadata']=dict(timestamp=int(time.time()),upload_prefix=a.upload_prefix,cdn_base=a.get_cdn)
        m['components']=dict()
        m['statistics']=dict(total_files_attempted=0,total_files_uploaded=0,total_files_skipped=0,total_size_bytes=0)
        
        for comp,fdict in files.items():
          print('') 
          print(comp.upper())
          cr=dict()
          for fkey,(fdesc,fpath) in fdict.items():
            m['statistics']['total_files_attempted']+=1
            res=up(fpath,fdesc)
            if res:
              cr[fkey]=res
              m['statistics']['total_files_uploaded']+=1
              m['statistics']['total_size_bytes']+=res['size_kb']*1024
            else:
              m['statistics']['total_files_skipped']+=1
          if cr:
            m['components'][comp]=dict(files=cr,files_uploaded=len(cr))
        
        m['statistics']['total_size_mb']=round(m['statistics']['total_size_bytes']/(1024*1024),2)
        Path(a.output_cdn_manifest).parent.mkdir(parents=True,exist_ok=True)
        with open(a.output_cdn_manifest,'w') as f: json.dump(m,f,indent=2)
        
        sep='='*80
        lines=[]
        lines.append(sep)
        lines.append('CDN UPLOAD COMPLETE')
        lines.append(sep)
        lines.append('Files attempted: %d' % m['statistics']['total_files_attempted'])
        lines.append('Files uploaded: %d' % m['statistics']['total_files_uploaded'])
        lines.append('Files skipped: %d' % m['statistics']['total_files_skipped'])
        lines.append('Total size: %.2f MB' % m['statistics']['total_size_mb'])
        lines.append(sep)
        summ='\n'.join(lines)
        print(summ)
        Path(a.output_upload_summary).parent.mkdir(parents=True,exist_ok=True)
        with open(a.output_upload_summary,'w') as f: f.write(summ)

    args:
      - --loaded_data
      - {inputPath: loaded_data}
      - --loaded_metadata
      - {inputPath: loaded_metadata}
      - --loaded_ground_truth
      - {inputPath: loaded_ground_truth}
      - --validation_data
      - {inputPath: validation_data}
      - --validation_report
      - {inputPath: validation_report}
      - --missing_handled_data
      - {inputPath: missing_handled_data}
      - --missing_handled_metadata
      - {inputPath: missing_handled_metadata}
      - --train_indices
      - {inputPath: train_indices}
      - --test_indices
      - {inputPath: test_indices}
      - --split_metadata
      - {inputPath: split_metadata}
      - --outlier_handled_data
      - {inputPath: outlier_handled_data}
      - --outlier_handled_metadata
      - {inputPath: outlier_handled_metadata}
      - --encoded_data
      - {inputPath: encoded_data}
      - --encoded_metadata
      - {inputPath: encoded_metadata}
      - --engineered_data
      - {inputPath: engineered_data}
      - --engineered_metadata
      - {inputPath: engineered_metadata}
      - --scaled_data
      - {inputPath: scaled_data}
      - --scaled_metadata
      - {inputPath: scaled_metadata}
      - --fitted_scaler
      - {inputPath: fitted_scaler}
      - --reduced_data
      - {inputPath: reduced_data}
      - --reduced_metadata
      - {inputPath: reduced_metadata}
      - --train_data
      - {inputPath: train_data}
      - --test_data
      - {inputPath: test_data}
      - --distance_statistics
      - {inputPath: distance_statistics}
      - --validation_report_distance
      - {inputPath: validation_report_distance}
      - --parameter_report
      - {inputPath: parameter_report}
      - --validated_params
      - {inputPath: validated_params}
      - --bearer_token
      - {inputValue: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      - --upload_prefix
      - {inputValue: upload_prefix}
      - --output_cdn_manifest
      - {outputPath: cdn_manifest}
      - --output_upload_summary
      - {outputPath: upload_summary}
