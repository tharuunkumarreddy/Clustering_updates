name: Comprehensive Preprocessing Data Upload to CDN (Pipeline-Specific)
description: Uploads ALL preprocessing outputs from the complete clustering pipeline to CDN. Includes original data, validation outputs, missing value handling, outlier detection, encoding, feature engineering, scaling, dimensionality reduction, final train/test splits, and validation reports.

inputs:
  # ============================================================================
  # ORIGINAL LOADED DATA (Component 02)
  # ============================================================================
  - name: loaded_data
    type: Data
    description: "Original loaded data from PI API (Component 02: output_data)"
    optional: true
  - name: loaded_metadata
    type: Data
    description: "Original metadata from PI API (Component 02: output_metadata)"
    optional: true
  - name: loaded_ground_truth
    type: Data
    description: "Original ground truth labels (Component 02: output_ground_truth)"
    optional: true
  
  # ============================================================================
  # QUALITY VALIDATION (Component 03)
  # ============================================================================
  - name: validation_data
    type: Data
    description: "Quality validated data (Component 03: output_validated_data)"
    optional: true
  - name: validation_report
    type: Data
    description: "Quality validation report JSON (Component 03: output_validation_report)"
    optional: true
  
  # ============================================================================
  # MISSING VALUES HANDLING (Component 04)
  # ============================================================================
  - name: missing_handled_data
    type: Data
    description: "Data after missing value handling (Component 04: output_data)"
    optional: true
  - name: missing_handled_metadata
    type: Data
    description: "Metadata after missing value handling (Component 04: output_metadata)"
    optional: true
  
  # ============================================================================
  # DATA SPLITTING INDICES (Component 05)
  # ============================================================================
  - name: train_indices
    type: Data
    description: "Training indices NPY (Component 05: output_train_indices)"
    optional: true
  - name: test_indices
    type: Data
    description: "Test indices NPY (Component 05: output_test_indices)"
    optional: true
  - name: split_metadata
    type: Data
    description: "Train/test split metadata (Component 05: output_metadata)"
    optional: true
  
  # ============================================================================
  # OUTLIER DETECTION (Component 06)
  # ============================================================================
  - name: outlier_handled_data
    type: Data
    description: "Data after outlier handling (Component 06: output_preprocessed_data)"
    optional: true
  - name: outlier_handled_metadata
    type: Data
    description: "Outlier metadata (Component 06: output_metadata)"
    optional: true
  
  # ============================================================================
  # CATEGORICAL ENCODING (Component 07)
  # ============================================================================
  - name: encoded_data
    type: Data
    description: "Categorically encoded data (Component 07: output_preprocessed_data)"
    optional: true
  - name: encoded_metadata
    type: Data
    description: "Encoding metadata with mappings (Component 07: output_metadata)"
    optional: true
  
  # ============================================================================
  # FEATURE ENGINEERING (Component 08)
  # ============================================================================
  - name: engineered_data
    type: Data
    description: "Feature engineered data (Component 08: output_preprocessed_data)"
    optional: true
  - name: engineered_metadata
    type: Data
    description: "Feature engineering metadata (Component 08: output_metadata)"
    optional: true
  
  # ============================================================================
  # SCALING (Component 09)
  # ============================================================================
  - name: scaled_data
    type: Data
    description: "Scaled/normalized data (Component 09: output_preprocessed_data)"
    optional: true
  - name: scaled_metadata
    type: Data
    description: "Scaling metadata with parameters (Component 09: output_metadata)"
    optional: true
  - name: fitted_scaler
    type: Data
    description: "Fitted scaler object for inference (Component 09: output_fitted_scaler)"
    optional: true
  
  # ============================================================================
  # DIMENSIONALITY REDUCTION (Component 10)
  # ============================================================================
  - name: reduced_data
    type: Data
    description: "Dimensionality reduced data (Component 10: output_preprocessed_data)"
    optional: true
  - name: reduced_metadata
    type: Data
    description: "Reduction metadata (Component 10: output_metadata)"
    optional: true
  
  # ============================================================================
  # FINAL TRAIN/TEST SPLIT (Component 11)
  # ============================================================================
  - name: train_data
    type: Data
    description: "Final training dataset CSV (Component 11: output_train_data)"
    optional: true
  - name: test_data
    type: Data
    description: "Final test dataset CSV (Component 11: output_test_data)"
    optional: true
  
  # ============================================================================
  # DISTANCE VALIDATION (Component 12)
  # ============================================================================
  - name: distance_statistics
    type: Data
    description: "Distance matrix statistics JSON (Component 12: output_distance_statistics)"
    optional: true
  - name: validation_report_distance
    type: Data
    description: "Distance validation report JSON (Component 12: output_validation_report)"
    optional: true
  
  # ============================================================================
  # PARAMETER VALIDATION (Component 13)
  # ============================================================================
  - name: parameter_report
    type: Data
    description: "Parameter validation report JSON (Component 13: output_validation_report)"
    optional: true
  - name: validated_params
    type: Data
    description: "Validated algorithm parameters JSON (Component 13: output_validated_params)"
    optional: true
  
  # ============================================================================
  # CDN CONFIGURATION
  # ============================================================================
  - name: bearer_token
    type: String
    description: "Bearer token for CDN authentication (from Component 01: accesstoken)"
  - name: domain
    type: String
    description: "Upload service base domain"
  - name: get_cdn
    type: String
    description: "Public CDN base domain"
  - name: upload_prefix
    type: String
    description: "Optional prefix for organized storage (e.g., 'experiment_001/')"
    default: ""

outputs:
  - name: cdn_manifest
    type: Data
    description: "Complete manifest of all uploaded files with CDN URLs (JSON)"
  - name: upload_summary
    type: String
    description: "Human-readable summary of upload results"

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - sh
      - -ec
      - |
        if ! command -v curl >/dev/null 2>&1; then
          apt-get update >/dev/null && apt-get install -y curl >/dev/null
        fi
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import subprocess
        import json
        import os
        import time
        import tempfile
        import shutil
        from pathlib import Path
        
        parser = argparse.ArgumentParser()
        
        # Original loaded data (Component 02)
        parser.add_argument('--loaded_data', help="Original loaded data")
        parser.add_argument('--loaded_metadata', help="Original metadata")
        parser.add_argument('--loaded_ground_truth', help="Original ground truth")
        
        # Quality validation (Component 03)
        parser.add_argument('--validation_data', help="Validated data")
        parser.add_argument('--validation_report', help="Validation report")
        
        # Missing values (Component 04)
        parser.add_argument('--missing_handled_data', help="Data after missing handling")
        parser.add_argument('--missing_handled_metadata', help="Metadata after missing handling")
        
        # Split indices (Component 05)
        parser.add_argument('--train_indices', help="Training indices")
        parser.add_argument('--test_indices', help="Test indices")
        parser.add_argument('--split_metadata', help="Split metadata")
        
        # Outlier detection (Component 06)
        parser.add_argument('--outlier_handled_data', help="Data after outlier handling")
        parser.add_argument('--outlier_handled_metadata', help="Outlier metadata")
        
        # Categorical encoding (Component 07)
        parser.add_argument('--encoded_data', help="Encoded data")
        parser.add_argument('--encoded_metadata', help="Encoding metadata")
        
        # Feature engineering (Component 08)
        parser.add_argument('--engineered_data', help="Engineered data")
        parser.add_argument('--engineered_metadata', help="Engineering metadata")
        
        # Scaling (Component 09)
        parser.add_argument('--scaled_data', help="Scaled data")
        parser.add_argument('--scaled_metadata', help="Scaling metadata")
        parser.add_argument('--fitted_scaler', help="Fitted scaler")
        
        # Dimensionality reduction (Component 10)
        parser.add_argument('--reduced_data', help="Reduced data")
        parser.add_argument('--reduced_metadata', help="Reduction metadata")
        
        # Final split (Component 11)
        parser.add_argument('--train_data', help="Training data")
        parser.add_argument('--test_data', help="Test data")
        
        # Distance validation (Component 12)
        parser.add_argument('--distance_statistics', help="Distance statistics")
        parser.add_argument('--validation_report_distance', help="Distance validation report")
        
        # Parameter validation (Component 13)
        parser.add_argument('--parameter_report', help="Parameter report")
        parser.add_argument('--validated_params', help="Validated parameters")
        
        # CDN config
        parser.add_argument('--bearer_token', required=True)
        parser.add_argument('--domain', required=True)
        parser.add_argument('--get_cdn', required=True)
        parser.add_argument('--upload_prefix', default="")
        
        # Outputs
        parser.add_argument('--output_cdn_manifest', required=True)
        parser.add_argument('--output_upload_summary', required=True)
        
        args = parser.parse_args()
        
        print("="*80)
        print("COMPREHENSIVE CDN UPLOAD - CLUSTERING PIPELINE")
        print("="*80)
        print("Domain:", args.domain)
        print("CDN Base:", args.get_cdn)
        print("Upload Prefix:", args.upload_prefix if args.upload_prefix else "(none)")
        print("")
        
        bearer_token = args.bearer_token
        
        upload_url = (
            args.domain +
            "/mobius-content-service/v1.0/content/upload"
            "?filePathAccess=private&filePath=%2Fbottle%2Flimka%2Fsoda%2F"
        )
        
        if args.upload_prefix:
            encoded_prefix = args.upload_prefix.replace("/", "%2F")
            upload_url += encoded_prefix
        
        def encode_special_chars(text):
            replacements = {
                "$": "%24", "(": "%28", ")": "%29",
                "[": "%5B", "]": "%5D", "{": "%7B",
                "}": "%7D", " ": "%20"
            }
            for char, encoded in replacements.items():
                text = text.replace(char, encoded)
            return text
        
        def get_file_extension(file_path):
            return os.path.splitext(file_path)[1]
        
        def detect_file_type(file_path):
            ext = get_file_extension(file_path).lower()
            
            if ext in ['.csv', '.tsv']:
                return 'csv', ext
            elif ext in ['.npy', '.npz']:
                return 'numpy', ext
            elif ext in ['.json']:
                return 'json', ext
            elif ext in ['.pkl', '.pickle']:
                return 'pickle', ext
            else:
                try:
                    with open(file_path, 'r') as f:
                        first_line = f.readline()
                        if ',' in first_line or '\t' in first_line:
                            return 'csv', '.csv'
                        elif first_line.strip().startswith('{') or first_line.strip().startswith('['):
                            return 'json', '.json'
                except:
                    pass
                
                return 'unknown', ext if ext else '.dat'
        
        def ensure_proper_filename(file_path, stage_name):
            original_name = os.path.basename(file_path)
            file_type, detected_ext = detect_file_type(file_path)
            current_ext = get_file_extension(original_name)
            
            if not current_ext or current_ext.lower() != detected_ext.lower():
                temp_dir = tempfile.gettempdir()
                base_name = os.path.splitext(original_name)[0]
                if not base_name:
                    base_name = stage_name.lower().replace(' ', '_')
                
                new_name = base_name + detected_ext
                new_path = os.path.join(temp_dir, new_name)
                
                shutil.copy2(file_path, new_path)
                return new_path, new_path, detected_ext
            
            return file_path, None, current_ext
        
        def upload_file(file_path, stage_name, file_description):
            if not file_path or not os.path.exists(file_path):
                return None
            
            print('Uploading:', file_description)
            print('  Local path:', file_path)
            
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                print('  ⚠ WARNING: File is empty, skipping')
                return None
            
            print('  Size:', str(round(file_size/1024, 2)), 'KB')
            
            proper_path, temp_file, extension = ensure_proper_filename(file_path, stage_name)
            
            try:
                cmd = [
                    "curl",
                    "--location", upload_url,
                    "--header", "Authorization: Bearer " + bearer_token,
                    "--form", "file=@" + proper_path,
                    "--fail",
                    "--show-error",
                    "--silent"
                ]
                
                result = subprocess.run(cmd, capture_output=True, check=True)
                response = json.loads(result.stdout.decode())
                
                relative_url = response.get("cdnUrl")
                if not relative_url:
                    raise RuntimeError('cdnUrl missing in response')
                
                relative_url = encode_special_chars(relative_url)
                full_url = args.get_cdn + relative_url
                
                print('  ✓ Success:', full_url)
                print("")
                
                return {
                    'cdn_url': full_url,
                    'local_path': file_path,
                    'filename': os.path.basename(proper_path),
                    'extension': extension,
                    'size_bytes': file_size,
                    'size_kb': round(file_size / 1024, 2),
                    'size_mb': round(file_size / (1024*1024), 2),
                    'upload_timestamp': int(time.time())
                }
            
            except subprocess.CalledProcessError as e:
                print('  ✗ Upload failed:', e.stderr.decode())
                return None
            
            finally:
                if temp_file and os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                    except:
                        pass
        
        # File mappings organized by pipeline component
        file_mappings = {
            'component_02_data_loading': {
                'loaded_data': ('Original Loaded Data (PI API)', args.loaded_data),
                'loaded_metadata': ('Original Metadata', args.loaded_metadata),
                'loaded_ground_truth': ('Original Ground Truth Labels', args.loaded_ground_truth)
            },
            'component_03_quality_validation': {
                'validation_data': ('Quality Validated Data', args.validation_data),
                'validation_report': ('Quality Validation Report', args.validation_report)
            },
            'component_04_missing_values': {
                'missing_handled_data': ('Missing Values Handled Data', args.missing_handled_data),
                'missing_handled_metadata': ('Missing Values Metadata', args.missing_handled_metadata)
            },
            'component_05_data_splitting': {
                'train_indices': ('Training Indices (NPY)', args.train_indices),
                'test_indices': ('Test Indices (NPY)', args.test_indices),
                'split_metadata': ('Train/Test Split Metadata', args.split_metadata)
            },
            'component_06_outlier_detection': {
                'outlier_handled_data': ('Outlier Handled Data', args.outlier_handled_data),
                'outlier_handled_metadata': ('Outlier Detection Metadata', args.outlier_handled_metadata)
            },
            'component_07_categorical_encoding': {
                'encoded_data': ('Categorically Encoded Data', args.encoded_data),
                'encoded_metadata': ('Encoding Metadata & Mappings', args.encoded_metadata)
            },
            'component_08_feature_engineering': {
                'engineered_data': ('Feature Engineered Data', args.engineered_data),
                'engineered_metadata': ('Feature Engineering Metadata', args.engineered_metadata)
            },
            'component_09_scaling': {
                'scaled_data': ('Scaled/Normalized Data', args.scaled_data),
                'scaled_metadata': ('Scaling Parameters Metadata', args.scaled_metadata),
                'fitted_scaler': ('Fitted Scaler Object', args.fitted_scaler)
            },
            'component_10_dimensionality_reduction': {
                'reduced_data': ('Dimensionality Reduced Data', args.reduced_data),
                'reduced_metadata': ('Reduction Metadata', args.reduced_metadata)
            },
            'component_11_final_split': {
                'train_data': ('Final Training Dataset (CSV)', args.train_data),
                'test_data': ('Final Test Dataset (CSV)', args.test_data)
            },
            'component_12_distance_validation': {
                'distance_statistics': ('Distance Matrix Statistics', args.distance_statistics),
                'validation_report_distance': ('Distance Validation Report', args.validation_report_distance)
            },
            'component_13_parameter_validation': {
                'parameter_report': ('Parameter Validation Report', args.parameter_report),
                'validated_params': ('Validated Algorithm Parameters', args.validated_params)
            }
        }
        
        manifest = {
            'upload_metadata': {
                'timestamp': int(time.time()),
                'upload_prefix': args.upload_prefix,
                'cdn_base': args.get_cdn,
                'pipeline': 'clustering_preprocessing_v3'
            },
            'components': {},
            'statistics': {
                'total_files_attempted': 0,
                'total_files_uploaded': 0,
                'total_files_skipped': 0,
                'total_size_bytes': 0,
                'total_size_mb': 0
            }
        }
        
        print("="*80)
        print("UPLOADING FILES BY PIPELINE COMPONENT")
        print("="*80)
        print("")
        
        for component_name, files in file_mappings.items():
            print("-" * 80)
            print("COMPONENT:", component_name.upper().replace('_', ' '))
            print("-" * 80)
            
            component_results = {}
            files_uploaded_in_component = 0
            
            for file_key, (file_desc, file_path) in files.items():
                manifest['statistics']['total_files_attempted'] += 1
                
                result = upload_file(file_path, component_name, file_desc)
                
                if result:
                    component_results[file_key] = result
                    files_uploaded_in_component += 1
                    manifest['statistics']['total_files_uploaded'] += 1
                    manifest['statistics']['total_size_bytes'] += result['size_bytes']
                else:
                    manifest['statistics']['total_files_skipped'] += 1
            
            if component_results:
                manifest['components'][component_name] = {
                    'files': component_results,
                    'files_uploaded': files_uploaded_in_component
                }
                print("✓ Component complete:", files_uploaded_in_component, "files uploaded")
            else:
                print("○ Component skipped: No files available")
            
            print("")
        
        manifest['statistics']['total_size_mb'] = round(
            manifest['statistics']['total_size_bytes'] / (1024*1024), 2
        )
        
        # Save manifest
        Path(args.output_cdn_manifest).parent.mkdir(parents=True, exist_ok=True)
        with open(args.output_cdn_manifest, 'w') as f:
            json.dump(manifest, f, indent=2)
        
        # Create summary
        summary_lines = [
            "="*80,
            "CDN UPLOAD SUMMARY - CLUSTERING PIPELINE",
            "="*80,
            "",
            "Statistics:",
            f"  Total files attempted: {manifest['statistics']['total_files_attempted']}",
            f"  Successfully uploaded: {manifest['statistics']['total_files_uploaded']}",
            f"  Skipped (missing/empty): {manifest['statistics']['total_files_skipped']}",
            f"  Total size: {manifest['statistics']['total_size_mb']} MB",
            "",
            "Files by Component:",
            ""
        ]
        
        for comp_name, comp_data in manifest['components'].items():
            summary_lines.append(f"  {comp_name.upper().replace('_', ' ')}:")
            summary_lines.append(f"    Files: {comp_data['files_uploaded']}")
            for file_key, file_data in comp_data['files'].items():
                summary_lines.append(f"      - {file_key}: {file_data['size_kb']} KB")
            summary_lines.append("")
        
        summary_lines.extend([
            "="*80,
            "Manifest saved: Use cdn_manifest output to access all CDN URLs",
            "="*80
        ])
        
        summary_text = "\n".join(summary_lines)
        print(summary_text)
        
        Path(args.output_upload_summary).parent.mkdir(parents=True, exist_ok=True)
        with open(args.output_upload_summary, 'w') as f:
            f.write(summary_text)
    
    args:
      # Component 02 - Data Loading
      - --loaded_data
      - {inputPath: loaded_data}
      - --loaded_metadata
      - {inputPath: loaded_metadata}
      - --loaded_ground_truth
      - {inputPath: loaded_ground_truth}
      
      # Component 03 - Quality Validation
      - --validation_data
      - {inputPath: validation_data}
      - --validation_report
      - {inputPath: validation_report}
      
      # Component 04 - Missing Values
      - --missing_handled_data
      - {inputPath: missing_handled_data}
      - --missing_handled_metadata
      - {inputPath: missing_handled_metadata}
      
      # Component 05 - Data Splitting
      - --train_indices
      - {inputPath: train_indices}
      - --test_indices
      - {inputPath: test_indices}
      - --split_metadata
      - {inputPath: split_metadata}
      
      # Component 06 - Outliers
      - --outlier_handled_data
      - {inputPath: outlier_handled_data}
      - --outlier_handled_metadata
      - {inputPath: outlier_handled_metadata}
      
      # Component 07 - Encoding
      - --encoded_data
      - {inputPath: encoded_data}
      - --encoded_metadata
      - {inputPath: encoded_metadata}
      
      # Component 08 - Feature Engineering
      - --engineered_data
      - {inputPath: engineered_data}
      - --engineered_metadata
      - {inputPath: engineered_metadata}
      
      # Component 09 - Scaling
      - --scaled_data
      - {inputPath: scaled_data}
      - --scaled_metadata
      - {inputPath: scaled_metadata}
      - --fitted_scaler
      - {inputPath: fitted_scaler}
      
      # Component 10 - Dimensionality Reduction
      - --reduced_data
      - {inputPath: reduced_data}
      - --reduced_metadata
      - {inputPath: reduced_metadata}
      
      # Component 11 - Final Split
      - --train_data
      - {inputPath: train_data}
      - --test_data
      - {inputPath: test_data}
      
      # Component 12 - Distance Validation
      - --distance_statistics
      - {inputPath: distance_statistics}
      - --validation_report_distance
      - {inputPath: validation_report_distance}
      
      # Component 13 - Parameter Validation
      - --parameter_report
      - {inputPath: parameter_report}
      - --validated_params
      - {inputPath: validated_params}
      
      # CDN config
      - --bearer_token
      - {inputValue: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      - --upload_prefix
      - {inputValue: upload_prefix}
      
      # Outputs
      - --output_cdn_manifest
      - {outputPath: cdn_manifest}
      - --output_upload_summary
      - {outputPath: upload_summary}
