name: Unified Outlier Detection & Handling Component (FIXED - File Corruption)
description: Train-aware outlier detection for unified preprocessing. FIXED - Proper file saving with explicit flushing and validation to prevent empty/corrupted files.

inputs:
  - name: combined_data
    type: Data
    description: 'Combined dataset (train + test together) (CSV)'
  - name: train_indices
    type: Data
    description: 'Train row indices (JSON array)'
  - name: detection_method
    type: String
    description: 'Outlier detection method: zscore, iqr, lof, isolation_forest, none'
    default: 'zscore'
  - name: detection_params
    type: String
    description: 'Detection parameters as JSON'
    default: '{}'
  - name: handling_method
    type: String
    description: 'Outlier handling method: remove, clip, cap, winsorize, log_transform, sqrt_transform, flag, none'
    default: 'clip'
  - name: handling_params
    type: String
    description: 'Handling parameters as JSON'
    default: '{}'

outputs:
  - name: preprocessed_data
    type: Data
    description: 'Dataset with outliers handled (CSV)'
  - name: report
    type: Data
    description: 'Outlier detection and handling report (JSON)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pandas as pd
        import numpy as np
        from scipy import stats
        from scipy.stats import mstats
        from sklearn.neighbors import LocalOutlierFactor
        from sklearn.ensemble import IsolationForest
        from pathlib import Path
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger('outlier_unified')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory:
                os.makedirs(directory, exist_ok=True)
        
        def load_data(input_path):
            ext = Path(input_path).suffix.lower()
            if ext in ['.parquet', '.pq']:
                return pd.read_parquet(input_path)
            return pd.read_csv(input_path)
        
        def load_indices(indices_path):
            with open(indices_path, 'r') as f:
                return json.load(f)
        
        def save_dataframe_safely(df, output_path):
            #Save DataFrame with explicit flushing and validation#
            logger.info(f"Saving DataFrame to: {output_path}")
            logger.info(f"  Shape: {df.shape}")
            logger.info(f"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
            
            # Ensure directory exists
            ensure_directory_exists(output_path)
            
            # Save with explicit parameters
            df.to_csv(
                output_path,
                index=False,
                encoding='utf-8',
                chunksize=10000  # Write in chunks for large files
            )
            
            # Force flush and sync
            import time
            time.sleep(1)  # Give filesystem time to sync
            
            # Validate file was written
            if not os.path.exists(output_path):
                raise IOError(f"File was not created: {output_path}")
            
            file_size = os.path.getsize(output_path)
            if file_size == 0:
                raise IOError(f"File is empty (0 bytes): {output_path}")
            
            logger.info(f"  File saved: {file_size / 1024**2:.2f} MB")
            logger.info(f"  File exists: {os.path.exists(output_path)}")
            
            # Quick validation: try to read back first few rows
            try:
                df_test = pd.read_csv(output_path, nrows=5)
                logger.info(f"  Validation: Successfully read back {len(df_test)} rows")
                logger.info(f"  Columns: {len(df_test.columns)}")
            except Exception as e:
                logger.error(f"  Validation FAILED: {e}")
                raise
            
            return output_path
        
        def detect_outliers_unified(df, train_indices, method, params):
            logger.info("="*80)
            logger.info("UNIFIED OUTLIER DETECTION")
            logger.info("="*80)
            logger.info("Method: " + str(method))
            logger.info("Train indices: " + str(len(train_indices)) + " rows")
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - no detection performed")
                return pd.Series(False, index=df.index), None, {
                    'method': 'none',
                    'n_outliers': 0,
                    'outlier_percentage': 0.0,
                    'note': 'No outlier detection performed'
                }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns")
                return pd.Series(False, index=df.index), None, {
                    'method': method,
                    'n_outliers': 0,
                    'warning': 'no_numeric_columns'
                }
            
            logger.info("Analyzing " + str(len(numeric_cols)) + " numeric columns")
            
            train_df = df.iloc[train_indices]
            
            detection_info = {
                'method': method,
                'params': params,
                'numeric_columns': numeric_cols,
                'n_numeric_cols': len(numeric_cols)
            }
            
            detector = None
            outlier_mask = pd.Series(False, index=df.index)
            
            if method == 'zscore':
                threshold = params.get('threshold', 3.0)
                logger.info("Z-score threshold: " + str(threshold))
                
                train_mean = train_df[numeric_cols].mean()
                train_std = train_df[numeric_cols].std()
                
                z_scores = np.abs((df[numeric_cols] - train_mean) / train_std)
                outlier_mask = (z_scores > threshold).any(axis=1)
                
                detector = {
                    'method': 'zscore',
                    'threshold': threshold,
                    'mean': train_mean.to_dict(),
                    'std': train_std.to_dict()
                }
                
                detection_info['threshold'] = threshold
            
            elif method == 'iqr':
                factor = params.get('factor', 1.5)
                logger.info("IQR factor: " + str(factor))
                
                bounds = {}
                for col in numeric_cols:
                    Q1 = train_df[col].quantile(0.25)
                    Q3 = train_df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    
                    lower = Q1 - factor * IQR
                    upper = Q3 + factor * IQR
                    
                    bounds[col] = {
                        'lower': float(lower),
                        'upper': float(upper),
                        'Q1': float(Q1),
                        'Q3': float(Q3),
                        'IQR': float(IQR)
                    }
                    
                    col_outliers = (df[col] < lower) | (df[col] > upper)
                    outlier_mask = outlier_mask | col_outliers
                
                detector = {'method': 'iqr', 'bounds': bounds, 'factor': factor}
                detection_info['bounds'] = bounds
            
            elif method == 'lof':
                n_neighbors = params.get('n_neighbors', 20)
                contamination = params.get('contamination', 0.1)
                
                X_train = train_df[numeric_cols].values
                
                if len(train_df) < n_neighbors:
                    n_neighbors = max(2, len(train_df) - 1)
                
                lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, novelty=True)
                lof.fit(X_train)
                
                X_all = df[numeric_cols].values
                outlier_labels = lof.predict(X_all)
                outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                
                detector = lof
                detection_info['n_neighbors'] = n_neighbors
                detection_info['contamination'] = contamination
            
            elif method == 'isolation_forest':
                contamination = params.get('contamination', 0.1)
                n_estimators = params.get('n_estimators', 100)
                random_state = params.get('random_state', 42)
                
                X_train = train_df[numeric_cols].values
                
                iso = IsolationForest(
                    contamination=contamination,
                    n_estimators=n_estimators,
                    random_state=random_state,
                    n_jobs=-1
                )
                iso.fit(X_train)
                
                X_all = df[numeric_cols].values
                outlier_labels = iso.predict(X_all)
                outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                
                detector = iso
                detection_info['contamination'] = contamination
                detection_info['n_estimators'] = n_estimators
            
            n_outliers = outlier_mask.sum()
            pct = (n_outliers / len(df) * 100) if len(df) > 0 else 0
            detection_info['n_outliers'] = int(n_outliers)
            detection_info['outlier_percentage'] = float(pct)
            
            logger.info("Detected " + str(n_outliers) + " outliers (" + str(round(pct, 2)) + "%)")
            logger.info("")
            
            return outlier_mask, detector, detection_info
        
        def handle_outliers_unified(df, outlier_mask, method, params, detection_info):
            logger.info("="*80)
            logger.info("HANDLING OUTLIERS")
            logger.info("="*80)
            logger.info("Method: " + str(method))
            logger.info("")
            
            if method == 'none':
                logger.info("Method is 'none' - passing data through")
                return df, {
                    'method': 'none', 
                    'n_rows_removed': 0,
                    'initial_shape': list(df.shape),
                    'final_shape': list(df.shape),
                    'note': 'Data passed through unchanged'
                }
            
            df_handled = df.copy()
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            stats = {
                'method': method,
                'params': params,
                'n_outliers_detected': int(outlier_mask.sum()),
                'n_rows_removed': 0
            }
            
            if method == 'remove':
                df_handled = df[~outlier_mask].reset_index(drop=True)
                stats['n_rows_removed'] = outlier_mask.sum()
            
            elif method in ['clip', 'cap']:
                use_iqr = params.get('use_iqr', True)
                factor = params.get('factor', 1.5)
                
                if detection_info and 'bounds' in detection_info:
                    bounds = detection_info['bounds']
                elif use_iqr:
                    bounds = {}
                    for col in numeric_cols:
                        Q1 = df[col].quantile(0.25)
                        Q3 = df[col].quantile(0.75)
                        IQR = Q3 - Q1
                        bounds[col] = {
                            'lower': Q1 - factor * IQR,
                            'upper': Q3 + factor * IQR
                        }
                else:
                    bounds = {col: {'lower': df[col].min(), 'upper': df[col].max()} for col in numeric_cols}
                
                for col in numeric_cols:
                    if col in bounds:
                        df_handled[col] = df_handled[col].clip(
                            lower=bounds[col]['lower'],
                            upper=bounds[col]['upper']
                        )
            
            elif method == 'winsorize':
                limits = params.get('limits', [0.05, 0.05])
                for col in numeric_cols:
                    df_handled[col] = mstats.winsorize(df_handled[col], limits=limits)
            
            elif method == 'log_transform':
                for col in numeric_cols:
                    if (df_handled[col] > 0).all():
                        df_handled[col] = np.log1p(df_handled[col])
            
            elif method == 'sqrt_transform':
                for col in numeric_cols:
                    if (df_handled[col] >= 0).all():
                        df_handled[col] = np.sqrt(df_handled[col])
            
            elif method == 'flag':
                flag_col = params.get('flag_column', 'is_outlier')
                df_handled[flag_col] = outlier_mask.astype(int)
            
            stats['initial_shape'] = list(df.shape)
            stats['final_shape'] = list(df_handled.shape)
            
            logger.info("Final shape: " + str(df_handled.shape))
            logger.info("")
            
            return df_handled, stats
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--combined_data", required=True)
            parser.add_argument("--train_indices", required=True)
            parser.add_argument("--detection_method", default='zscore')
            parser.add_argument("--detection_params", default='{}')
            parser.add_argument("--handling_method", default='clip')
            parser.add_argument("--handling_params", default='{}')
            parser.add_argument("--output_preprocessed_data", required=True)
            parser.add_argument("--output_report", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("UNIFIED OUTLIER DETECTION & HANDLING")
            logger.info("="*80)
            logger.info("Detection: " + args.detection_method)
            logger.info("Handling: " + args.handling_method)
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_preprocessed_data)
                ensure_directory_exists(args.output_report)
                
                detection_params = json.loads(args.detection_params)
                handling_params = json.loads(args.handling_params)
                
                df = load_data(args.combined_data)
                train_indices = load_indices(args.train_indices)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                logger.info("Input data shape: " + str(df.shape))
                logger.info("")
                
                # PASSTHROUGH MODE
                if args.detection_method == 'none' and args.handling_method == 'none':
                    logger.info("="*80)
                    logger.info("PASSTHROUGH MODE")
                    logger.info("="*80)
                    logger.info("Both detection and handling are 'none'")
                    logger.info("Passing data through unchanged")
                    logger.info("")
                    
                    # CRITICAL FIX: Use safe save function
                    save_dataframe_safely(df, args.output_preprocessed_data)
                    
                    report = {
                        'detection': {
                            'method': 'none',
                            'n_outliers': 0,
                            'outlier_percentage': 0.0,
                            'note': 'No outlier detection performed'
                        },
                        'handling': {
                            'method': 'none',
                            'n_rows_removed': 0,
                            'initial_shape': list(df.shape),
                            'final_shape': list(df.shape),
                            'note': 'Data passed through unchanged'
                        },
                        'passthrough': True
                    }
                    
                    with open(args.output_report, 'w') as f:
                        json.dump(report, f, indent=2)
                    logger.info("Report saved")
                    
                    logger.info("")
                    logger.info("="*80)
                    logger.info("PASSTHROUGH COMPLETED")
                    logger.info("="*80)
                    logger.info("Final shape: " + str(df.shape))
                    logger.info("="*80)
                    
                else:
                    # Normal processing
                    outlier_mask, detector, detection_info = detect_outliers_unified(
                        df=df,
                        train_indices=train_indices,
                        method=args.detection_method,
                        params=detection_params
                    )
                    
                    df_handled, handling_stats = handle_outliers_unified(
                        df=df,
                        outlier_mask=outlier_mask,
                        method=args.handling_method,
                        params=handling_params,
                        detection_info=detection_info
                    )
                    
                    # CRITICAL FIX: Use safe save function
                    save_dataframe_safely(df_handled, args.output_preprocessed_data)
                    
                    report = {
                        'detection': detection_info,
                        'handling': handling_stats,
                        'passthrough': False
                    }
                    
                    with open(args.output_report, 'w') as f:
                        json.dump(report, f, indent=2)
                    logger.info("Report saved")
                    
                    logger.info("")
                    logger.info("="*80)
                    logger.info("OUTLIER PROCESSING COMPLETED")
                    logger.info("="*80)
                    logger.info("Detection: " + args.detection_method)
                    logger.info("Final shape: " + str(df_handled.shape))
                    logger.info("="*80)
                
            except Exception as e:
                logger.error("ERROR: " + str(e))
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --combined_data
      - {inputPath: combined_data}
      - --train_indices
      - {inputPath: train_indices}
      - --detection_method
      - {inputValue: detection_method}
      - --detection_params
      - {inputValue: detection_params}
      - --handling_method
      - {inputValue: handling_method}
      - --handling_params
      - {inputValue: handling_params}
      - --output_preprocessed_data
      - {outputPath: preprocessed_data}
      - --output_report
      - {outputPath: report}
