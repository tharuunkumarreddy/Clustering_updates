name: Generic Outlier Detection & Handling Component (Enhanced)
description: Comprehensive outlier detection and handling for clustering with 4 detection methods and 7 handling strategies. Supports fit/transform modes for train/test splitting. Detection methods include Z-score, IQR, LOF, and Isolation Forest. Handling methods include remove, clip, winsorize, transforms, and flagging.

inputs:
  - name: input_data
    type: Data
    description: 'Input dataset (CSV or Parquet)'
  - name: detection_method
    type: String
    description: 'Outlier detection method: zscore, iqr, lof, isolation_forest, none'
    default: 'zscore'
  - name: detection_params
    type: String
    description: 'Detection method parameters as JSON string. Examples: {"threshold":3.0}, {"factor":1.5}, {"n_neighbors":20}'
    default: '{}'
  - name: handling_method
    type: String
    description: 'Outlier handling method: remove, clip, cap, winsorize, log_transform, sqrt_transform, flag, none'
    default: 'clip'
  - name: handling_params
    type: String
    description: 'Handling method parameters as JSON string. Examples: {"use_iqr":true,"factor":1.5}, {"limits":[0.05,0.05]}'
    default: '{}'
  - name: mode
    type: String
    description: 'Processing mode: fit_transform (train) or transform (test)'
    default: 'fit_transform'
  - name: fitted_detector
    type: Model
    description: 'Pre-fitted detector for transform mode (PKL format, only used in transform mode)'
    optional: true

outputs:
  - name: data
    type: Data
    description: 'Dataset with outliers handled (CSV format)'
  - name: outlier_info
    type: Model
    description: 'Outlier detection information and fitted detector (PKL format)'
  - name: report
    type: Data
    description: 'Comprehensive outlier detection and handling report (JSON format)'

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import argparse
        import logging
        import pickle
        import pandas as pd
        import numpy as np
        from scipy import stats
        from scipy.stats import mstats
        from sklearn.neighbors import LocalOutlierFactor
        from sklearn.ensemble import IsolationForest
        from pathlib import Path
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger('outlier_handler')
        
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                logger.info(f"Created directory: {directory}")
        
        
        def load_data(input_path):
            logger.info(f"Loading dataset from: {input_path}")
            ext = Path(input_path).suffix.lower()
            
            try:
                if ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(input_path)
                    logger.info("Loaded Parquet file")
                else:
                    df = pd.read_csv(input_path)
                    logger.info("Loaded CSV file")
                
                logger.info(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
                return df
            except Exception as e:
                logger.error(f"Error loading data: {str(e)}")
                raise
        
        
        def detect_outliers(df, method, detection_params):
            #Detect outliers - FIT mode#
            logger.info("="*80)
            logger.info("DETECTING OUTLIERS (FIT)")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            
            if detection_params:
                logger.info(f"Parameters: {detection_params}")
            logger.info("")
            
            if method == 'none':
                logger.info("Detection method is 'none' - skipping")
                return pd.Series(False, index=df.index), None, {
                    'method': 'none',
                    'n_outliers': 0,
                    'outlier_percentage': 0.0
                }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) == 0:
                logger.warning("No numeric columns for outlier detection")
                return pd.Series(False, index=df.index), None, {
                    'method': method,
                    'n_outliers': 0,
                    'outlier_percentage': 0.0,
                    'warning': 'No numeric columns found'
                }
            
            logger.info(f"Analyzing {len(numeric_cols)} numeric columns")
            
            detection_info = {
                'method': method,
                'params': detection_params,
                'numeric_columns': numeric_cols,
                'numeric_column_count': len(numeric_cols)
            }
            
            detector = None
            
            # METHOD 1: Z-Score
            if method == 'zscore':
                threshold = detection_params.get('threshold', 3.0)
                logger.info(f"Z-score threshold: {threshold}")
                
                z_scores = np.abs(stats.zscore(df[numeric_cols], nan_policy='omit'))
                outlier_mask = (z_scores > threshold).any(axis=1)
                
                # Store thresholds for test data
                detector = {
                    'method': 'zscore',
                    'threshold': threshold,
                    'mean': df[numeric_cols].mean().to_dict(),
                    'std': df[numeric_cols].std().to_dict(),
                    'numeric_columns': numeric_cols
                }
                
                detection_info['threshold'] = threshold
                
                logger.info("Per-column outlier counts:")
                for col in numeric_cols:
                    col_outliers = (z_scores[:, numeric_cols.index(col)] > threshold).sum()
                    if col_outliers > 0:
                        logger.info(f"  {col}: {col_outliers} outliers")
            
            # METHOD 2: IQR
            elif method == 'iqr':
                factor = detection_params.get('factor', 1.5)
                logger.info(f"IQR factor: {factor}")
                
                outlier_mask = pd.Series(False, index=df.index)
                bounds = {}
                
                logger.info("Calculating IQR bounds per column:")
                for col in numeric_cols:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    
                    lower_bound = Q1 - factor * IQR
                    upper_bound = Q3 + factor * IQR
                    
                    bounds[col] = {
                        'lower': float(lower_bound),
                        'upper': float(upper_bound),
                        'Q1': float(Q1),
                        'Q3': float(Q3),
                        'IQR': float(IQR)
                    }
                    
                    col_outliers = (df[col] < lower_bound) | (df[col] > upper_bound)
                    outlier_mask = outlier_mask | col_outliers
                    
                    n_col_outliers = col_outliers.sum()
                    if n_col_outliers > 0:
                        logger.info(
                            f"  {col}: {n_col_outliers} outliers "
                            f"(bounds: [{lower_bound:.3f}, {upper_bound:.3f}])"
                        )
                
                detector = {
                    'method': 'iqr',
                    'bounds': bounds,
                    'factor': factor,
                    'numeric_columns': numeric_cols
                }
                
                detection_info['bounds'] = bounds
                detection_info['factor'] = factor
            
            # METHOD 3: LOF
            elif method == 'lof':
                n_neighbors = detection_params.get('n_neighbors', 20)
                contamination = detection_params.get('contamination', 0.1)
                
                logger.info(f"LOF parameters:")
                logger.info(f"  n_neighbors: {n_neighbors}")
                logger.info(f"  contamination: {contamination}")
                
                X = df[numeric_cols].values
                
                if len(df) < n_neighbors:
                    logger.warning(f"Not enough samples ({len(df)}) for n_neighbors={n_neighbors}")
                    n_neighbors = max(2, len(df) - 1)
                    logger.warning(f"Reduced n_neighbors to {n_neighbors}")
                
                lof = LocalOutlierFactor(
                    n_neighbors=n_neighbors,
                    contamination=contamination,
                    novelty=True  # Enable predict for test data
                )
                lof.fit(X)
                outlier_labels = lof.predict(X)
                lof_scores = lof.score_samples(X)
                
                outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                
                detector = lof  # Store fitted LOF
                
                detection_info['lof_scores_sample'] = lof_scores.tolist()[:100]
                detection_info['n_neighbors'] = n_neighbors
                detection_info['contamination'] = contamination
            
            # METHOD 4: Isolation Forest
            elif method == 'isolation_forest':
                contamination = detection_params.get('contamination', 0.1)
                random_state = detection_params.get('random_state', 42)
                n_estimators = detection_params.get('n_estimators', 100)
                
                logger.info(f"Isolation Forest parameters:")
                logger.info(f"  contamination: {contamination}")
                logger.info(f"  n_estimators: {n_estimators}")
                
                X = df[numeric_cols].values
                
                iso_forest = IsolationForest(
                    contamination=contamination,
                    random_state=random_state,
                    n_estimators=n_estimators,
                    n_jobs=-1
                )
                outlier_labels = iso_forest.fit_predict(X)
                outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                
                detector = iso_forest  # Store fitted forest
                
                detection_info['contamination'] = contamination
                detection_info['n_estimators'] = n_estimators
            
            else:
                raise ValueError(
                    f"Unknown detection method: {method}. "
                    f"Available: zscore, iqr, lof, isolation_forest, none"
                )
            
            n_outliers = outlier_mask.sum()
            pct_outliers = (n_outliers / len(df)) * 100 if len(df) > 0 else 0
            detection_info['n_outliers'] = int(n_outliers)
            detection_info['outlier_percentage'] = float(pct_outliers)
            
            logger.info("")
            logger.info(f"Found {n_outliers} outliers ({pct_outliers:.2f}%)")
            logger.info("")
            
            return outlier_mask, detector, detection_info
        
        
        def apply_detector_transform(df, detector, detection_method):
            #Apply pre-fitted detector - TRANSFORM mode#
            logger.info("="*80)
            logger.info("APPLYING PRE-FITTED DETECTOR (TRANSFORM)")
            logger.info("="*80)
            logger.info(f"Method: {detection_method}")
            logger.info("")
            
            if detection_method == 'none':
                logger.info("Detection method is 'none' - skipping")
                return pd.Series(False, index=df.index), {
                    'method': 'none',
                    'mode': 'transform',
                    'n_outliers': 0
                }
            
            if detector is None:
                logger.warning("No detector provided, returning no outliers")
                return pd.Series(False, index=df.index), {
                    'method': detection_method,
                    'mode': 'transform',
                    'warning': 'no_detector',
                    'n_outliers': 0
                }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            try:
                if detection_method == 'zscore':
                    threshold = detector['threshold']
                    train_mean = detector['mean']
                    train_std = detector['std']
                    
                    logger.info(f"Using train statistics for z-score (threshold={threshold})")
                    
                    z_scores = np.abs((df[numeric_cols] - pd.Series(train_mean)) / pd.Series(train_std))
                    outlier_mask = (z_scores > threshold).any(axis=1)
                    
                elif detection_method == 'iqr':
                    bounds = detector['bounds']
                    logger.info("Using train IQR bounds")
                    
                    outlier_mask = pd.Series(False, index=df.index)
                    for col in numeric_cols:
                        if col in bounds:
                            lower = bounds[col]['lower']
                            upper = bounds[col]['upper']
                            col_outliers = (df[col] < lower) | (df[col] > upper)
                            outlier_mask = outlier_mask | col_outliers
                
                elif detection_method == 'lof':
                    X = df[numeric_cols].values
                    outlier_labels = detector.predict(X)
                    outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                    logger.info("Applied fitted LOF detector")
                
                elif detection_method == 'isolation_forest':
                    X = df[numeric_cols].values
                    outlier_labels = detector.predict(X)
                    outlier_mask = pd.Series(outlier_labels == -1, index=df.index)
                    logger.info("Applied fitted Isolation Forest")
                
                n_outliers = outlier_mask.sum()
                logger.info(f"Detected {n_outliers} outliers in test data")
                logger.info("")
                
                return outlier_mask, {
                    'method': detection_method,
                    'mode': 'transform',
                    'n_outliers': int(n_outliers),
                    'outlier_percentage': float(n_outliers / len(df) * 100) if len(df) > 0 else 0
                }
                
            except Exception as e:
                logger.error(f"Error applying detector: {str(e)}")
                logger.error("Train/test data may have different structures")
                raise
        
        
        def handle_outliers(df, outlier_mask, method, handling_params, detection_info):
            #Handle detected outliers#
            logger.info("="*80)
            logger.info("HANDLING OUTLIERS")
            logger.info("="*80)
            logger.info(f"Method: {method}")
            
            if handling_params:
                logger.info(f"Parameters: {handling_params}")
            logger.info("")
            
            if method == 'none':
                logger.info("Handling method is 'none' - skipping")
                return df, {'method': 'none', 'n_rows_removed': 0}
            
            df_handled = df.copy()
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            stats = {
                'method': method,
                'params': handling_params,
                'n_outliers_detected': int(outlier_mask.sum()),
                'n_rows_removed': 0,
                'columns_processed': numeric_cols,
                'handling_details': {}
            }
            
            # METHOD 1: Remove
            if method == 'remove':
                removed_indices = df[outlier_mask].index.tolist()
                df_handled = df[~outlier_mask].reset_index(drop=True)
                stats['n_rows_removed'] = len(removed_indices)
                logger.info(f"Removed {len(removed_indices)} outlier rows")
            
            # METHOD 2: Clip/Cap
            elif method in ['clip', 'cap']:
                use_iqr = handling_params.get('use_iqr', True)
                factor = handling_params.get('factor', 1.5)
                
                if detection_info and 'bounds' in detection_info:
                    bounds = detection_info['bounds']
                    logger.info("Using bounds from IQR detection")
                elif use_iqr:
                    logger.info(f"Calculating IQR bounds (factor={factor})")
                    bounds = {}
                    for col in numeric_cols:
                        Q1 = df[col].quantile(0.25)
                        Q3 = df[col].quantile(0.75)
                        IQR = Q3 - Q1
                        bounds[col] = {
                            'lower': float(Q1 - factor * IQR),
                            'upper': float(Q3 + factor * IQR)
                        }
                else:
                    bounds = {}
                    for col in numeric_cols:
                        bounds[col] = {
                            'lower': float(df[col].min()),
                            'upper': float(df[col].max())
                        }
                
                total_clipped = 0
                for col in numeric_cols:
                    if col in bounds:
                        lower = bounds[col]['lower']
                        upper = bounds[col]['upper']
                        values_clipped = ((df[col] < lower) | (df[col] > upper)).sum()
                        total_clipped += values_clipped
                        df_handled[col] = df_handled[col].clip(lower=lower, upper=upper)
                        
                        if values_clipped > 0:
                            logger.info(f"  {col}: clipped {values_clipped} values")
                
                logger.info(f"Total values clipped: {total_clipped}")
            
            # METHOD 3: Winsorize
            elif method == 'winsorize':
                limits = handling_params.get('limits', [0.05, 0.05])
                logger.info(f"Winsorizing with limits: {limits}")
                
                for col in numeric_cols:
                    df_handled[col] = mstats.winsorize(df_handled[col], limits=limits)
                    logger.info(f"  {col}: winsorized")
            
            # METHOD 4: Log transform
            elif method == 'log_transform':
                for col in numeric_cols:
                    if (df_handled[col] > 0).all():
                        df_handled[col] = np.log1p(df_handled[col])
                        logger.info(f"  {col}: log transformed")
                    else:
                        logger.warning(f"  {col}: skipped (non-positive values)")
            
            # METHOD 5: Sqrt transform
            elif method == 'sqrt_transform':
                for col in numeric_cols:
                    if (df_handled[col] >= 0).all():
                        df_handled[col] = np.sqrt(df_handled[col])
                        logger.info(f"  {col}: sqrt transformed")
                    else:
                        logger.warning(f"  {col}: skipped (negative values)")
            
            # METHOD 6: Flag
            elif method == 'flag':
                flag_column = handling_params.get('flag_column', 'is_outlier')
                df_handled[flag_column] = outlier_mask.astype(int)
                logger.info(f"Added '{flag_column}' flag column")
            
            else:
                raise ValueError(
                    f"Unknown handling method: {method}. "
                    f"Available: remove, clip, cap, winsorize, log_transform, sqrt_transform, flag, none"
                )
            
            stats['initial_shape'] = list(df.shape)
            stats['final_shape'] = list(df_handled.shape)
            
            logger.info(f"Final shape: {df_handled.shape}")
            logger.info("")
            
            return df_handled, stats
        
        
        def main():
            parser = argparse.ArgumentParser(
                description="Generic Outlier Detection & Handling (Enhanced)"
            )
            parser.add_argument("--input_data", required=True)
            parser.add_argument("--detection_method", default='zscore')
            parser.add_argument("--detection_params", type=str, default='{}')
            parser.add_argument("--handling_method", default='clip')
            parser.add_argument("--handling_params", type=str, default='{}')
            parser.add_argument("--mode", default='fit_transform',
                               choices=['fit_transform', 'transform'])
            parser.add_argument("--fitted_detector", default=None, help="Path to fitted detector (optional, only for transform mode)")
            parser.add_argument("--output_data", required=True)
            parser.add_argument("--output_outlier_info", required=True)
            parser.add_argument("--output_report", required=True)
            
            args = parser.parse_args()
            
            logger.info("="*80)
            logger.info("OUTLIER DETECTION & HANDLING COMPONENT (ENHANCED)")
            logger.info("="*80)
            logger.info(f"Mode: {args.mode}")
            logger.info(f"Detection: {args.detection_method}")
            logger.info(f"Handling: {args.handling_method}")
            logger.info("")
            
            try:
                ensure_directory_exists(args.output_data)
                ensure_directory_exists(args.output_outlier_info)
                ensure_directory_exists(args.output_report)
                
                # Parse parameters
                try:
                    detection_params = json.loads(args.detection_params)
                    handling_params = json.loads(args.handling_params)
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON: {e}")
                    sys.exit(1)
                
                # Load data
                df = load_data(args.input_data)
                
                if df.empty:
                    logger.error("ERROR: Dataset is empty")
                    sys.exit(1)
                
                # Process based on mode
                if args.mode == 'fit_transform':
                    # TRAIN: Detect and handle
                    outlier_mask, detector, detection_info = detect_outliers(
                        df=df,
                        method=args.detection_method,
                        detection_params=detection_params
                    )
                    
                    # Save detector
                    with open(args.output_outlier_info, 'wb') as f:
                        pickle.dump({
                            'detector': detector,
                            'detection_info': detection_info,
                            'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist()
                        }, f)
                    logger.info(f"Detector saved: {args.output_outlier_info}")
                    
                elif args.mode == 'transform':
                    # TEST: Apply pre-fitted detector
                    if not args.fitted_detector:
                        logger.error("ERROR: fitted_detector is required for transform mode")
                        logger.error("In YAML, make sure to pass the fitted_detector artifact from the train step:")
                        logger.error("  artifacts:")
                        logger.error("  - name: fitted_detector")
                        logger.error("    from: '{{tasks.X-train.outputs.artifacts.outlier_info}}'")
                        sys.exit(1)
                    
                    if not os.path.exists(args.fitted_detector):
                        logger.error(f"ERROR: fitted_detector file not found: {args.fitted_detector}")
                        sys.exit(1)
                    
                    logger.info(f"Loading detector from: {args.fitted_detector}")
                    with open(args.fitted_detector, 'rb') as f:
                        detector_data = pickle.load(f)
                    
                    detector = detector_data['detector']
                    logger.info(f"Loaded detector from: {args.fitted_detector}")
                    
                    outlier_mask, detection_info = apply_detector_transform(
                        df=df,
                        detector=detector,
                        detection_method=args.detection_method
                    )
                    
                    # Don't save new detector in transform mode
                    with open(args.output_outlier_info, 'wb') as f:
                        pickle.dump(None, f)
                
                # Handle outliers
                df_handled, handling_stats = handle_outliers(
                    df=df,
                    outlier_mask=outlier_mask,
                    method=args.handling_method,
                    handling_params=handling_params,
                    detection_info=detection_info
                )
                
                # Save data
                df_handled.to_csv(args.output_data, index=False)
                logger.info(f"Data saved: {args.output_data}")
                
                # Save report
                report = {
                    'mode': args.mode,
                    'detection': {
                        'method': args.detection_method,
                        'params': detection_params,
                        'n_outliers': detection_info.get('n_outliers', 0),
                        'outlier_percentage': detection_info.get('outlier_percentage', 0.0)
                    },
                    'handling': {
                        'method': args.handling_method,
                        'params': handling_params,
                        'stats': handling_stats
                    }
                }
                
                with open(args.output_report, 'w') as f:
                    json.dump(report, f, indent=2)
                logger.info(f"Report saved: {args.output_report}")
                
                logger.info("")
                logger.info("="*80)
                logger.info("OUTLIER HANDLING COMPLETED")
                logger.info("="*80)
                logger.info(f"Mode: {args.mode}")
                logger.info(f"Outliers detected: {detection_info.get('n_outliers', 0)}")
                logger.info(f"Final shape: {df_handled.shape}")
                logger.info("="*80)
                
            except Exception as e:
                logger.error(f"ERROR: {str(e)}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        
        if __name__ == "__main__":
            main()
    args:
      - --input_data
      - {inputPath: input_data}
      - --detection_method
      - {inputValue: detection_method}
      - --detection_params
      - {inputValue: detection_params}
      - --handling_method
      - {inputValue: handling_method}
      - --handling_params
      - {inputValue: handling_params}
      - --mode
      - {inputValue: mode}
      - --fitted_detector
      - {inputPath: fitted_detector}
      - --output_data
      - {outputPath: data}
      - --output_outlier_info
      - {outputPath: outlier_info}
      - --output_report
      - {outputPath: report}
